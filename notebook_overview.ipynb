{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEV6_H3_jXfW"
      },
      "source": [
        "# Business Context and Project Scenario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBE-cTwWQYIU"
      },
      "source": [
        "The Bank of England prudentially regulates and supervises financial services firms through the Prudential Regulation Authority (PRA). The PRA are responsible for the prudential regulation and supervision of around 1,500 banks, building societies, credit unions, insurers, and major investment firms.  To achieve this, the PRA examines various data sources, some more accessible than others. Quarterly result announcements are a particularly challenging data source to analyse. Traditional data science methods struggle to fully utilise this type of data because of the following two reasons:\n",
        "\n",
        "- They are unstructured in the form of text and/or video/webcasts.\n",
        "- They are complex, requiring technical and financial background knowledge.\n",
        "\n",
        "This project aims to enhance the use of these data sets to improve our risk assessment of individual firms and, in doing so, maintain financial stability.\n",
        "\n",
        "The Bank of England’s RegTech, Data and Innovation team (the ‘Team’) would like to understand if the quarterly result announcements provide additional information or insights on a firm. This includes, but is not limited to, the following:\n",
        "\n",
        "1. **Topic modelling and sentiment analysis**: Using a mix of text pre-processing and pre-trained language models (e.g. FinBERT and frameworks like BERTopic), is it possible to cluster the key topics raised by industry analysts and the sentiments related to those topics during Q&A sessions with the senior management team (e.g. in the earnings call Q&A transcripts)?\n",
        "2. **Information summarisation**: Using a mix of text pre-processing and pre-trained language models, can language models be used to extract and summarise key takeaways raised in these transcripts? Some text preprocessing/intermediary language model pipelines would probably be needed before generating summaries themselves so that the summaries are grouped in a manner that makes sense. Example groups could be:\n",
        "\n",
        "  - By topic; for example, your choice of specific issues raised by analysts (two or three topics would do, although you are welcome to cover more ground)\n",
        "  - By specific metrics; for example, a summary of all instances related to metric X, a summary of all instances related to metric Y, etc. (two or three metrics would do, although you are welcome to cover more metrics)\n",
        "  - By speaker; for example, analyst or banker.\n",
        "\n",
        "The team is also keen to explore additional methods for extracting value from these data sources. This includes new technical approaches using GenAI/language models, as well as innovative ways to analyse and compare data, such as benchmarking a firm against its peers.\n",
        "\n",
        "As the PRA regulates many institutions, the Team proposes focusing on one or two banks which have been identified in the list of global systemically important banks (G-SIBs).\n",
        "\n",
        "For this project, call transcripts from JPMorgan Chase were selected foe initial development and assessment of methodologies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uUeRA7Zms6-"
      },
      "source": [
        "# Setup: Libraries & Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "99rSDddomt4U",
        "outputId": "57bd09e6-837d-4132-9c97-608739ffafc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.11/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.1.0)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=4.7.0->bertopic) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.52.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.14.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.6.15)\n",
            "Downloading bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2825262420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install bertopic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install prettytable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install scikit-learn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install sentence-transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install spacy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_installation_commands.py\u001b[0m in \u001b[0;36m_pip_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Colab is set up such that pip does the right thing, and pip install\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# will properly trigger the pip install warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%pip install bertopic\n",
        "%pip install prettytable\n",
        "%pip install scikit-learn\n",
        "%pip install sentence-transformers\n",
        "%pip install spacy\n",
        "%pip install venn\n",
        "%pip install PyMuPDF\n",
        "%pip install -U -q PyDrive\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# standard library imports\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# third-party imports\n",
        "import csv\n",
        "import fitz\n",
        "import json\n",
        "import nltk\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import tensorflow_hub as hub\n",
        "import torch\n",
        "import zipfile\n",
        "\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from dateutil import parser\n",
        "from hdbscan import HDBSCAN\n",
        "from ipywidgets import Button, HBox, Output\n",
        "from google.colab import drive, files, userdata\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib_venn import venn2, venn3\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from prettytable import PrettyTable\n",
        "from scipy.stats import gaussian_kde, linregress, mannwhitneyu, pearsonr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, ConfusionMatrixDisplay\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from umap import UMAP\n",
        "from wordcloud import WordCloud\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeuL3BVNp8bP"
      },
      "source": [
        "**Raw Data folder:** contains the raw data, financial metrics data\n",
        "\n",
        "**Processed Data folder** contains evaluation data sets, cleaned and tabularised data and Phi-3.5 summarised data\n",
        "\n",
        "**Output Data folder:** contains files created by various models for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DabEpl6QoTV_"
      },
      "outputs": [],
      "source": [
        "# Get the base repository folder by going one level up from the notebooks folder\n",
        "base_folder = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"data\"))\n",
        "\n",
        "# Define the path for each subfolder\n",
        "raw_data_folder = os.path.join(base_folder, \"Raw_Data\")\n",
        "processed_data_folder = os.path.join(base_folder, \"Processed_Data\")\n",
        "output_data_folder = os.path.join(base_folder, \"Output_Data\")\n",
        "\n",
        "\n",
        "# Create the directories if they do not exist\n",
        "for folder in [raw_data_folder, processed_data_folder, output_data_folder]:\n",
        "    os.makedirs(folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IWF0u0doot6"
      },
      "source": [
        "# 0 Exploration of Financial Metrics Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8DuFnuF143b"
      },
      "outputs": [],
      "source": [
        "metrics_path = raw_data_folder + '/key_financial_metrics_JPMorgan_clean.xlsx'\n",
        "metrics_df = pd.read_excel(metrics_path)\n",
        "\n",
        "metrics_df['date'] = pd.to_datetime(metrics_df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LG1X10vHveF0"
      },
      "outputs": [],
      "source": [
        "# Getting the unique metric types\n",
        "unique_metric_types = metrics_df['metric_type'].unique()\n",
        "\n",
        "# Defining colors for each plot\n",
        "colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "# Plotting in a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 9))\n",
        "\n",
        "# Flattening axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Looping through metric types and corresponding axes\n",
        "for i, metric in enumerate(unique_metric_types):\n",
        "    subset = metrics_df[metrics_df['metric_type'] == metric]\n",
        "    axes[i].plot(subset['Q&FY'], subset['metric_value'], marker='o', color=colors[i])\n",
        "    axes[i].set_title(f'Metric: {metric}', fontsize=16, color=colors[i])\n",
        "    axes[i].set_xlabel('Quarter', fontsize=14)\n",
        "    axes[i].set_ylabel('Metric Value', fontsize=14)\n",
        "    axes[i].tick_params(axis='x', labelsize=12, rotation=45)\n",
        "    axes[i].tick_params(axis='y', labelsize=12)\n",
        "    axes[i].grid(visible=True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdyTK-q2HF0"
      },
      "source": [
        "Picking out two interesting dates to explore based on the charts above of key financial metrics:\n",
        "\n",
        "**1Q22**\n",
        "\n",
        "In 1Q22, the bank faced significant financial challenges, marked by a low CET1 capital ratio and declining net income and EPS. This suggests reduced profitability and shareholder returns. The increase in provisions for credit losses suggests heightened caution regarding potential loan defaults. Overall, this quarter indicates a period of financial strain and risk management.\n",
        "\n",
        "**2Q24**\n",
        "\n",
        "In 2Q24, the bank reached peak performance across several key metrics, with its CET1 capital ratio, net income, and EPS all at their highest levels. This suggests a strong capital position, robust profitability, and high returns to shareholders. Provisions for credit losses, however, remained elevated. This indicates a continued cautious approach toward potential credit risks. Overall, this quarter indicates a period of strong financial results although the bank is maintaining caution to guard against possible economic uncertainties or loan defaults.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAoM9dJYmaju"
      },
      "source": [
        "# 1 Data Collection and Pre-Processing of Transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pKNvLJJ21px"
      },
      "source": [
        "\n",
        "Firstly, we downloaded all the earning calls transcripts from 2021Q2 to 2024Q3 from JPMorgan's website, and saved them as a zip file in the raw data folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1fsWvNMtUG-"
      },
      "source": [
        "## 1.1 Extract information from transcripts pdf, put into a table and output as a csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_BhK6fvcW9"
      },
      "source": [
        "The following code performs batch-processing of PDF files without specifying links to individual documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eItQu2zhwCPG"
      },
      "source": [
        "### 1.1.0 Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn5Hnb7_wE12"
      },
      "outputs": [],
      "source": [
        "def get_valid_date(text_string):\n",
        "    \"\"\"\n",
        "    helper function to check if a string is a valid date\n",
        "\n",
        "    PARAMS:\n",
        "        text_string (str) : some text\n",
        "\n",
        "    RETURNS:\n",
        "        (bool) : whether the text is a valid date\n",
        "        parsed_date (datetime.datetime) : date if detected, None otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parsed_date = parser.parse(text_string)\n",
        "        return True, parsed_date\n",
        "    except (ValueError, OverflowError):\n",
        "        return False, None\n",
        "\n",
        "def detect_date_in_list_of_strings(text_lines):\n",
        "    \"\"\"\n",
        "    detects a valid date given a list of strings\n",
        "\n",
        "    raises an error if a valid date is not detected\n",
        "\n",
        "    PARAMS:\n",
        "        text_lines (list) : list of strings\n",
        "\n",
        "    RETURNS:\n",
        "        date (datetime.datetime) : the detected date if present\n",
        "    \"\"\"\n",
        "    try: # expect date to be on the last line\n",
        "        valid_date, date = get_valid_date(text_lines[-1])\n",
        "    except:\n",
        "        for line in text_lines[:-1]:\n",
        "          valid_date, date = get_valid_date(line)\n",
        "          if valid_date:\n",
        "              break\n",
        "    if not valid_date:\n",
        "        raise ValueError(f\"Could not find a valid date!\")\n",
        "\n",
        "    return date\n",
        "\n",
        "def detect_pattern(text_lines, pattern, unique=True):\n",
        "    \"\"\"\n",
        "    detects the only (unique=True) or all occurrences of a string pattern\n",
        "    given a list of strings\n",
        "\n",
        "    raises an error if the pattern is not detected\n",
        "\n",
        "    PARAMS:\n",
        "        text_lines (list) : list of strings\n",
        "        pattern (str) : a string to be detected in the provided strings\n",
        "        unique (bool) : whether the pattern must be present in exactly one\n",
        "                        string in text_lines; defaults to True\n",
        "\n",
        "    RETURNS:\n",
        "        detected_text (str or list) : Text containing the pattern (if present)\n",
        "          If unique is True, there must be only one occurrence - returns a string\n",
        "          If unique is False, returns a list of all occurrences\n",
        "    \"\"\"\n",
        "    # expect the pattern to be in the first item\n",
        "    if pattern in text_lines[0]:\n",
        "        return text_lines[0]\n",
        "\n",
        "    detected_texts = [text for text in text_lines if pattern in text]\n",
        "\n",
        "    if unique:\n",
        "        if len(detected_texts)>1:\n",
        "            raise ValueError(\"More than one occurrence of the pattern detected!\")\n",
        "        elif not detected_texts:\n",
        "            raise ValueError(\"Pattern not detected!\")\n",
        "        return detected_texts[0]  # return the first (and only) detected text\n",
        "    else:\n",
        "        return detected_texts  # return all occurrences (even if empty list)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def is_valid_word(word):\n",
        "    \"\"\"\n",
        "    checks if a word is title case, fully uppercase (acronym) or mixed-case\n",
        "\n",
        "    PARAMS:\n",
        "      word (str)\n",
        "\n",
        "    RETURNS:\n",
        "      bool\n",
        "    \"\"\"\n",
        "    return (\n",
        "        word.istitle() or\n",
        "        word.isupper() or\n",
        "        re.match(r'^[A-Z][a-zA-Z0-9]*$', word)\n",
        "    )\n",
        "\n",
        "def get_all_caps_lines(text_lines):\n",
        "  \"\"\"\n",
        "  given a list of strings, returns the strings where all words are title case,\n",
        "  upper case (e.g. acronyms), or mixed-case. Small words, like \"a\", \"or\", \"for\",\n",
        "  are ignored, so \"Bank of America\" still gets returned\n",
        "\n",
        "  PARAMS:\n",
        "    text_lines (list of strs) : list that contains any sort of strings\n",
        "\n",
        "  RETURNS:\n",
        "    all_caps_lines (list of strs) : list of strings where all words (separated by spaces)\n",
        "                                    are title-, upper-, or mixed-case\n",
        "  \"\"\"\n",
        "  all_caps_lines = []\n",
        "  for line in text_lines:\n",
        "      # remove punctuation\n",
        "      line_clean = remove_punctuation(line)\n",
        "\n",
        "      # remove small words that are typically not capitalised\n",
        "      small_words = [\n",
        "        'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'so',\n",
        "        'yet', 'at', 'by', 'for', 'from', 'in', 'of', 'on', 'to', 'with'\n",
        "      ]\n",
        "      words = line_clean.split()\n",
        "      filtered_words = [word for word in words if word.lower() not in small_words]\n",
        "      line_clean = ' '.join(filtered_words)\n",
        "\n",
        "      # check if all words are capitalised (as in names or titles)\n",
        "      if all(is_valid_word(word) for word in line_clean.split()):\n",
        "          all_caps_lines.append(line)\n",
        "\n",
        "  return all_caps_lines\n",
        "\n",
        "def remove_preceding_numbers(text):\n",
        "    \"\"\"\n",
        "    replaces leading numbers in a string with an empty string\n",
        "    \"\"\"\n",
        "    return re.sub(r'^\\d+\\s*', '', text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPvX9ttk-v8n"
      },
      "source": [
        "### 1.1.1 Unzip files and define the path to PDF file folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ld_yFMYABet"
      },
      "outputs": [],
      "source": [
        "# Path to the zipped file\n",
        "zip_file_path = os.path.join(raw_data_folder, \"JPMorgan.zip\")\n",
        "\n",
        "# Path to the sub-directory for extracted files\n",
        "jpm_folder = os.path.join(raw_data_folder, \"JPMorgan\")\n",
        "\n",
        "# Create the target folder if it doesn't exist\n",
        "os.makedirs(jpm_folder, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "if os.path.exists(zip_file_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(jpm_folder)\n",
        "    print(f\"Extracted PDF files from {zip_file_path} to {jpm_folder}\")\n",
        "else:\n",
        "    print(f\"{zip_file_path} not found. Please check the file location.\")\n",
        "\n",
        "# Define the paths to the extracted PDF files\n",
        "paths = [os.path.join(jpm_folder,f) for f in os.listdir(jpm_folder) if f.endswith(\".pdf\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xd1Zax5dSr3"
      },
      "source": [
        "### 1.1.2 Defining a JPMorgan PDF-to-table converter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhOhBUkd27xE"
      },
      "source": [
        "The following function is tailored to the format of JPMorgan transcripts by assuming that:\n",
        "1. The first page contains date on its own line\n",
        "2. The first page contains the quarter info formatted as `\"xQYY\"`\n",
        "3. Speakers are separated by `\".................\"`\n",
        "4. Everything said by the operator is preceded by `\"Operator: \"`\n",
        "5. There is a `\"MANAGEMENT DISCUSSION SECTION\"` and a `\"QUESTION AND ANSWER SECTION\"` titled in all-caps\n",
        "6. All words in speaker and company names are capitalised. The only exception is small words like \"of\", \"at\", etc.\n",
        "7. All Qs and As start with speaker information formatted like `[name]\\n[title, firm]\\n[Q/A]`\n",
        "\n",
        "If the above assumptions are met, this function will return a dataframe with the following information:\n",
        "- **uid** (str) : informative unique identifier for each row\n",
        "- **bank** (str) : bank name (for `process_jpmorgan_pdf`, it is always JPMorgan)\n",
        "- **year** (int) : year discussed in the earnings call (for 4Q22, this is 2022, even though the earnings call took place in January 2023)\n",
        "- **quarter** (int) : quarter discussed in the earnings call\n",
        "- **date** (datetime) : date the earnings call took place\n",
        "- **section** (str) : section the text comes from (`\"management_discussion\" or \"questions_answers\"`)\n",
        "- **name** (str) : name of the speaker\n",
        "- **title** (str) : job title of the speaker\n",
        "- **firm** (str) : firm the speaker represents\n",
        "- **qa_type** (str) : text type (`\"Q\"`, `\"A\"`, `\"N\"` for questions, answers, and neither respectively)\n",
        "- **qa_num_within** (int) : number of the question and answer _within_ an earnings call; this can be used to map answers to questions within a call after processing\n",
        "- **qa_num** (int) : number of the question and answer _across_ all earnings calls; this can be used to map answers to questions across calls after processing\n",
        "- **qa_text** (str) : question or answer text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0egiISm_EBqx"
      },
      "outputs": [],
      "source": [
        "def process_jpmorgan_pdf(path, q_num_init=0):\n",
        "    doc = fitz.open(path)\n",
        "\n",
        "    print(f\"This document consists of {len(doc)} pages\")\n",
        "    bank = \"JPMorganChase\"\n",
        "\n",
        "    df = pd.DataFrame(columns =[\"uid\", \"uid_prelim\", \"bank\", \"year\", \"quarter\", \"date\", \"section\",\n",
        "                            \"name\", \"title\", \"firm\", \"qa_type\", \"qa_num_within\", \"qa_num\", \"qa_text\"])\n",
        "    last_speaker = \"\"\n",
        "    current_section = \"\"\n",
        "    q_num_within = 0\n",
        "    q_num = q_num_init         # count questions to be able to link As to Qs in case of separate processing\n",
        "    # iterate over pages\n",
        "    for page_num in range(len(doc)):\n",
        "        print(f\"Working on page {page_num}...\")\n",
        "        page = doc.load_page(page_num)                          # load a page\n",
        "        text = page.get_text()                                  # extract text\n",
        "        text_stripped = text.strip()\n",
        "        text_lines = text_stripped.split('\\n')                  # split text into a list of strings\n",
        "\n",
        "        if page_num == 0:                                       # first page\n",
        "              date = detect_date_in_list_of_strings(text_lines)   # when the report was published\n",
        "              year_published = str(date.year)\n",
        "\n",
        "              try:\n",
        "                  expected_pattern = f\"Q{year_published[-2:]}\"\n",
        "                  quarter = detect_pattern(text_lines, expected_pattern, unique=True)\n",
        "                  year = int(year_published)                             # year the report refers to\n",
        "              except:   # Q4 are typically released in January, so it refers to the previous year\n",
        "                  expected_pattern = f\"Q{int(year_published[-2:])-1}\"\n",
        "                  quarter = detect_pattern(text_lines, expected_pattern, unique=True)\n",
        "                  year = int(year_published)-1\n",
        "              if quarter.split(\" \"):\n",
        "                  # if quarter returns several \"words\", get only the word with the pattern\n",
        "                  quarter = detect_pattern(quarter.split(\" \"), expected_pattern, unique=True)\n",
        "              continue\n",
        "\n",
        "        # process the other pages\n",
        "        text_sections = text_stripped.split(\"........................................................\")\n",
        "        text_sections = [item for item in text_sections if item.strip()]  # filter out empty strings\n",
        "\n",
        "        # iterate over sections\n",
        "        for section in text_sections:\n",
        "            if \"Operator: \" in section:\n",
        "                last_speaker = \"Operator\"\n",
        "                if \"QUESTION AND ANSWER\" in section:\n",
        "                    current_section = \"questions_answers\"\n",
        "                continue\n",
        "\n",
        "            if \"MANAGEMENT DISCUSSION\" in section:\n",
        "                current_section = \"management_discussion\"\n",
        "                continue\n",
        "            elif \"QUESTION AND ANSWER\" in section:\n",
        "                current_section = \"questions_answers\"\n",
        "                continue\n",
        "\n",
        "            # remove leading dots and empty lines\n",
        "            section_nodots = re.sub(r'^\\.+', '', section)\n",
        "            lines = [item for item in section_nodots.split(\"\\n\") if item.strip()]\n",
        "            if not lines:             # empty list\n",
        "                continue\n",
        "\n",
        "            # get lines were all words are capitalised (names, acronyms, \"Q \" or \"A \")\n",
        "            all_caps_lines = get_all_caps_lines(lines)\n",
        "\n",
        "            # define desirable conditions\n",
        "            bool_length = len(all_caps_lines)>=2\n",
        "\n",
        "            # if not all_caps_lines and last_speaker != \"Operator\":\n",
        "            if not bool_length and last_speaker != \"Operator\":\n",
        "                # a section starts without anyone being introduced and it is not a\n",
        "                # continuation of Operator's speech means that previous Q/A is continued\n",
        "                # -- this will need to be merged with the preceding row afterwards\n",
        "                name = title = firm = qa_type = \"\"\n",
        "                qa_text = remove_preceding_numbers(section_nodots).replace('\\n', '')\n",
        "\n",
        "                df.at[df.index[-1], 'qa_text'] = df.iloc[-1].qa_text + \" \" + qa_text\n",
        "\n",
        "            elif bool_length:\n",
        "                name = all_caps_lines[0].strip()\n",
        "                # print(all_caps_lines)\n",
        "                title = all_caps_lines[1].split(\",\")[0].strip()\n",
        "                firm = all_caps_lines[1].split(\",\")[1].strip()\n",
        "\n",
        "                # catch cases where three all-caps phrases get picked\n",
        "                # due to lines with just one capitalised word\n",
        "                wrong_catch = False\n",
        "                if len(all_caps_lines)>2:\n",
        "                  if all_caps_lines[2].strip() not in ['Q', 'A']:\n",
        "                    wrong_catch = True\n",
        "\n",
        "                if len(all_caps_lines)==2 or wrong_catch:\n",
        "                    qa_type = \"N\"\n",
        "                    qa_id_within = np.nan\n",
        "                    qa_id = np.nan\n",
        "                    qa_text = section_nodots.split(firm)[-1].replace('\\n', '').strip()\n",
        "                else:\n",
        "                    qa_type = all_caps_lines[2].strip()\n",
        "                    if qa_type == \"Q\" and name != last_speaker:\n",
        "                      q_num_within += 1\n",
        "                      q_num += 1\n",
        "                    qa_id_within = q_num_within\n",
        "                    qa_id = q_num\n",
        "\n",
        "                    # # if an executive answers before the analyst asks a question,\n",
        "                    # # we do not want to count it as an answer to the earlier question\n",
        "                    # if qa_type == \"A\" and last_speaker==\"Operator\":\n",
        "                    #   qa_id_within = np.nan\n",
        "                    #   qa_id = np.nan\n",
        "                    # # commenting this out because sometimes exeuctives continue\n",
        "                    # # providing useful information even after the operator intervenes\n",
        "\n",
        "                    str_splitter = section_nodots.split(qa_type)[0]\n",
        "                    qa_text = section_nodots.split(str_splitter)[-1][2:].replace('\\n', '').strip()\n",
        "                    if qa_text==\"\": # try to catch PyMuPDF formatting errors!\n",
        "                          if current_section=='questions_answers' and lines[0].strip()==name and not lines[2].strip()==qa_type:\n",
        "                                qa_text = \" \".join([item for item in lines if item not in all_caps_lines])\n",
        "\n",
        "                # define an informative unique identified\n",
        "                prelim_uid = f\"{bank}_{quarter}_{qa_type}_{q_num_within}\"\n",
        "                num_prelim_uids = (df.uid_prelim == prelim_uid).sum()\n",
        "                uid = prelim_uid + f\".{num_prelim_uids}\"\n",
        "\n",
        "                # check if it is not the same speaker continuing on a new page\n",
        "                if name == last_speaker:\n",
        "                    df.at[df.index[-1], 'qa_text'] = df.iloc[-1].qa_text + \" \" + qa_text\n",
        "                # elif qa_id_within==0: # can happen if Q&A is started off by an executive\n",
        "                #     pass\n",
        "                else:\n",
        "                    df_to_append = pd.DataFrame({\n",
        "                    \"uid\": [uid],\n",
        "                    \"uid_prelim\": [prelim_uid],\n",
        "                    \"bank\": [bank],\n",
        "                    \"year\": [year],\n",
        "                    \"quarter\": [int(quarter[0])],\n",
        "                    \"date\": [date],\n",
        "                    \"section\": [current_section],\n",
        "                    \"name\": [name],\n",
        "                    \"title\": [title],\n",
        "                    \"firm\": [firm],\n",
        "                    \"qa_type\": [qa_type],\n",
        "                    \"qa_num_within\" : [qa_id_within],\n",
        "                    \"qa_num\": [qa_id],\n",
        "                    \"qa_text\": [qa_text]\n",
        "                    })\n",
        "\n",
        "                    if not df_to_append.isna().all(axis=1).any():\n",
        "                        df = pd.concat((df, df_to_append), ignore_index=True)\n",
        "\n",
        "                last_speaker = name\n",
        "\n",
        "            else:\n",
        "                print(\"Excluding:\\n\", section_nodots)\n",
        "                continue\n",
        "                # raise ValueError(\"Failed!\")\n",
        "\n",
        "    df = df.drop([\"uid_prelim\"], axis=1)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hn0qG3-Ndbe4"
      },
      "source": [
        "### 1.1.3 Defining a wrapper function to batch-process all PDF files in the folder\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FujPLKsm2-VX"
      },
      "source": [
        "Since each bank will need a dedicated transcript processing function, this wrapper function asks the user to input the bank they wish to process.\n",
        "\n",
        "All files in the supplied `pathlist` must belong to the same bank. Otherwise, the wrapper function returns an error.\n",
        "\n",
        "All information from the target bank is concatenated in a single `csv` file called `\"transcripts_tabular_{bank}.csv\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKrz5BDhdfgw"
      },
      "outputs": [],
      "source": [
        "def transcript_pdf_to_csv(pathlist, save_folder):\n",
        "  banks = ['JPMorgan']\n",
        "  print(\"Which bank are you seeking to process?\")\n",
        "  bank = input(banks)\n",
        "  print(\"Do all paths in `pathlist` lead to files from this bank?\")\n",
        "  confirmation = input(['yes', 'no'])\n",
        "  if confirmation != 'yes':\n",
        "      raise ValueError(\"All paths must lead to the files from the chosen bank!\")\n",
        "\n",
        "  df_all_transcripts = pd.DataFrame(columns =[\"uid\", \"bank\", \"year\", \"quarter\", \"date\", \"section\",\n",
        "                            \"name\", \"title\", \"firm\", \"qa_type\", \"qa_num_within\", \"qa_num\", \"qa_text\"])\n",
        "  for path in pathlist:\n",
        "    q_num_init = 0 if pd.isna(df_all_transcripts.qa_num.max()) else df_all_transcripts.qa_num.max()\n",
        "    if bank == 'JPMorgan':\n",
        "      df = process_jpmorgan_pdf(path, q_num_init)\n",
        "      df_all_transcripts = pd.concat((df_all_transcripts, df), ignore_index=True)\n",
        "\n",
        "  df_all_transcripts.to_csv(os.path.join(save_folder, f\"transcripts_tabular_{bank}.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJLg1jvVDJqA"
      },
      "source": [
        "### 1.1.4 Converting all PDF files in the folder to a single csv table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO2mUhJ83CE8"
      },
      "source": [
        "Running the function prints all information that gets excluded. In case of JPMorgan, it excludes the disclaimers on the last page of all transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vQBqY7dEf-Er"
      },
      "outputs": [],
      "source": [
        "transcript_pdf_to_csv(pathlist=paths, save_folder=processed_data_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lea6chiGwfSV"
      },
      "source": [
        "### 1.1.5 Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFaSge5axO2O"
      },
      "outputs": [],
      "source": [
        "df_jpm = pd.read_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan.csv\")\n",
        "df_jpm.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ghQiQM1GEo"
      },
      "source": [
        "### 1.1.6 Checking the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyCx-5661Gf_"
      },
      "outputs": [],
      "source": [
        "def check_dataset(df):\n",
        "  print('########## Confirm that all text rows contain text ##########')\n",
        "  print(f\"There are {df.qa_text.isna().sum()} empty text rows\")\n",
        "  print('\\n')\n",
        "\n",
        "  print('########## Check whether categorical rows have the expected value range ##########')\n",
        "  categoricals = ['bank', 'section', 'name', 'title', 'firm', 'qa_type']\n",
        "  for cat in categoricals:\n",
        "    print(f\"{cat}: {df[cat].unique()}\")\n",
        "  print('\\n')\n",
        "\n",
        "  print('########## Check that numerical columns have the expected value range ##########')\n",
        "  display(df.describe())\n",
        "  print('\\n')\n",
        "\n",
        "  print('########## Confirm that all uids are unique ##########')\n",
        "  print(f\"There are {df.uid.unique().shape[0]} unique uids and {df.shape[0]} rows in the dataset\")\n",
        "  print('\\n')\n",
        "\n",
        "check_dataset(df_jpm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Hjck-b2G4f"
      },
      "source": [
        "## 1.2 Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyAQl-af2qq2"
      },
      "source": [
        "In the transcripts, there are situations where interruptions occured. This is reflected as '...' in the texts. Furthermore, at the end of some of the analysts questions, there are brief conversations of greetings. These do not provide any information and should be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnViCTf3MMTD"
      },
      "outputs": [],
      "source": [
        "# load the csv from the previous step\n",
        "data = pd.read_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_YAQNvsTAr5"
      },
      "source": [
        "### 1.2.0 Helper functions / Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUh6EyodTB7Z"
      },
      "outputs": [],
      "source": [
        "def print_row(row, uids):\n",
        "  if row['uid'] in uids:\n",
        "    print(f\"\\033[31m{row['qa_type']} ({row['name']}): {row['qa_text']}\\033[0m\")\n",
        "  else:\n",
        "    print(f\"{row['qa_type']} ({row['name']}): {row['qa_text']}\")\n",
        "\n",
        "\n",
        "def print_multi_question(data, qa_num, uids):\n",
        "  data_qa = data[data['qa_num'] == qa_num]\n",
        "  data_qa.apply(lambda x: print_row(x, uids), axis=1)\n",
        "\n",
        "def print_single_question(data, qa_num, uid):\n",
        "  data_qa = data[data['qa_num'].isin([qa_num, qa_num-1, qa_num+1])]\n",
        "  data_qa.apply(lambda x: print_row(x, [uid]), axis=1)\n",
        "\n",
        "\n",
        "def get_action_for_multi(uids):\n",
        "  print_multi_question(data, num, uids)\n",
        "  action = input(\"Do you want to consider them together?\\n\")\n",
        "  return action\n",
        "\n",
        "def get_action_for_single(uid):\n",
        "  pass\n",
        "\n",
        "\n",
        "def split_string_by_punctuation(text):\n",
        "  punct_regex = r\"(?=\\S)(?:i.e.|J.P.|U.S.|ex.|[A-Z][a-z]{0,3}\\.|[^.?!]|\\.(?!\\s+[A-Z]))*.?\"\n",
        "  return re.findall(punct_regex, text)\n",
        "\n",
        "\n",
        "def remove_sentence_with_three_dots(text):\n",
        "  sentences = split_string_by_punctuation(text)\n",
        "  if text.startswith('...') or text.startswith('…'):\n",
        "    sentences = sentences[1:]\n",
        "  if text.endswith('...') or text.endswith('…'):\n",
        "    sentences = sentences[:-1]\n",
        "  return ' '.join(sentences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH3R3TYX4bNg"
      },
      "source": [
        "### 1.2.1 Dealing with interruptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykZHw5lt4heH"
      },
      "source": [
        "Identify where interruptions occurs in transcripts by searching for '…' or '...'.\n",
        "\n",
        "Then go through each instance, ask whether to delete the entire row ('delete all'), delete the sentence containing the triple dots ('delete part') or keep it.\n",
        "\n",
        "If there are two instances identified in within the same Q/A, there's an extra option of merging the two rows and delete everything in between ('merge'), corresponds to the case where an unimportant interruption happened during someone's speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLXYegQYEDzh"
      },
      "outputs": [],
      "source": [
        "def clean_interruptions(df):\n",
        "  # identify interruptions by search for triple dots\n",
        "  df['qa_text'] = df['qa_text'].astype(str)\n",
        "  df['interruption'] = df.apply(lambda x: x['qa_text'].find('...')!=-1 or x['qa_text'].find('…')!=-1, axis=1)\n",
        "\n",
        "  interruption_num = df[df['interruption'] == True]['qa_num'].unique()\n",
        "  # create a dictionary of {qa_num: [list of uids]}\n",
        "  interruption_dict = {num: df[(df['interruption'] == True) & (df['qa_num'] == num)]['uid'].tolist() for num in interruption_num}\n",
        "\n",
        "  # create an interface to display interruptions and ask for actions\n",
        "  out = Output(layout={'width': '50em'})\n",
        "  display(out)\n",
        "\n",
        "  with out:\n",
        "    for num in interruption_num:\n",
        "      uids = interruption_dict[num]\n",
        "      out.clear_output()\n",
        "\n",
        "      # deal with the special case of 2 interruptions within one Q/A\n",
        "      if len(uids) == 2:\n",
        "        print_multi_question(df, num, uids)\n",
        "        action = input(\"Do you want to consider them together? \\n\")\n",
        "        if action.lower() == \"yes\":\n",
        "          action = input(\"What's your action? \\nType 'merge' to combine the two rows and delete anything in between. \\nType 'keep' to do nothing. \\n\")\n",
        "          if action == 'merge':\n",
        "            # append the text from the second row to the first row\n",
        "            df.loc[df['uid'] == uids[0], 'qa_text'] += ' ' + df.loc[df['uid'] == uids[1], 'qa_text'].values\n",
        "            # delete rows in between\n",
        "            start_index = df.index[df['uid'] == uids[0]][0]\n",
        "            end_index = df.index[df['uid'] == uids[1]][0]\n",
        "            index_to_delete = range(start_index+1, end_index+1)\n",
        "            df = df.drop(index_to_delete).reset_index(drop=True)\n",
        "        else:\n",
        "          for uid in uids:\n",
        "            out.clear_output()\n",
        "            print_single_question(df, num, uid)\n",
        "            action = input(\"What's your action? \\nType 'delete all' to delete the entire row. \\nType 'delete part' to remove the sentence containing the triple dots. \\nType 'keep' to do nothing. \\n\")\n",
        "\n",
        "            if action == 'delete all':\n",
        "              df = df[df['uid'] != uid]\n",
        "            elif action == 'delete part':\n",
        "              df.loc[df['uid'] == uid, \"qa_text\"] = df.loc[df['uid'] == uid, \"qa_text\"].apply(remove_sentence_with_three_dots)\n",
        "      else:\n",
        "        for uid in uids:\n",
        "          out.clear_output()\n",
        "          print_single_question(df, num, uid)\n",
        "          action = input(\"What's your action? \\nType 'delete all' to delete the entire row. \\nType 'delete part' to remove the sentence containing the triple dots. \\nType 'keep' to do nothing. \\n\")\n",
        "\n",
        "          if action == 'delete all':\n",
        "            df = df[df['uid'] != uid]\n",
        "          elif action == 'delete part':\n",
        "            df.loc[df['uid'] == uid, \"qa_text\"] = df.loc[df['uid'] == uid, \"qa_text\"].apply(remove_sentence_with_three_dots)\n",
        "  df.drop(columns=['interruption'], inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpMireSSGJx6"
      },
      "source": [
        "### 1.2.2 Deal with short texts\n",
        "Identify rows that are short (word count <= 5). A large amount of these will be greetings recorded as messages.\n",
        "\n",
        "The program goes through each instance, and ask whether to delete or keep the row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdoIVMbCGSeh"
      },
      "outputs": [],
      "source": [
        "def clean_short(df):\n",
        "  # identify short texts that contain at most 5 words\n",
        "  df['short'] = df.apply(lambda x: len(x['qa_text'].split(' ')) <= 5, axis=1)\n",
        "\n",
        "  short_num = df[df['short'] == True]['qa_num'].unique()\n",
        "  # create a dictionary of {qa_num: [list of uids]}\n",
        "  short_dict = {num: df[(df['short'] == True) & (df['qa_num'] == num)]['uid'].tolist() for num in short_num}\n",
        "\n",
        "  out = Output(layout={'width': '50em'})\n",
        "  display(out)\n",
        "\n",
        "  with out:\n",
        "    for num in short_num:\n",
        "      uids = short_dict[num]\n",
        "      out.clear_output()\n",
        "      for uid in uids:\n",
        "        out.clear_output()\n",
        "        print_single_question(df, num, uid)\n",
        "        action = input(\"Do you want to delete this line? \\nType 'yes' or 'no'. \\n\")\n",
        "\n",
        "        if action=='yes':\n",
        "          df = df[df['uid'] != uid]\n",
        "\n",
        "  df.drop(columns=['short'], inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0fOHRFOGT-z"
      },
      "source": [
        "### 1.2.3 Deal with hanging Q or A\n",
        "Check for any hanging Q or A (Question or Answer that without their counterpart).\n",
        "\n",
        "Then the program goes through each instance, ask whether to delete the row or merge to the previous Q/A. In the case of a lone answer, it will be appended to the end of the previous Q/A as another answer. In the case of a lone question, it will be appended to the start of the next Q/A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6Qj_7zHL5Hb"
      },
      "outputs": [],
      "source": [
        "def clean_hanging(df):\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # getting all unique qa_num from the dataframe, then remove nan\n",
        "  qa_num = pd.unique(df['qa_num'])\n",
        "  qa_num = qa_num[~pd.isnull(qa_num)]\n",
        "\n",
        "  # loop through each question number, if there are less than 2 rows with such question number,\n",
        "  # then we have a hanging Q/A\n",
        "  hanging_num = []\n",
        "  for num in qa_num:\n",
        "    if df[df['qa_num'] == num].shape[0] < 2:\n",
        "      hanging_num.append(num)\n",
        "  hanging_dict = {num: df[df['qa_num'] == num]['uid'].tolist() for num in hanging_num}\n",
        "\n",
        "  out = Output(layout={'width': '50em'})\n",
        "  display(out)\n",
        "\n",
        "  with out:\n",
        "    for num in hanging_num:\n",
        "      for uid in hanging_dict[num]:\n",
        "        out.clear_output()\n",
        "        print_single_question(df, num, uid)\n",
        "        action = input(\"Do you want to merge or delete? \\nType 'merge' to merge to the previous/next A/Q. \\nType 'delete' to remove the row.\\n\")\n",
        "\n",
        "        if action == 'merge':\n",
        "          if df[df['uid'] == uid]['qa_type'].values[0] == 'Q':\n",
        "            # append to the next question\n",
        "            index = df[df['uid'] == uid].index[0]\n",
        "            next_qa_num = df.loc[index+1, 'qa_num']\n",
        "            next_qa_num_within = df.loc[index+1, 'qa_num_within']\n",
        "            df.loc[index, 'qa_num'] = next_qa_num\n",
        "            df.loc[index, 'qa_num_within'] = next_qa_num_within\n",
        "          elif df[df['uid'] == uid]['qa_type'].values[0] == 'A':\n",
        "            # append to the previous answer\n",
        "            index = df[df['uid'] == uid].index[0]\n",
        "            prev_qa_num = df.loc[index-1, 'qa_num']\n",
        "            prev_qa_num_within = df.loc[index-1, 'qa_num_within']\n",
        "            df.loc[index, 'qa_num'] = prev_qa_num\n",
        "            df.loc[index, 'qa_num_within'] = prev_qa_num_within\n",
        "        elif action == 'delete':\n",
        "          df = df[df['uid'] != uid]\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN6ZRb-iL6cq"
      },
      "source": [
        "### 1.2.4 Update the uid to reflect changes in question number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNFnCIHuMGWC"
      },
      "outputs": [],
      "source": [
        "def update_uid(df):\n",
        "  df.loc[~data['qa_num'].isna(), \"uid\"] = (\n",
        "      df[~data['qa_num'].isna()]\n",
        "      .assign(uid=lambda x: x.groupby('qa_num').cumcount() + 1)\n",
        "      .assign(uid=lambda x: [f\"{bank}_{int(quarter)}Q{int(year-2000)}_{qa_type}_{uid}.0\" for bank,quarter,year,qa_type,uid in zip(x['bank'], x['quarter'], x['year'], x['qa_type'], x['uid'])])\n",
        "  )\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WpXXmc9PNLq"
      },
      "source": [
        "### 1.2.5 Running the cleaning process and save the cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK7exShCPWv7"
      },
      "outputs": [],
      "source": [
        "data = clean_interruptions(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-JPUZgfPSUa"
      },
      "outputs": [],
      "source": [
        "data = clean_short(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO_YPGyyPYyj"
      },
      "outputs": [],
      "source": [
        "data = clean_hanging(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZdkpv1jPawC"
      },
      "outputs": [],
      "source": [
        "data = update_uid(data)\n",
        "# save the data\n",
        "data.to_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan_clean.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulJpPr25NLLk"
      },
      "source": [
        "### 1.2.6 Test on subset of dataset (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLbKMy9ANSED"
      },
      "outputs": [],
      "source": [
        "# load the uncleaned dataset, select the first 146 rows (corresopnd to the latest 2 quarters)\n",
        "test_data = pd.read_csv(processed_data_folder+\"/transcripts_tabular_JPMorgan.csv\")\n",
        "test_data = test_data.head(146)\n",
        "# manually add a row to showcase merging in same question\n",
        "new_row = {'uid':'Extra', 'qa_type':'A', 'qa_num_within':4, 'qa_num':4, 'qa_text':'Random Insert' }\n",
        "insert_index = 10\n",
        "test_data = pd.concat([test_data.iloc[:insert_index], pd.DataFrame([new_row]), test_data.iloc[insert_index:]]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8g5BaaqDXEN"
      },
      "outputs": [],
      "source": [
        "test_data = clean_interruptions(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZdrPKsmFWU9"
      },
      "outputs": [],
      "source": [
        "test_data = clean_short(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M4F3sqPORlT"
      },
      "outputs": [],
      "source": [
        "test_data = clean_hanging(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWXQ6NncOZXT"
      },
      "outputs": [],
      "source": [
        "test_data = update_uid(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1i0v1Psvk46"
      },
      "outputs": [],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gToxu1uPq78"
      },
      "source": [
        "## 1.3 Initial Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZvsq-u2joga"
      },
      "source": [
        "### 1.3.1 Transcripts overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZEPzw90sYX1"
      },
      "outputs": [],
      "source": [
        "# Defining a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Checking if the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Converting the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation from the text (including special quotation marks and apostrophes)\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "\n",
        "    # Removing numbers from the text\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenizing the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing stopwords from the text\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatizing the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi0NE_dTsgxc"
      },
      "outputs": [],
      "source": [
        "# load the cleaned dataset\n",
        "transcripts_df = pd.resad_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan_clean.csv\")\n",
        "\n",
        "# Preprocessing the 'qa_text' column in the transcripts data\n",
        "transcripts_df['qa_text_processed'] = transcripts_df['qa_text'].apply(preprocess_text)\n",
        "\n",
        "# Viewing the preprocessed data\n",
        "transcripts_df[['qa_text', 'qa_text_processed']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHMDSmuYsxUs"
      },
      "outputs": [],
      "source": [
        "# Pulling all words from the 'qa_text_processed' column into a list\n",
        "transcripts_all_words = [word for tokens in transcripts_df['qa_text_processed'] for word in tokens]\n",
        "\n",
        "# Calculating the frequency distribution of the words\n",
        "transcripts_freq_dist = FreqDist(transcripts_all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpvComTMsy0M"
      },
      "outputs": [],
      "source": [
        "# Getting the top 10 words and their frequencies\n",
        "transcripts_top_10 = transcripts_freq_dist.most_common(10)\n",
        "transcripts_words, transcripts_counts = zip(*transcripts_top_10)\n",
        "\n",
        "# Plotting the top 10 words in a barplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=list(transcripts_words), y=list(transcripts_counts), palette=\"viridis\")\n",
        "plt.title('Top 10 Words in the Transcripts')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id97qcU3s-O9"
      },
      "outputs": [],
      "source": [
        "# Combining the tokens from the 'qa_text_processed' column\n",
        "transcripts_text = ' '.join([' '.join(tokens) for tokens in transcripts_df['qa_text_processed']])\n",
        "\n",
        "# Generating the word cloud for the text\n",
        "transcripts_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(transcripts_text)\n",
        "\n",
        "# Plotting the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(transcripts_wordcloud, interpolation='bilinear')\n",
        "plt.title('Word Cloud for the Transcripts')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QozRjOARkxKs"
      },
      "source": [
        "Many of the words that appear are not hugely informative. As such, I will preprocess the text again removing extra words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_18oMA9gPU8"
      },
      "outputs": [],
      "source": [
        "# Defining a function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Checking if the input is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Converting the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing punctuation from the text (including special quotation marks and apostrophes)\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "\n",
        "    # Removing numbers from the text\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Tokenizing the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Defining stopwords and adding custom words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    custom_stop_words = {\"think\", \"going\", \"thats\", \"like\", \"bit\", \"thing\", \"yeah\",\n",
        "                         \"see\", \"would\", \"youre\", \"question\", \"could\", \"dont\",\n",
        "                         \"stuff\", \"jeremy\", \"lot\", \"betsy\", \"teresa\", \"michael\",\n",
        "                         \"hi\", \"hey\", \"hello\", \"maybe\", \"jamie\", \"go\", \"weve\"}\n",
        "    stop_words.update(custom_stop_words)\n",
        "\n",
        "    # Removing stopwords from the text\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatizing the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvgfvndolI0I"
      },
      "outputs": [],
      "source": [
        "# Preprocessing the 'qa_text' column in the transcripts data\n",
        "transcripts_df['qa_text_processed'] = transcripts_df['qa_text'].apply(preprocess_text)\n",
        "\n",
        "# Viewing the preprocessed data\n",
        "transcripts_df[['qa_text', 'qa_text_processed']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdR_pm2FlI0J"
      },
      "outputs": [],
      "source": [
        "# Pulling all words from the 'qa_text_processed' column into a list\n",
        "transcripts_all_words = [word for tokens in transcripts_df['qa_text_processed'] for word in tokens]\n",
        "\n",
        "# Calculating the frequency distribution of the words\n",
        "transcripts_freq_dist = FreqDist(transcripts_all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6euLPEglI0J"
      },
      "outputs": [],
      "source": [
        "# Getting the top 10 words and their frequencies\n",
        "transcripts_top_10 = transcripts_freq_dist.most_common(10)\n",
        "transcripts_words, transcripts_counts = zip(*transcripts_top_10)\n",
        "\n",
        "# Plotting the top 10 words in a barplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=list(transcripts_words), y=list(transcripts_counts), palette=\"viridis\")\n",
        "plt.title('Top 10 Words in the Transcripts')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DdB0lEylI0K"
      },
      "outputs": [],
      "source": [
        "# Combining the tokens from the 'qa_text_processed' column\n",
        "transcripts_text = ' '.join([' '.join(tokens) for tokens in transcripts_df['qa_text_processed']])\n",
        "\n",
        "# Generating the word cloud for the text\n",
        "transcripts_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(transcripts_text)\n",
        "\n",
        "# Plotting the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(transcripts_wordcloud, interpolation='bilinear')\n",
        "plt.title('Word Cloud for the Transcripts')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJmirMnAj8o6"
      },
      "source": [
        "### 1.3.2 1Q22 and 2Q24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LADe3eGjDwoR"
      },
      "source": [
        "Now, let's identify two intersting quarters to investigate based on the financial metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByrnQfvSDqD9"
      },
      "outputs": [],
      "source": [
        "# Getting the unique metric types\n",
        "unique_metric_types = metrics_df['metric_type'].unique()\n",
        "\n",
        "# Defining colors for each plot\n",
        "colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "# Plotting in a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 9))\n",
        "\n",
        "# Flattening axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Looping through metric types and corresponding axes\n",
        "for i, metric in enumerate(unique_metric_types):\n",
        "    subset = metrics_df[metrics_df['metric_type'] == metric]\n",
        "    axes[i].plot(subset['Q&FY'], subset['metric_value'], marker='o', color=colors[i])\n",
        "    axes[i].set_title(f'Metric: {metric}', fontsize=16, color=colors[i])\n",
        "    axes[i].set_xlabel('Quarter', fontsize=14)\n",
        "    axes[i].set_ylabel('Metric Value', fontsize=14)\n",
        "    axes[i].tick_params(axis='x', labelsize=12, rotation=45)\n",
        "    axes[i].tick_params(axis='y', labelsize=12)\n",
        "    axes[i].grid(visible=True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-fY_dbUD-ZD"
      },
      "source": [
        "Picking out two interesting dates to explore based on the charts above of key financial metrics:\n",
        "\n",
        "**1Q22**\n",
        "\n",
        "In 1Q22, the bank faced significant financial challenges, marked by a low CET1 capital ratio and declining net income and EPS. This suggests reduced profitability and shareholder returns. The increase in provisions for credit losses suggests heightened caution regarding potential loan defaults. Overall, this quarter indicates a period of financial strain and risk management.\n",
        "\n",
        "**2Q24**\n",
        "\n",
        "In 2Q24, the bank reached peak performance across several key metrics, with its CET1 capital ratio, net income, and EPS all at their highest levels. This suggests a strong capital position, robust profitability, and high returns to shareholders. Provisions for credit losses, however, remained elevated. This indicates a continued cautious approach toward potential credit risks. Overall, this quarter indicates a period of strong financial results although the bank is maintaining caution to guard against possible economic uncertainties or loan defaults.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvWXkd0BxdQC"
      },
      "outputs": [],
      "source": [
        "# Converting the date column to datetime\n",
        "transcripts_df['date'] = pd.to_datetime(transcripts_df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAtt4wv2vIAw"
      },
      "outputs": [],
      "source": [
        "# Creating seperate dataframes for the Apr-22 and Jul-24 Q&As\n",
        "apr_22 = transcripts_df[(transcripts_df['date'] >= '2022-04-01') & (transcripts_df['date'] < '2022-05-01')]\n",
        "jul_24 = transcripts_df[(transcripts_df['date'] >= '2024-07-01') & (transcripts_df['date'] < '2024-08-01')]\n",
        "\n",
        "# Pulling all words from qa_text_processed column into a list\n",
        "apr_22_all_words = [word for tokens in apr_22['qa_text_processed'] for word in tokens]\n",
        "jul_24_all_words = [word for tokens in jul_24['qa_text_processed'] for word in tokens]\n",
        "\n",
        "# Calculating the frequency distribution of the words from each dataset\n",
        "apr_22_freq_dist = FreqDist(apr_22_all_words)\n",
        "jul_24_freq_dist = FreqDist(jul_24_all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTyuF9dKvIAw"
      },
      "outputs": [],
      "source": [
        "# Getting the top 10 words and their frequencies from the Apr-22 Q&As\n",
        "apr_22_top_10 = apr_22_freq_dist.most_common(10)\n",
        "apr_22_words, apr_22_counts = zip(*apr_22_top_10)\n",
        "\n",
        "# Getting the top 10 words and their frequencies from the Jul-24 Q&As\n",
        "jul_24_top_10 = jul_24_freq_dist.most_common(10)\n",
        "jul_24_words, jul_24_counts = zip(*jul_24_top_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlqVUWkjvIAw"
      },
      "outputs": [],
      "source": [
        "# Creating a figure with 1 row and 2 columns of subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plotting the top 10 words in a barplot for the Apr-22 Q&As\n",
        "sns.barplot(x=list(apr_22_words), y=list(apr_22_counts), palette=\"viridis\", ax=ax1)\n",
        "ax1.set_title('Top 10 Words in the April 2022 Q&As')\n",
        "ax1.set_xlabel('Words')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
        "\n",
        "# Plotting the top 10 words in a barplot for the Jul-24 Q&As\n",
        "sns.barplot(x=list(jul_24_words), y=list(jul_24_counts), palette=\"viridis\", ax=ax2)\n",
        "ax2.set_title('Top 10 Words in the July 2024 Q&As')\n",
        "ax2.set_xlabel('Words')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VtVmZKbzoEN"
      },
      "outputs": [],
      "source": [
        "# Creating a dataframe excluding Apr-22 Q&As\n",
        "not_apr_22 = transcripts_df[(transcripts_df['date'] < '2022-04-01') | (transcripts_df['date'] >= '2022-05-01')]\n",
        "\n",
        "# Creating a dataframe excluding Jul-24 Q&As\n",
        "not_jul_24 = transcripts_df[(transcripts_df['date'] < '2024-07-01') | (transcripts_df['date'] >= '2024-08-01')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlUHpIwlzoEO"
      },
      "outputs": [],
      "source": [
        "# Getting unique words for the Apr-22 Q&As\n",
        "apr_22_unique_words = set(apr_22_freq_dist.keys()) - set(not_apr_22['qa_text_processed'].explode())\n",
        "apr_22_unique_counts = {word: apr_22_freq_dist[word] for word in apr_22_unique_words}\n",
        "apr_22_unique_df = pd.DataFrame(apr_22_unique_counts.items(), columns=['word', 'apr_22_frequency'])\n",
        "\n",
        "# Getting unique words for the Jul-24 Q&As\n",
        "jul_24_unique_words = set(jul_24_freq_dist.keys()) - set(not_jul_24['qa_text_processed'].explode())\n",
        "jul_24_unique_counts = {word: jul_24_freq_dist[word] for word in jul_24_unique_words}\n",
        "jul_24_unique_df = pd.DataFrame(jul_24_unique_counts.items(), columns=['word', 'jul_24_frequency'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PueNoYRHzoEO"
      },
      "outputs": [],
      "source": [
        "# Getting the top 10 unique words for Apr-22 and Jul-24\n",
        "apr_22_top_words = apr_22_unique_df.nlargest(10, 'apr_22_frequency')\n",
        "jul_24_top_words = jul_24_unique_df.nlargest(10, 'jul_24_frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6awu88lqzoEO"
      },
      "outputs": [],
      "source": [
        "# Setting up the plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Creating a bar plot for the top 10 Apr-22 exclusive words\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(data=apr_22_top_words, x='word', y='apr_22_frequency', palette='Blues')\n",
        "plt.title('Top 10 Unique Words Used in the April 2022 Q&As')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Creating a bar plot for the top 10 Jul-24 exclusive words\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(data=jul_24_top_words, x='word', y='jul_24_frequency', palette='Oranges')\n",
        "plt.title('Top 10 Unique Words Used in the July 2024 Q&As')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NKmuTTmSHO"
      },
      "source": [
        "In 1Q22, the term “russia-associated,” is likely tied to the Russia-Ukraine conflict. Similarly, the word “nickel” may indicate exposure to commodity price volatility, possibly linked to the Russia-Ukraine conflict.\n",
        "\n",
        "In 2Q24, words like “index” and “governor” indicate attention to macroeconomic indicators and regulatory guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZCL2vvRkHmi"
      },
      "source": [
        "### 1.3.3 Emerging risks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxy0ensxEygq"
      },
      "source": [
        "To exlpore emerging risks, let's focus on the last two quarters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwZNMDsUEv4R"
      },
      "outputs": [],
      "source": [
        "# Getting the unique metric types\n",
        "unique_metric_types = metrics_df['metric_type'].unique()\n",
        "\n",
        "# Defining colors for each plot\n",
        "colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "# Plotting in a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 9))\n",
        "\n",
        "# Flattening axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Looping through metric types and corresponding axes\n",
        "for i, metric in enumerate(unique_metric_types):\n",
        "    subset = metrics_df[metrics_df['metric_type'] == metric]\n",
        "    axes[i].plot(subset['Q&FY'], subset['metric_value'], marker='o', color=colors[i])\n",
        "    axes[i].set_title(f'Metric: {metric}', fontsize=16, color=colors[i])\n",
        "    axes[i].set_xlabel('Quarter', fontsize=14)\n",
        "    axes[i].set_ylabel('Metric Value', fontsize=14)\n",
        "    axes[i].tick_params(axis='x', labelsize=12, rotation=45)\n",
        "    axes[i].tick_params(axis='y', labelsize=12)\n",
        "    axes[i].grid(visible=True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoJRKjndFDsC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdh4ve3WpPhD"
      },
      "outputs": [],
      "source": [
        "# Creating a dataframs for the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24 = transcripts_df[(transcripts_df['date'] >= '2024-07-01') & (transcripts_df['date'] < '2024-11-01')]\n",
        "\n",
        "# Pulling all words from qa_text_processed column into a list\n",
        "Q23_FY24_all_words = [word for tokens in Q23_FY24['qa_text_processed'] for word in tokens]\n",
        "\n",
        "# Calculating the frequency distribution of the words from the dataset\n",
        "Q23_FY24_freq_dist = FreqDist(Q23_FY24_all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x5bZIgqpPhE"
      },
      "outputs": [],
      "source": [
        "# Getting the top 10 words and their frequencies from the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_top_10 = Q23_FY24_freq_dist.most_common(10)\n",
        "Q23_FY24_words, Q23_FY24_counts = zip(*Q23_FY24_top_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Hpt5vJpPhE"
      },
      "outputs": [],
      "source": [
        "# Setting up the plot\n",
        "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "# Plotting the top 10 words in a barplot for the 2Q24 and 3Q24 Q&As\n",
        "sns.barplot(x=list(Q23_FY24_words), y=list(Q23_FY24_counts), palette=\"viridis\", ax=ax1)\n",
        "ax1.set_title('Top 10 Words in the 2Q24 and 3Q24 Q&As')\n",
        "ax1.set_xlabel('Words')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGlSaLErpPhF"
      },
      "outputs": [],
      "source": [
        "# Creating a dataframe excluding 2Q24 and 3Q24 Q&As\n",
        "not_Q23_FY24 = transcripts_df[(transcripts_df['date'] < '2024-07-01') | (transcripts_df['date'] >= '2024-11-01')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L4Uo0xUpPhF"
      },
      "outputs": [],
      "source": [
        "# Getting unique words for the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_unique_words = set(Q23_FY24_freq_dist.keys()) - set(not_Q23_FY24['qa_text_processed'].explode())\n",
        "Q23_FY24_unique_counts = {word: Q23_FY24_freq_dist[word] for word in Q23_FY24_unique_words}\n",
        "Q23_FY24_unique_df = pd.DataFrame(Q23_FY24_unique_counts.items(), columns=['word', 'Q23_FY24_frequency'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHGwgMP-pPhF"
      },
      "outputs": [],
      "source": [
        "# Getting the top 10 unique words for the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_top_words = Q23_FY24_unique_df.nlargest(10, 'Q23_FY24_frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAobK854pPhF"
      },
      "outputs": [],
      "source": [
        "# Setting up the plot\n",
        "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "# Creating a bar plot for the top 10 2Q24 and 3Q24 exclusive words\n",
        "sns.barplot(data=Q23_FY24_top_words, x='word', y='Q23_FY24_frequency', palette='Blues')\n",
        "plt.title('Top 10 Unique Words Used in the 2Q24 and 3Q24 Q&As')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeOpb8cmtBcO"
      },
      "source": [
        "Words like “spike” and “trough” could reflect the recent fluctuations in net income and EPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9oFuggokNmF"
      },
      "outputs": [],
      "source": [
        "# Calculating word frequencies and total word count in 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_freq = transcripts_df[(transcripts_df['date'] >= '2024-07-01') & (transcripts_df['date'] < '2024-11-01')]\n",
        "Q23_FY24_word_counts = Q23_FY24_freq['qa_text_processed'].explode().value_counts()\n",
        "Q23_FY24_total_words = Q23_FY24_word_counts.sum()  # Total word count for 2Q24 & 3Q24\n",
        "\n",
        "# Calculating word frequencies and total word count in other quarters\n",
        "not_Q23_FY24_word_counts = not_Q23_FY24['qa_text_processed'].explode().value_counts()\n",
        "not_Q23_FY24_total_words = not_Q23_FY24_word_counts.sum()  # Total word count for other quarters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMV0BQkWkY0N"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame comparing relative frequencies\n",
        "word_comparison_df = pd.DataFrame({\n",
        "    'Q23_FY24_proportion': Q23_FY24_word_counts / Q23_FY24_total_words,\n",
        "    'other_quarters_proportion': not_Q23_FY24_word_counts / not_Q23_FY24_total_words\n",
        "}).fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RCJkNsukfyd"
      },
      "outputs": [],
      "source": [
        "# Adding a column for proportion difference\n",
        "word_comparison_df['proportion_difference'] = word_comparison_df['Q23_FY24_proportion'] - word_comparison_df['other_quarters_proportion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEoIUMVAkksz"
      },
      "outputs": [],
      "source": [
        "# Filtering for words that are relatively more frequent in 2Q24 and 3Q24\n",
        "higher_in_Q23_FY24 = word_comparison_df[word_comparison_df['proportion_difference'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv-mrjjjkq02"
      },
      "outputs": [],
      "source": [
        "# Selecting the top 10 words with the highest proportion difference\n",
        "top_higher_words = higher_in_Q23_FY24.nlargest(10, 'proportion_difference').reset_index().rename(columns={'qa_text_processed': 'word'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw3vX8nBkweM"
      },
      "outputs": [],
      "source": [
        "# Plotting the top words by proportional difference\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "sns.barplot(data=top_higher_words, x='word', y='proportion_difference', palette='Purples')\n",
        "plt.title('Top 10 Words with Higher Proportion in 2Q24 and 3Q24 Compared to Other Quarters')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Proportional Difference')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEOI_kEotWB8"
      },
      "source": [
        "A higher usage of \"capital\" could relate to how the bank has maintained a high CET1 capital ratio, indicating a strong capital position.\n",
        "\n",
        "A higher usage of \"nii\" (net interest income) could relate to the recent fluctuations in net income and EPS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SOpP_wkpAuc"
      },
      "source": [
        "# 2 Selecting Models and Scalability Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKSm1k4Xo3Sa"
      },
      "source": [
        "## 2.0 Creation of evaluation dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLTiHpflVFn-"
      },
      "source": [
        "To evaluate model performance and facilitate in the decision of choosing the best model for further analysis, we create two datasets:\n",
        "\n",
        "- Synthetic Data: Text data with predefined sentiment, topic, and evasion labels was generated by GPT-4 and extensively refined manually to ensure accuracy. This dataset allowed us to test the models against controlled baselines and evaluate their precision in tasks with known outcomes. This file is created manually and uploaded in the clean data folder.\n",
        "\n",
        "- Ground Truth Data: Two transcripts’ worth of texts were randomly sampled and manually annotated for sentiment, topic, and question evasion status. This dataset provided a benchmark for assessing the models' real-world performance. The code selected random rows to form a ground truth dataset, saved as an .xlsx file. We then manually labelled them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooPwLos6WkEm"
      },
      "source": [
        "### 2.0.1 Selecting ground truth data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT8Mj7_-WoYW"
      },
      "outputs": [],
      "source": [
        "def create_ground_truth(folder_path, output_folder_path):\n",
        "    # define the bank of interest\n",
        "    banks = ['JPMorgan']\n",
        "    print(\"Which bank's data do you wish to label?\")\n",
        "    bank = input(banks)\n",
        "\n",
        "    label_types = ['sentiment', 'topic', 'Q_outcome']\n",
        "\n",
        "    # load ground truth data (if it exists), else create the file\n",
        "\n",
        "    print(\"Creating a new ground truth dataset...\")\n",
        "    transcript_csv_path = os.path.join(folder_path, f\"/transcripts_tabular_{bank}_clean.csv\")\n",
        "    transcripts_all = pd.read_csv(transcript_csv_path)\n",
        "\n",
        "    transcripts_qa = transcripts_all[transcripts_all.section=='questions_answers'].reset_index()\n",
        "\n",
        "    # consider only the qa_num where there are both Q and A (i.e. at least two rows per qa_num)\n",
        "    qa_nums_all = transcripts_qa.qa_num.value_counts().loc[lambda x: x > 1].index.unique()\n",
        "\n",
        "    median_num_Q = int(transcripts_all.qa_num_within.median())\n",
        "    print(f\"Median number of Q&As per transcript: {median_num_Q}\")\n",
        "\n",
        "    # how many transcripts should the Qs represent?\n",
        "    NUM_TRANSCRIPTS = 2\n",
        "    SEED = 42\n",
        "\n",
        "    # sample unique Q&As\n",
        "    random.seed(SEED)\n",
        "    qa_num_sample = random.sample(list(qa_nums_all), int(NUM_TRANSCRIPTS*median_num_Q))\n",
        "    ground_truth = transcripts_all[transcripts_all.qa_num.isin(qa_num_sample)]\n",
        "    ground_truth = ground_truth[['uid', 'qa_type', 'qa_num', 'qa_text']]\n",
        "\n",
        "    for col in label_types:\n",
        "      ground_truth[f\"true_{col}\"] = np.nan\n",
        "    ground_truth_path = os.path.join(output_folder_path, f\"/ground_truth_{bank}_manual.xlsx\")\n",
        "    ground_truth.to_excel(ground_truth_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shYsN8U2WfZn"
      },
      "outputs": [],
      "source": [
        "create_ground_truth(processed_data_folder, output_data_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ezEcZjpXJqP"
      },
      "source": [
        "The file is saved as an Excel file in the output_data. We then manually labelled the FinBERT topic, sentiment and evasion for each text on Google Sheet, where ChatGPT faciliated in the decision making. **This altered (human-annotated) ground truth file was then saved in the processed_data folder and used for all subsequent ground truth analyses**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnBIjXgvMMD4"
      },
      "source": [
        "## 2.1 Phi 3.5 for summarisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-JTsNL3sVjw"
      },
      "source": [
        "**SECTION TAKES UP TO 32GB RAM TO RUN - A100 GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il6XBUiFmEh3"
      },
      "source": [
        "### 2.1.0 Phi 3.5 initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft9IgUdhlWWK"
      },
      "outputs": [],
      "source": [
        "# Initialise the pipeline - note T4 GPU does not contain enough RAM, so use CPU and ignore warning OR run on A100 GPU\n",
        "pipe = pipeline(\"text-generation\", model=\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzzDsIF6lnJ-"
      },
      "source": [
        "### 2.1.1 Summarisation function using Phi 3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKXE_xtkltwn"
      },
      "source": [
        "The prompt we are using is\n",
        "\n",
        "```Summarise the following text in a consistent, concise format. Limit to 1-2 sentences focusing only on main financial themes,metrics, and indicators relevant to financial performance. Avoid interpretations, bullet points, and variable formatting.Do not allow style drift in the answers. \\n\\nText:\\n{text}\\n\\nSummary:```\n",
        "\n",
        "where ```{text}``` is the text from each question or answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lum0hk1llbnv"
      },
      "outputs": [],
      "source": [
        "# Summarisation function\n",
        "def phi_summarise(input_df, input_col, batch_size=8):\n",
        "  \"\"\"\n",
        "  Function to Summarise text when given a pre-processed Q&A table as input\n",
        "    - Focusses on financial themes\n",
        "    - Summarises in 1-2 sentences\n",
        "  \"\"\"\n",
        "\n",
        "  start_time = time.time()\n",
        "  total_count = len(input_df)\n",
        "  x = 0\n",
        "\n",
        "  # Initialise a new column for summarised text\n",
        "  input_df['summarised_text'] = \"\"\n",
        "\n",
        "    # Process the DataFrame in batches\n",
        "  for start in range(0, total_count, batch_size):\n",
        "      end = min(start + batch_size, total_count)\n",
        "      batch_texts = input_df[input_col][start:end].tolist()\n",
        "\n",
        "      # Define prompt for each text in the batch\n",
        "      prompt = [\n",
        "          (\n",
        "              f\"Summarise the following text in a consistent, concise format. Limit to 1-2 sentences focusing only on main financial themes, \"\n",
        "              f\"metrics, and indicators relevant to financial performance. Avoid interpretations, bullet points, and variable formatting. \"\n",
        "              f\"Do not allow style drift in the answers. \\n\\nText:\\n{text}\\n\\nSummary:\"\n",
        "          )\n",
        "          for text in batch_texts\n",
        "      ]\n",
        "\n",
        "      print(f\"Processing batch {start // batch_size + 1}/{(total_count + batch_size - 1) // batch_size}\")\n",
        "\n",
        "      # Run the model on the batch prompt\n",
        "      batch_summaries = pipe(prompt, max_new_tokens=75, do_sample=False)\n",
        "\n",
        "      # Extract summaries and clean up the text\n",
        "      for i, summary in enumerate(batch_summaries):\n",
        "          generated_text = summary[0]['generated_text'].replace(prompt[i], \"\").replace(\"Text:\\n\", \"\").replace(\"Summary:\", \"\").strip()\n",
        "          first_line = generated_text.split('\\n', 1)[0].strip()\n",
        "          cleaned_text = re.sub(r\"#[-•]\\s*\", \"\", first_line).strip()\n",
        "\n",
        "          # Update Input DataFrame with the cleaned summary\n",
        "          input_df.at[start + i, 'summarised_text'] = cleaned_text\n",
        "          x += 1\n",
        "\n",
        "  # Calculate time for full dataset\n",
        "  end_time = time.time()\n",
        "  time_taken = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken for {total_count} rows: {round(time_taken/60, 2)} minutes\")\n",
        "  print(f\"Estimate for all transcripts (927): {round((time_taken/total_count * 927)/60/60, 2)} hours\")\n",
        "\n",
        "  return input_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojA1RyADqYPI"
      },
      "source": [
        "### 2.1.2 Generate summarised text for Q&A tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwRy0S_TqVSn"
      },
      "outputs": [],
      "source": [
        "# Load ground truth files\n",
        "ground_truth_df = pd.read_excel(processed_data_folder + \"/ground_truth_JPMorgan_manual.xlsx\")\n",
        "\n",
        "# Load full Q&A table\n",
        "qa_df = pd.read_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan_clean.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hgWgQPdwa8rd"
      },
      "outputs": [],
      "source": [
        "# Run summarisation function on Ground Truth Q&A dataset (not aggregated)\n",
        "summarised_df_gt = phi_summarise(ground_truth_df, 'qa_text', 8)\n",
        "display(summarised_df_gt.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kny2wDiVq0CX"
      },
      "outputs": [],
      "source": [
        "# Save summarised ground truth file\n",
        "summarised_df_gt.to_excel(processed_data_folder + \"/phi_ground_truth_summarised.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vrwteR2Ka8rd"
      },
      "outputs": [],
      "source": [
        "# Run summarisation function on full Q&A tabular dataset (note that 'N' values have been removed, so only Q and A remain)\n",
        "summarised_df_full = phi_summarise(qa_df, 'qa_text', 8)\n",
        "\n",
        "# View results\n",
        "display(summarised_df_full.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rYzc5FNr2wn"
      },
      "outputs": [],
      "source": [
        "# Save summarised file\n",
        "summarised_df_gt.to_excel(processed_data_folder + \"/phi_fulltable_summarised.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEaQZLtIsdcn"
      },
      "source": [
        "These summarised tables (ground truth and full) will be passed as input to finBERT and compared to analysis on non-summarised table data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0tUuTSOrxk-"
      },
      "source": [
        "## 2.2 Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOCr0o1zsQ-I"
      },
      "source": [
        "We compare two sentiment classification models, \"yiyanghkust/finbert-tone\" and \"soleimanian/financial-roberta-large-sentiment\", both from huggingface. Both returns either 'positive', 'negative' or 'neutral' where the first model capitalises the first letter.\n",
        "\n",
        "Finbert-tone is a fine-tuned version of FinBert for sentiment classification on 10,000 manually annotated sentences from analyst reports. On the other hand, the Roberta model was trained on a large corpus including CSR reports, ESG news and eaernings call transcripts.\n",
        "\n",
        "We are using the ground truth dataset to evluating which model is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhzWpRmdsLU_"
      },
      "source": [
        "### 2.2.0 Load the ground truth dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLmyvda6t9Xx"
      },
      "source": [
        "A new column in the evaluation dataset is created where the labels are converted to integer. (positive to 1, negative to -1, neutral to 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mX9TfdjC3_Eq"
      },
      "outputs": [],
      "source": [
        "score_dict = {'neutral':0, 'positive':1, 'negative':-1}\n",
        "eval_data = pd.read_excel(processed_data_folder + \"/ground_truth_JPMorgan_manual.xlsx\")\n",
        "eval_data.drop(['qa_type', 'qa_num', 'true_topic', 'true_Q_outcome'], inplace=True, axis=1)\n",
        "eval_data['true_score'] = eval_data['true_sentiment'].map(score_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvgnmTcV4QNh"
      },
      "source": [
        "### 2.2.1 Class for running sentiment classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i4T5WMx4zhh"
      },
      "source": [
        "There are multiple ways to use the sentiment analysis models. The most striaght forward way is getting the label for each of the text, and compare the accuracy, precision, recall and f1_score of the two models on the ground truth. This is stored in a column with suffix '_sentiment'.\n",
        "\n",
        "Alternatively, we can use the probability outputs of the label to compute a numeric score between -1 and 1. For each text, the formula would simply be $$ \\text{score} = \\text{probability of positive} - \\text{probability of negative.}$$ This is stored in a column with suffix '_score'. Then by converting the truth label to -1, 0 and 1, we can compare the MSE of the two models.\n",
        "\n",
        "Another way of quantifying the output is breaking each text into sentences and feed each sentence to the model. Then a score can be computed from the number of positive and negative sentences. The formula is $$ \\text{score} = \\frac{\\text{number of positive sentences} - \\text{number of negative sentences}}{\\text{number of positive sentences} + \\text{number of negative sentences}}.$$ This is stored in a column with suffix '_average_sentence_label_score'. We can then compare the the MSE of the two models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpk9P3bR4XaB"
      },
      "outputs": [],
      "source": [
        "## Helper functions\n",
        "def get_quantiles(num_list, num_chunks):\n",
        "  \"\"\"\n",
        "  Calculate approximate quantiles from a list of numbers, dividing the list into equal chunks\n",
        "\n",
        "  Args:\n",
        "    num_list (list of int or float): A list of numeric values to be split into approximately equal chunks, pre-sorted.\n",
        "    num_chunks (int): Number of chunks the list to be split into.\n",
        "\n",
        "  Returns:\n",
        "    list of float or int: A list of values from 'num_list' that correspond to approximate quantiles.\n",
        "\n",
        "  \"\"\"\n",
        "  quantiles = [(i + 1) / num_chunks for i in range(num_chunks - 1)]\n",
        "\n",
        "  selected_positions = []\n",
        "  for q in quantiles:\n",
        "      if num_list:\n",
        "          pos_index = int(len(num_list) * q) - 1\n",
        "          if pos_index >= 0:\n",
        "              selected_positions.append(num_list[min(pos_index, len(num_list) - 1)])\n",
        "  return selected_positions\n",
        "\n",
        "\n",
        "def chunk_text(text, num_chunks):\n",
        "  \"\"\"\n",
        "  Split a given text into approximately equal chunks based on positions of periods.\n",
        "\n",
        "  Args:\n",
        "    text (str): The text to be split into chunks.\n",
        "    num_chunks (int): The number of chunks to create.\n",
        "\n",
        "  Returns:\n",
        "    list of str: A list of text chunks.\n",
        "  \"\"\"\n",
        "  positions = [index for index, char in enumerate(text) if char == '.']\n",
        "  if positions:\n",
        "      split_positions = get_quantiles(positions, num_chunks)\n",
        "\n",
        "      last_position = 0\n",
        "      chunks = []\n",
        "      for pos in split_positions:\n",
        "          chunks.append(text[last_position:pos + 1].strip())\n",
        "          last_position = pos + 1\n",
        "      chunks.append(text[last_position:].strip())\n",
        "      return chunks\n",
        "\n",
        "\n",
        "def compute_score_from_prob(response):\n",
        "  \"\"\"\n",
        "  Compute a sentiment score based on probabilities for positive and negative labels of a response.\n",
        "  The formula is probability of positive - probability of negative.\n",
        "\n",
        "  Args:\n",
        "    response (list of dict): A list of dictionaries where each dictionary contains a `label` and a `score`. It would work on any classification response from huggingface.\n",
        "\n",
        "  Returns:\n",
        "    float: The computed sentiment score.\n",
        "  \"\"\"\n",
        "  positive_score = next(item['score'] for item in response if item['label'].lower() == 'positive')\n",
        "  negative_score = next(item['score'] for item in response if item['label'].lower() == 'negative')\n",
        "  return positive_score - negative_score\n",
        "\n",
        "\n",
        "def split_string_by_punctuation(text):\n",
        "  \"\"\"\n",
        "  Split a given text into sentences based on specific punctuation marks, while avoiding splitting on periods in abbreviations like i.e. and U.S..\n",
        "\n",
        "  Args:\n",
        "    text (str): The text to be split into sentences.\n",
        "\n",
        "  Returns:\n",
        "    list of str: A list of sentences.\n",
        "  \"\"\"\n",
        "  punct_regex = r\"(?=\\S)(?:i.e.|J.P.|U.S.|ex.|[A-Z][a-z]{0,3}\\.|[^.?!]|\\.(?!\\s+[A-Z]))*.?\"\n",
        "  return re.findall(punct_regex, text)\n",
        "\n",
        "\n",
        "def compute_label_from_score(score):\n",
        "  if score > 0.2:\n",
        "      return 'positive'\n",
        "  elif score < -0.2:\n",
        "      return 'negative'\n",
        "  else:\n",
        "      return 'neutral'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hPrfRiJ4b8q"
      },
      "outputs": [],
      "source": [
        "## Class\n",
        "class Model:\n",
        "  \"\"\"\n",
        "  Base class for using huggingface model\n",
        "\n",
        "  Args:\n",
        "    name (str): Custom name of the model to be used in dataframe heading.\n",
        "    huggingface_model (str): The name of the model on huggingface hub, has a structure of {creator}/{model_name}\n",
        "    task (str): The machine learning task the model performs (should be one of many from huggingface)\n",
        "  \"\"\"\n",
        "  def __init__(self, name, huggingface_model, task):\n",
        "      self.name = name\n",
        "      self.huggingface_model = huggingface_model\n",
        "      self.task = task\n",
        "      self.model = pipeline(self.task, model=self.huggingface_model)\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(self.huggingface_model)\n",
        "      self.token_max_length = self.model.model.config.max_position_embeddings\n",
        "\n",
        "  def get_model_response_for_df(self, data, column):\n",
        "      pass\n",
        "\n",
        "\n",
        "class ClassificationModel(Model):\n",
        "    def __init__(self, name, huggingface_model):\n",
        "        super().__init__(name, huggingface_model, \"text-classification\")\n",
        "\n",
        "    def get_model_response_for_df(self, data, column):\n",
        "        data[self.name + '_sentiment'], data[self.name + '_score'] = zip(\n",
        "            *data.apply(lambda x: self.get_model_response(x, column), axis=1))\n",
        "        return data\n",
        "\n",
        "    def get_model_response(self, row, column):\n",
        "        input = row[column]\n",
        "        token_length = len(self.tokenizer(input)['input_ids'])\n",
        "        if token_length <= self.token_max_length:\n",
        "            response = self.model(input, top_k=3)\n",
        "            label = response[0]['label'].lower()\n",
        "            score = compute_score_from_prob(response)\n",
        "            return label, score\n",
        "\n",
        "        else:\n",
        "            chunk_number = token_length // self.token_max_length + 1\n",
        "            chunks = chunk_text(input, chunk_number)\n",
        "\n",
        "            for chunk in chunks:\n",
        "                if len(self.tokenizer(chunk)['input_ids']) > self.token_max_length:\n",
        "                    chunk_number = len(self.tokenizer(chunk)['input_ids']) // self.token_max_length + 1\n",
        "                    chunks.extend(chunk_text(chunk, chunk_number))\n",
        "                    chunks.remove(chunk)\n",
        "\n",
        "            responses = self.model(chunks, top_k=3)\n",
        "            scores = [compute_score_from_prob(response) for response in responses]\n",
        "            score = sum(scores) / len(scores)\n",
        "            label = compute_label_from_score(score)\n",
        "\n",
        "            return label, score\n",
        "\n",
        "    def get_model_response_from_sentences_df(self, data, column):\n",
        "        data[self.name + '_average_sentence_label_score'] = data.apply(\n",
        "            lambda x: self.get_model_response_from_sentences(x, column), axis=1)\n",
        "        return data\n",
        "\n",
        "    def get_model_response_from_sentences(self, row, column):\n",
        "        input = row[column]\n",
        "        sentences = split_string_by_punctuation(input)\n",
        "        num_pos = 0\n",
        "        num_neg = 0\n",
        "        for sentence in sentences:\n",
        "            response = self.model(sentence)[0]['label'].lower()\n",
        "            if response == 'positive':\n",
        "                num_pos += 1\n",
        "            if response == 'negative':\n",
        "                num_neg += 1\n",
        "\n",
        "        if num_pos == 0 and num_neg == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return (num_pos - num_neg) / (num_pos + num_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MWM0gBy4lXx"
      },
      "source": [
        "### 2.2.2 Comparing the two models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmL9VHfp43g6"
      },
      "outputs": [],
      "source": [
        "def get_model_accuracy(model_name, data):\n",
        "  true_response = data['true_sentiment'].to_list()\n",
        "  model_response = data[model_name+'_sentiment'].to_list()\n",
        "  cf = ConfusionMatrixDisplay(confusion_matrix(true_response, model_response), display_labels=['negative', 'neutral', 'positive'])\n",
        "  cf.plot()\n",
        "  colorbar = cf.ax_.images[0].colorbar\n",
        "  colorbar.set_label('Frequency')\n",
        "  plt.title(f\"Confusion matrix for {model_name}\")\n",
        "  plt.show()\n",
        "  print(classification_report(true_response, model_response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A8b_Lm4430q"
      },
      "outputs": [],
      "source": [
        "# Load the two models and run them on the ground truth data set\n",
        "finbert_model = ClassificationModel(\"finbert-tone\", \"yiyanghkust/finbert-tone\")\n",
        "eval_data = finbert_model.get_model_response_for_df(eval_data, 'qa_text')\n",
        "roberta_model = ClassificationModel(\"financial-roberta-large\", \"soleimanian/financial-roberta-large-sentiment\")\n",
        "eval_data = roberta_model.get_model_response_for_df(eval_data, 'qa_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBYX_Fep5BPx"
      },
      "outputs": [],
      "source": [
        "# save the results of ground truth data set in the output folder\n",
        "eval_data.to_csv(output_data_folder + \"/sentiment_eval_result_ground_truth.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woq299k4pQDw"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from ipywidgets import Output, HBox\n",
        "\n",
        "out1 = Output()\n",
        "out2 = Output()\n",
        "\n",
        "with out1:\n",
        "  get_model_accuracy(\"finbert-tone\", eval_data)\n",
        "with out2:\n",
        "  get_model_accuracy(\"financial-roberta-large\", eval_data)\n",
        "display(HBox([out1, out2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp10o4GDfb8l"
      },
      "source": [
        "From the result, we can see that the Roberta model has a slightly higher accuracy of 67% compared to FinBert's 65%. Although the model has similar f1-score for neutral, FinBert's precision and recall are unbalanced, suggesting that the FinBert model is quite conservative and predict most labels as 'neutral'. This is also reflected in the recall for 'positive' and 'negative'.\n",
        "\n",
        "For our analysis, we are more interested in the positive and negative classifications than the neutral ones, so it is better to choose the Roberta model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgoQ_pTLlru_"
      },
      "source": [
        "### 2.2.3 Running the model on the full data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URYN9v6Ul0Gm"
      },
      "outputs": [],
      "source": [
        "# load the full dataset\n",
        "full_data = pd.read_csv(processed_data_folder + \"transcripts_tabular_JPMorgan_clean.csv\")\n",
        "full_data = full_data[full_data['qa_type'].isin(['Q', 'A'])]\n",
        "# running the model\n",
        "roberta_model = ClassificationModel(\"financial-roberta-large\", \"soleimanian/financial-roberta-large-sentiment\")\n",
        "full_data = roberta_model.get_model_response_for_df(full_data, 'qa_text')\n",
        "# save the response\n",
        "full_data.to_csv(output_data_folder + \"/sentiment_full_result.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I1Z_ZVQp4C1"
      },
      "source": [
        "## 2.3 Topic modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTGRiVy--00"
      },
      "source": [
        "### 2.3.1 FinBERT classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4sdACDSgA_e"
      },
      "source": [
        "#### 2.3.1.0 Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__nlxJNef3Ft"
      },
      "outputs": [],
      "source": [
        "# connecting to huggingface\n",
        "huggingface_token = userdata.get(\"huggingface_token\")\n",
        "!huggingface-cli login --token $huggingface_token\n",
        "\n",
        "# loading the tokeniser and the model\n",
        "finbert_topic_tokeniser = AutoTokenizer.from_pretrained(\"nickmuchi/finbert-tone-finetuned-finance-topic-classification\")\n",
        "finbert_topic_model = AutoModelForSequenceClassification.from_pretrained(\"nickmuchi/finbert-tone-finetuned-finance-topic-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-mEmNLwgHz-"
      },
      "outputs": [],
      "source": [
        "# define a label dictionary to help us interpret the labels predicted by the model\n",
        "# (obtained from HuggingFace)\n",
        "id2label= {\n",
        "    0: \"Analyst Update\",\n",
        "    1: \"Fed | Central Banks\",\n",
        "    2: \"Company | Product News\",\n",
        "    3: \"Treasuries | Corporate Debt\",\n",
        "    4: \"Dividend\",\n",
        "    5: \"Earnings\",\n",
        "    6: \"Energy | Oil\",\n",
        "    7: \"Financials\",\n",
        "    8: \"Currencies\",\n",
        "    9: \"General News | Opinion\",\n",
        "    10: \"Gold | Metals | Materials\",\n",
        "    11: \"IPO\",\n",
        "    12: \"Legal | Regulation\",\n",
        "    13: \"M&A | Investments\",\n",
        "    14: \"Macro\",\n",
        "    15: \"Markets\",\n",
        "    16: \"Politics\",\n",
        "    17: \"Personnel Change\",\n",
        "    18: \"Stock Commentary\",\n",
        "    19: \"Stock Movement\"\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX0L1GQzHO1v"
      },
      "outputs": [],
      "source": [
        "# Check token lengths\n",
        "def plot_finbert_token_lengths(data_folder, finbert_tokeniser, max_length=512,\n",
        "                               qa_only=True, summarised=False, appdx=\"\"):\n",
        "    \"\"\"\n",
        "    given the path to the data folder and the FinBERT tokeniser,\n",
        "    this function returns an array of token lengths\n",
        "    and plots their distribution\n",
        "\n",
        "    max_length (int) : max token length - will be plotted!\n",
        "    qa_only (bool) : keeps only non-nan qa_num if true\n",
        "    \"\"\"\n",
        "    banks = ['JPMorgan']\n",
        "    print(\"Which bank's data do you wish to label?\")\n",
        "    bank = input(banks)\n",
        "\n",
        "    # load the ddata\n",
        "    if summarised:\n",
        "        summarised_text_path = os.path.join(data_folder, f\"phi_fulltable{appdx}.xlsx\")\n",
        "        df = pd.read_excel(summarised_text_path)\n",
        "        df = df.drop(['qa_text'], axis=1)\n",
        "        df.rename(columns={'summarised_text': 'qa_text'}, inplace=True)\n",
        "    else:\n",
        "        transcript_csv_path = os.path.join(data_folder, f\"transcripts_tabular_{bank}_clean.csv\")\n",
        "        df = pd.read_csv(transcript_csv_path)\n",
        "    if qa_only:\n",
        "      df = df[~df.qa_num.isna()].reset_index(drop=True)\n",
        "    arr = df.qa_text\n",
        "\n",
        "    token_lengths = []\n",
        "    # estimate token length without truncation\n",
        "    for txt in arr:\n",
        "        inputs_nontruncated = finbert_tokeniser(\n",
        "                                    txt,\n",
        "                                    return_tensors=\"pt\",\n",
        "                                    truncation=False,\n",
        "                                    padding=False\n",
        "                                    )\n",
        "        token_lengths.append(len(inputs_nontruncated[0]))\n",
        "\n",
        "    fig, ax = plt.subplots(1,1, figsize=(3,3))\n",
        "    sns.histplot(token_lengths, ax =ax)\n",
        "    ax.vlines(max_length, 0, 140, ls='dashed', color='black')\n",
        "    ax.set_xlabel(\"Token length\")\n",
        "    ax.set_ylabel(\"Number of texts\")\n",
        "\n",
        "    num_too_long = (np.array(token_lengths)>max_length).sum()\n",
        "    print(f\"Max token length is exceeded by {num_too_long} ({num_too_long*100/arr.shape[0]:.0f}%) entries\")\n",
        "\n",
        "plot_finbert_token_lengths(processed_data_folder, finbert_topic_tokeniser, max_length=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgeNpfnzgvBt"
      },
      "source": [
        "FinBERT can only handle inputs of up to 512 tokens in length. If our texts are substantially longer, truncation could diminish the accuracy of the model. Therefore, we will begin by performing just the tokenisation step to check how long the data is.\n",
        "\n",
        "Evidently, only 3% of entries exceed token length, so this is not a major problem. Still, we will include chunking at least as an option in our FinBERT topic assignment function. Depending on classification accuracy, it might even be of interest to consider shorter than max allowed token max lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYiFJC1chIf1"
      },
      "source": [
        "#### 2.3.1.1 Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emEq-LaS4qju"
      },
      "source": [
        "##### 2.3.1.1.1 Function for label prediction\n",
        "We will define a function `get_finbert_topics()` that will take a string input (one question or answer) and generate:\n",
        "- predicted label ID\n",
        "- the corresponding label\n",
        "- logits for all possible topics\n",
        "\n",
        "If `chunking=True`, texts longer than `max_length` will be split into equal-sized chunks and a label will be predicted for each chunk. If the chunks have different labels, all will be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6sLXYL-hzcd"
      },
      "outputs": [],
      "source": [
        "# function for label prediction\n",
        "def get_finbert_topics(txt, finbert_tokeniser, finbert_model, label_dict,\n",
        "                       chunking=False, max_length=512):\n",
        "    \"\"\"\n",
        "    given an input string, tokeniser, model, and a dictionary of labels,\n",
        "    this function returns the predicted label id, predicted label, and the full\n",
        "    array of logits for that input string\n",
        "\n",
        "    chunking (bool): if True, texts longer than 512 tokens are split into the\n",
        "            minimum number of ~equal-size chunks possible for each chunk to be\n",
        "            smaller than 512 tokens. A label is then predicted for each chunk.\n",
        "            If the labels do not match, the label gets assigned based on the max\n",
        "            logit across chunks.\n",
        "    \"\"\"\n",
        "    MAX_LENGTH = max_length\n",
        "\n",
        "    if chunking:\n",
        "        # estimate token length without truncation\n",
        "        inputs_nontruncated = finbert_tokeniser(\n",
        "                                    txt,\n",
        "                                    return_tensors=\"pt\",\n",
        "                                    truncation=False,\n",
        "                                    padding=False\n",
        "                                    )\n",
        "        token_length = len(inputs_nontruncated[0])\n",
        "        if token_length>MAX_LENGTH:\n",
        "            print(f\"Token exceeds max length: {txt[:100]}\")\n",
        "            num_chunks = (token_length//MAX_LENGTH)+1\n",
        "            txt_split = txt.split(\" \")\n",
        "            chunk_length_str=int(np.ceil(len(txt_split)/num_chunks))\n",
        "            if len(txt_split)%num_chunks==1:  # odd len(txt_split)\n",
        "              chunk_length_str += 1\n",
        "\n",
        "            txt_list = [\" \".join(txt_split[i:i + chunk_length_str]) for i in range(0, len(txt_split), chunk_length_str)]\n",
        "        else:\n",
        "            txt_list = [txt]\n",
        "    else:\n",
        "        txt_list = [txt]\n",
        "\n",
        "    predicted_label_ids = []\n",
        "    predicted_logits = []\n",
        "    for i, txt_chunk in enumerate(txt_list):\n",
        "        inputs = finbert_tokeniser(\n",
        "            txt_chunk,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=MAX_LENGTH\n",
        "            )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = finbert_model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # get the predicted label\n",
        "        predicted_label_ids.append(torch.argmax(logits, dim=1).item())\n",
        "        predicted_logits.append(logits.detach().cpu().numpy().flatten())\n",
        "\n",
        "    # check if predicted labels are the same for all chunks\n",
        "    # if so, assign a single label to the input row\n",
        "    if len(set(predicted_label_ids))==1:\n",
        "        predicted_label = label_dict[predicted_label_ids[0]]\n",
        "        return predicted_label_ids[0], predicted_label, predicted_logits[0], None\n",
        "    else:\n",
        "        # return all predictions, as well as the text chunks\n",
        "        predicted_labels = [label_dict[predicted_label_ids[i]] for i in range(len(predicted_label_ids))]\n",
        "        return predicted_label_ids, predicted_labels, predicted_logits, txt_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJzKVJqu4y9e"
      },
      "source": [
        "##### 2.3.1.1.2 Function for label storage\n",
        "Next, we will define a function `process_finbert` that:\n",
        "- checks if a labelled dataset with the same chunking and max length already exists in the folder, else it will create a new file\n",
        "- runs `get_finbert_topics()` and handles multiple chunks\n",
        "  - if multiple labels are returned, the first will replace the existing row and the others will be appended to the labelled dataset as new rows. The `uid` of such new rows will be the same as that of the existing row to allow one-sided merging of this and the original dataset.\n",
        "- allows for labelling of only the Q&A sections (`qa_only=True`) or the management discussion section as well (`qa_only=False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AljOybEph7r9"
      },
      "outputs": [],
      "source": [
        "# Function for label storage\n",
        "def process_finbert(finbert_tokeniser, finbert_model, save_folder, label_dict,\n",
        "                    data_folder=None, chunking=False, max_length=512, qa_only=False,\n",
        "                    summarised=False, synthetic=False, appdx=\"\"):\n",
        "    # define the bank of interest\n",
        "    banks = ['JPMorgan']\n",
        "    print(\"Which bank's data do you wish to label?\")\n",
        "    bank = input(banks)\n",
        "\n",
        "    # define the columns of interest\n",
        "    cols_to_add = ['finbert_topic_id', 'finbert_topic_label']\n",
        "    [cols_to_add.append(f\"topic_{i}_logit\") for i in label_dict.keys()]\n",
        "\n",
        "    # load (partly) labelled dataset (if it exists), else create the file\n",
        "    finbert_topics_path = os.path.join(save_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    if os.path.exists(finbert_topics_path):\n",
        "        print(\"Loading (partly) labelled data...\")\n",
        "        finbert_df = pd.read_csv(finbert_topics_path)\n",
        "        transcripts_qa = finbert_df.copy()\n",
        "    else:\n",
        "        print(\"Creating a new dataset for labelled data...\")\n",
        "        if data_folder is None:\n",
        "            raise ValueError(\"Data folder not supplied!\")\n",
        "\n",
        "        if summarised:\n",
        "            summarised_text_path = os.path.join(data_folder, f\"phi_fulltable{appdx}.xlsx\")\n",
        "            transcripts_all = pd.read_excel(summarised_text_path)\n",
        "            transcripts_all = transcripts_all.drop(['qa_text'], axis=1)\n",
        "            transcripts_all.rename(columns={'summarised_text': 'qa_text'}, inplace=True)\n",
        "        elif synthetic:\n",
        "            synthetic_text_path = os.path.join(data_folder, f\"synthetic_data_for_finbert.csv\")\n",
        "            transcripts_all = pd.read_csv(synthetic_text_path)\n",
        "        else:\n",
        "            transcript_csv_path = os.path.join(data_folder, f\"transcripts_tabular_{bank}_clean.csv\")\n",
        "            transcripts_all = pd.read_csv(transcript_csv_path)\n",
        "\n",
        "        # subset only Q&A\n",
        "        if qa_only:\n",
        "            transcripts_qa = transcripts_all[~transcripts_all['qa_num'].isna()].copy().reset_index(drop=True)\n",
        "        else:\n",
        "            transcripts_qa = transcripts_all.copy()\n",
        "\n",
        "        # keep only the strictly necessary subset of columns\n",
        "        cols_to_keep = ['uid', 'qa_text']\n",
        "        finbert_df = transcripts_qa[cols_to_keep].copy()\n",
        "\n",
        "        # add cols for topic labels\n",
        "        for col in cols_to_add:\n",
        "            finbert_df.loc[:,col] = np.nan\n",
        "\n",
        "        # save file\n",
        "        finbert_df.to_csv(finbert_topics_path, index=False)\n",
        "\n",
        "    # iterate over texts and save labels in each loop\n",
        "    for i, row in transcripts_qa.iterrows():\n",
        "        print(f\"Processing text {i}/{transcripts_qa.shape[0]}...\")\n",
        "        predicted_label_id, predicted_label, logits, chunks = get_finbert_topics(\n",
        "                                    txt=row['qa_text'],\n",
        "                                    finbert_tokeniser=finbert_tokeniser,\n",
        "                                    finbert_model=finbert_model,\n",
        "                                    label_dict=label_dict,\n",
        "                                    chunking=chunking,\n",
        "                                    max_length=max_length\n",
        "                                    )\n",
        "\n",
        "        if chunks is not None:\n",
        "            print(f\"Multiple labels detected!\")\n",
        "            print(f\"-------label: {predicted_label[0]}\")\n",
        "            finbert_df.loc[finbert_df['uid']==row['uid'], \"qa_text\"] = chunks[0]\n",
        "            finbert_df.loc[finbert_df['uid']==row['uid'], cols_to_add[0]] = predicted_label_id[0]\n",
        "            finbert_df.loc[finbert_df['uid']==row['uid'], cols_to_add[1]] = predicted_label[0]\n",
        "            finbert_df.loc[finbert_df['uid']==row['uid'], cols_to_add[2:]] = logits[0]\n",
        "\n",
        "            for k in range(1, len(chunks)):\n",
        "                print(f\"-------label: {predicted_label[k]}\")\n",
        "                row_to_add = pd.DataFrame({\n",
        "                    \"uid\": [row['uid']],\n",
        "                    \"qa_text\": [chunks[k]],\n",
        "                    cols_to_add[0]: [predicted_label_id[k]],\n",
        "                    cols_to_add[1]: [predicted_label[k]],\n",
        "                    })\n",
        "                row_to_add[cols_to_add[2:]] = logits[k]\n",
        "                finbert_df = pd.concat((finbert_df, row_to_add), ignore_index=True)\n",
        "        else:\n",
        "            print(f\"-------label: {predicted_label}\")\n",
        "            finbert_df.loc[finbert_df.uid==row.uid, cols_to_add[0]] = predicted_label_id\n",
        "            finbert_df.loc[finbert_df.uid==row.uid, cols_to_add[1]] = predicted_label\n",
        "            finbert_df.loc[finbert_df.uid==row.uid, cols_to_add[2:]] = logits\n",
        "\n",
        "        # update the file\n",
        "        finbert_df.to_csv(finbert_topics_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crNQsyZ_46Yn"
      },
      "source": [
        "##### 2.3.1.1.3 Function for model evaluation\n",
        "\n",
        "We have a \"ground truth\" dataset where topics from the same topic list have been manually assigned to two transcripts' worth of randomly sampled question-answer pairs. This dataset can be used to evaluate model performance.\n",
        "\n",
        "This is a multi-class problem and the datasets can be expected to be quite imbalanced. Therefore, we will examine **precision** and **recall** by printing a classification report. We will also generate two confusion matrices - one normalised by the column (predicted label class) and the other normalised by the row (true label). This will help us see how misclassifications arise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUu_WYXV5Ak_"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_performance(data_folder, finbert_folder, label_dict,\n",
        "                               chunking=False, max_length=512, appdx=\"\",\n",
        "                               save_ground_truth=False):\n",
        "    # define the bank of interest\n",
        "    banks = ['JPMorgan']\n",
        "    print(\"Which bank's data do you wish to label?\")\n",
        "    bank = input(banks)\n",
        "\n",
        "    # load ground truth data\n",
        "    path_ground_truth = os.path.join(data_folder, f\"ground_truth_{bank}_manual.xlsx\")\n",
        "    df_ground_truth = pd.read_excel(path_ground_truth)\n",
        "\n",
        "    # load model labels\n",
        "    path_model = os.path.join(finbert_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    df_finbert = pd.read_csv(path_model)\n",
        "\n",
        "    # merge the datasets on uid (multiple labels per uid possible)\n",
        "    df_merged = df_finbert.merge(df_ground_truth, on=['uid'], how='left', suffixes=['_model', '_true'])\n",
        "\n",
        "    # exclude rows with no ground truth\n",
        "    df_merged = df_merged.dropna(axis=0, subset=['true_topic'])\n",
        "\n",
        "    if save_ground_truth:\n",
        "        logit_cols = [col for col in df_merged.columns if 'logit' in col]\n",
        "        df_merged_short = df_merged.drop(['qa_text_model','qa_text_true',\n",
        "                                          'true_sentiment', 'true_Q_outcome',\n",
        "                                          'qa_num', 'finbert_topic_id', 'qa_type'] + logit_cols,\n",
        "                                         axis=1)\n",
        "        df_merged_short.to_csv(os.path.join(finbert_folder, f\"finbert_topics_ground_truth_{bank}_chunking{chunking}_maxlength{max_length}_QA{appdx}.csv\"),\n",
        "                         index=False)\n",
        "\n",
        "    print(f\"There are {df_merged.shape[0]} texts in the evaluated dataset.\")\n",
        "    # get the topics covered in labelled and ground truth datasets\n",
        "    topics = np.unique(np.concatenate((df_merged['true_topic'],df_merged['finbert_topic_label'])))\n",
        "\n",
        "    # get classification report\n",
        "    report = classification_report(\n",
        "                  df_merged['true_topic'],\n",
        "                  df_merged['finbert_topic_label'],\n",
        "                  zero_division=0\n",
        "                  )\n",
        "    print(report)\n",
        "\n",
        "    # plot confusion matrix\n",
        "    fig, axes = plt.subplots(1,2, figsize=(8,3), sharey=True)\n",
        "    for i, (norm, tlt) in enumerate(zip(['pred', 'true'], ['Normalised by predicted', 'Normalised by true'])):\n",
        "        conf_matrix = confusion_matrix(df_merged['true_topic'],\n",
        "                                       df_merged['finbert_topic_label'],\n",
        "                                       normalize=norm)\n",
        "        sns.heatmap(conf_matrix, ax=axes[i], cmap='crest')\n",
        "        axes[i].set_xticklabels(topics, rotation=90)\n",
        "        axes[i].set_xlabel(\"Predicted label\")\n",
        "        axes[i].set_title(tlt)\n",
        "    axes[0].set_ylabel(\"True label\")\n",
        "    axes[0].set_yticklabels(topics, rotation=0)\n",
        "\n",
        "    # plot confusion matrix - just one for presentation\n",
        "    fig2, axes2 = plt.subplots(1,1, figsize=(9,8), sharey=True)\n",
        "    conf_matrix = confusion_matrix(df_merged['true_topic'],\n",
        "                                    df_merged['finbert_topic_label'],\n",
        "                                    normalize='true')\n",
        "    sns.heatmap(conf_matrix, ax=axes2, cmap='crest')\n",
        "    fsize=19\n",
        "    axes2.set_xticklabels(topics, rotation=90, fontsize=fsize)\n",
        "    axes2.set_xlabel(\"Predicted label\", fontsize=fsize)\n",
        "    axes2.set_title(\"Confusion matrix\", fontsize=fsize)\n",
        "    axes2.set_ylabel(\"True label\", fontsize=fsize)\n",
        "    axes2.set_yticklabels(topics, rotation=0, fontsize=fsize)\n",
        "    cbar = axes2.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=fsize)\n",
        "    cbar.set_label(\"Fraction of true label\", fontsize=fsize)\n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wYZSuQwKI8B"
      },
      "source": [
        "##### 2.3.1.1.4 Functions for plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJvRwTRF5Gl2"
      },
      "source": [
        "#### 2.3.1.2 Choosing the optimal chunking method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2KUaVNwKO3j"
      },
      "source": [
        "- `get_merged_data_for_plotting` will allow the user to select the bank and the type of text (Q, A, Q+A, management discussion, or all) that is of interest. It will also merge labelled data with the full dataset, add a`quarter_str` column for easy selection of a specific quarter, and compute probabilities from logits for each text.\n",
        "\n",
        "- `softmax(logits)` will compute probabilities of topic occurrence based on the multi-class logits provided by the model.\n",
        "\n",
        "\n",
        "- `get_optimal_subplot_dims(n)` will compute the number of rows and columns needed in a set of subplots based on the total number of subplots.\n",
        "\n",
        "- `get_stats_stars(pval, p_thresholds=[0.05,0.01,0.001])` will express the statistical significance of a p-value through 1-3 stars based on significance thresholds (* p<0.05, ** p<0.01, *** p<0.001 as a default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdIR_aoWKPdC"
      },
      "outputs": [],
      "source": [
        "def get_merged_data_for_plotting(finbert_folder, data_folder, label_dict,\n",
        "                               chunking=True, max_length=512, bank='JPMorgan',\n",
        "                                 datatype='all', appdx=\"\", summarised=False,\n",
        "                                 synthetic=False, sentiment=None, sentiment_folder=None):\n",
        "    \"\"\"\n",
        "    merge and subset datasets before plotting\n",
        "\n",
        "    datatype (str) can be 'Q' (questions only), 'A' (answers only),\n",
        "                      'QA' (Q+A with qa_num only), 'presentation' (management discussion)\n",
        "                      or 'all' (default; to proceed without subsetting )\n",
        "    \"\"\"\n",
        "    # load model labels\n",
        "    path_model = os.path.join(finbert_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    df_finbert = pd.read_csv(path_model)\n",
        "\n",
        "    # load cleaned data\n",
        "    if summarised:\n",
        "            summarised_text_path = os.path.join(data_folder, f\"phi_fulltable{appdx}.xlsx\")\n",
        "            transcripts_all = pd.read_excel(summarised_text_path)\n",
        "            transcripts_all = transcripts_all.drop(['qa_text'], axis=1)\n",
        "            transcripts_all.rename(columns={'summarised_text': 'qa_text'}, inplace=True)\n",
        "    elif synthetic:\n",
        "        synthetic_text_path = os.path.join(data_folder, f\"synthetic_data_for_finbert.csv\")\n",
        "        transcripts_all = pd.read_csv(synthetic_text_path)\n",
        "    else:\n",
        "        transcript_csv_path = os.path.join(data_folder, f\"transcripts_tabular_{bank}_clean.csv\")\n",
        "        transcripts_all = pd.read_csv(transcript_csv_path)\n",
        "    transcripts_all = transcripts_all.drop(['qa_text'], axis=1)\n",
        "\n",
        "    # merge the datasets and drop unlabelled rows\n",
        "    df_merged = df_finbert.merge(transcripts_all, on=['uid'], how='left')\n",
        "    df_merged = df_merged.dropna(axis=0, subset=['finbert_topic_label'])\n",
        "\n",
        "    # compute probabilities\n",
        "    logit_cols = [f\"topic_{i}_logit\" for i in label_dict.keys()]\n",
        "    prob_cols = [f\"topic_{i}_prob\" for i in label_dict.keys()]\n",
        "    for pc in prob_cols:\n",
        "      df_merged[pc] = np.nan\n",
        "    df_merged[prob_cols] = softmax(df_merged[logit_cols].to_numpy(), axis=1)\n",
        "\n",
        "    if not synthetic:\n",
        "        # add quarters\n",
        "        df_merged['quarter_str'] = [f\"{q}Q{str(y)[-2:]}\" for q, y in zip(df_merged['quarter'], df_merged['year'])]\n",
        "\n",
        "        # order quarters\n",
        "        quarter_str_order = df_merged['quarter_str'].unique()[::-1]\n",
        "        df_merged['quarter_str'] = pd.Categorical(df_merged['quarter_str'], categories=quarter_str_order, ordered=True)\n",
        "        # print(\"QUARTER ORDER: \", quarter_str_order)\n",
        "    else:\n",
        "        quarter_str_order = None\n",
        "\n",
        "    # subsetting data type\n",
        "    if datatype in [\"Q\", \"A\"]:\n",
        "        df_merged = df_merged[df_merged['qa_type']==datatype].copy().reset_index(drop=True)\n",
        "    elif datatype == \"QA\":\n",
        "        df_merged = df_merged[~df_merged['qa_num'].isna()].copy().reset_index(drop=True)\n",
        "    elif datatype == \"presentation\":\n",
        "        df_merged = df_merged[df_merged['section']=='management_discussion'].copy().reset_index(drop=True)\n",
        "\n",
        "    if sentiment is not None and sentiment_folder is not None:\n",
        "        sentiment_path = os.path.join(sentiment_folder, \"full_result.csv\")\n",
        "        sentiment_df = pd.read_csv(sentiment_path)\n",
        "        sentiment_colname = 'financial-roberta-large_sentiment'\n",
        "        sentiment_df = sentiment_df[['uid', sentiment_colname]]\n",
        "        df_merged_sentiment = pd.merge(df_merged, sentiment_df, on=['uid'], how='left')\n",
        "        if sentiment=='negative minus positive':\n",
        "            pass\n",
        "        else:\n",
        "            df_merged = df_merged_sentiment[df_merged_sentiment[sentiment_colname].eq(sentiment)].copy().reset_index()\n",
        "\n",
        "    return df_merged, quarter_str_order, (bank, datatype)\n",
        "\n",
        "# derive probabilities from logits\n",
        "def softmax(logits, axis=1):\n",
        "    exp_logits = np.exp(logits - np.nanmax(logits))\n",
        "    return exp_logits / exp_logits.sum(axis=axis).reshape(-1,1)\n",
        "\n",
        "def get_optimal_subplot_dims(n):\n",
        "    rows = math.floor(math.sqrt(n))\n",
        "    cols = math.ceil(n / rows)\n",
        "\n",
        "    if rows * cols < n:\n",
        "        rows += 1\n",
        "\n",
        "    return rows, cols\n",
        "\n",
        "def get_stats_stars(pval, p_thresholds=[0.05,0.01,0.001]):\n",
        "    p_thresholds = np.asarray(p_thresholds)\n",
        "    ptext = \"*\" * (pval<p_thresholds).sum()\n",
        "    if (pval<p_thresholds).sum()==0:\n",
        "        ptext=\"n.s.\"\n",
        "\n",
        "    return ptext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQUnUZ4lwnMZ"
      },
      "source": [
        "##### 2.3.1.2.1 `chunking=False`, `maxlength=512`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOunDrq_5OSe"
      },
      "outputs": [],
      "source": [
        "# generate predictions\n",
        "process_finbert(\n",
        "        finbert_tokeniser=finbert_topic_tokeniser,\n",
        "        finbert_model=finbert_topic_model,\n",
        "        save_folder=output_data_folder,\n",
        "        label_dict=id2label,\n",
        "        data_folder=output_data_folder,\n",
        "        chunking=False,\n",
        "        max_length=512,\n",
        "        qa_only=True\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRoR7unj_Qq0"
      },
      "outputs": [],
      "source": [
        "# evaluate model performance\n",
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          save_ground_truth=True\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0myrJCH3Qphj"
      },
      "source": [
        "Evidently, model <u>accuracy is above chance level</u> (5%).\n",
        "\n",
        "Still, correct classification of just 12% of inputs <u>cannot be considered a good performance</u>.\n",
        "\n",
        "Examining finer-grained performance metrics:\n",
        "- **General News | Opinion** has very high recall (0.90), but fairly low precision (0.11). Likewise, the indicating that <u>the model is prone to incorrectly applying this rather uninformative label to almost all other topics</u> (*see the General News | Opinion column in the right heatmap*).\n",
        "  - Politics and Stock Commentary do not seem affected by this misclassification only because there are no instances of these labels in the ground truth dataset.\n",
        "  - Texts belonging to Fed | Central Banks appear to be the least prone to being misclassified as General News | Opinion.\n",
        "- **Fed | Central Banks** and **Treasuries | Corporate Debt** also have higher recall than most topics, and non-zero precision.\n",
        "- All instances that the model classified as M&A | Investments, Macro, and Stock Commentary are labelled\n",
        "as **Earnings** in the ground truth dataset. In fact, no instance of Earnings in the ground truth dataset is labelled as such by the model.\n",
        "  - This potentially reflects a different interpretation of the Earnings topic by the financial experts, who labelled the data FinBERT was fine-tuned on, and the human labeller who created the \"ground truth\" dataset used here despite having no background in finance. In the current \"ground truth\" dataset, texts about NII in particular tended to be classified as Earnings...\n",
        "- The model also has not classified any of these texts as Legal | Regulation (7 instances in the ground truth dataset) or Markets (5 instances).\n",
        "-  The ground truth labeller primarily struggled to discriminate between Financials and Earnings; Legal | Regulation and Fed | Central Banks; Product | Company News and M&A | Investment. It is interesting that, rather than confusing specific pairs of classes, the model seems to have a tendency to label texts as General News | Opinion or (less often) Fed | Central Banks.\n",
        "\n",
        "\n",
        "To assess model performance beyond its tendency to classify texts as \"General News | Opinion\", we will now define a function that ignores the probability of \"General News | Opinion\" and assigns labels based on the probabilities of the remaining topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tihmkVBRRBrh"
      },
      "source": [
        "##### 2.3.1.2.2 Defining a function to reclassify texts ignoring \"General News | Opinions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZFrjBLWQ5qx"
      },
      "source": [
        "Preventing *any* texts to be classified as \"General News | Opinion\" is not ideal, because it really *is* the most suitable topic in some cases. Based on the randomly sampled ground truth dataset, we could expect about 9% of the texts to truly reflect general news or opinion. Unfortunately, logit values do not allow us to separate this minority of cases from many others where this topic is assigned erroneously. For example,\n",
        "- *All right. Thanks for all that.* -- would fit under \"General News | Opinion\" and has a logit value for this topic of 3.77\n",
        "- *Very good. Appreciate the color and candor, as always. Thank you.* -- would also fit under \"General News | Opinion\", but has a logit value of 5.70\n",
        "- *If we adjust [the consensus expense forecast] for the one-timers this year, that would suggest a core expense base that's just below $90 billion, so pretty healthy step-up in expenses. I know you've always had a strong commitment and discipline around investment. Just want to better understand where those incremental dollars are being deployed and just which investments are being prioritized in particular looking out to next year.* -- This is about investments, yet its logit value (4.26) is between the two more general texts.\n",
        "\n",
        "Therefore, a complete removal of this topic label will be the most effective approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gzYqO56Q8UJ"
      },
      "outputs": [],
      "source": [
        "def reclassify_finbert(finbert_folder, label_dict, exclude='General News | Opinions',\n",
        "                       chunking=False, max_length=512, appdx=\"\"):\n",
        "    # define the bank of interest\n",
        "    banks = ['JPMorgan']\n",
        "    print(\"Which bank's data do you wish to label?\")\n",
        "    bank = input(banks)\n",
        "\n",
        "    # load model labels\n",
        "    path_model = os.path.join(finbert_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    path_model_new = os.path.join(finbert_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}_relabelled.csv\")\n",
        "    df_finbert = pd.read_csv(path_model)\n",
        "\n",
        "    # find key to exclude and replace the respective col with nans\n",
        "    key_to_exclude = [key for key,val in label_dict.items() if val==exclude][0]\n",
        "\n",
        "    # isolate logit cols, except the key to exclude\n",
        "    logit_cols = [col for col in df_finbert.columns if \"_logit\" in col and f\"{key_to_exclude}_logit\" not in col]\n",
        "\n",
        "    df_finbert['finbert_topic_id'] = [int(i.split(\"_\")[1]) for i in df_finbert[logit_cols].idxmax(axis=1)]\n",
        "    df_finbert['finbert_topic_label'] = [label_dict[i] for i in df_finbert['finbert_topic_id']]\n",
        "\n",
        "    df_finbert.to_csv(path_model_new, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWbU5VMsGvem"
      },
      "outputs": [],
      "source": [
        "# reclassify the model\n",
        "reclassify_finbert(\n",
        "                  finbert_folder=output_data_folder,\n",
        "                  label_dict=id2label,\n",
        "                  chunking=False,\n",
        "                  max_length=512,\n",
        "                  exclude='General News | Opinion'\n",
        "                  )\n",
        "\n",
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          appdx='_relabelled'\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DJnztGtRJYJ"
      },
      "source": [
        "If texts are relabelled based on the max topic probability, ignoring \"General News | Opinion\", model accuracy increases from 12% to 19%.\n",
        "\n",
        "The confusions are also more in line with the uncertainties of the human labeller:\n",
        "- \"Earnings\" misclassified as \"Macro\", \"Financials\", \"Stock Commentary\"\n",
        "- \"Legal | Regulation\" misclassified as \"Fed | Central Banks\"\n",
        "- \"Fed | Central Banks\" misclassified as \"Politics\"\n",
        "- \"M&A | Investments\" misclassified as \"Company | Product News\" (also as \"Stock Commentary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLbtCL1hRL5x"
      },
      "source": [
        "##### 2.3.1.2.3 Defining a function to plot model logits for ground truth labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx3-tawKRQtw"
      },
      "source": [
        "A better way to visualise the behaviour of the model would be by looking beyond the assigned labels and plotting the logit density distributions of various ground truth labels directly.\n",
        "\n",
        "The following function will take the subset of data with a particular ground truth label and plot the density distributions of model logits. Kernel density curves with peaks above 0 (chance level logit) will be highlighted and included in the legend of the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nBscFsaRSPY"
      },
      "outputs": [],
      "source": [
        "def plot_model_logits(true_topic, data_folder, finbert_folder, label_dict,\n",
        "                               chunking=False, max_length=512, appdx=\"\", bank='JPMorgan',\n",
        "                      save_topic=None):\n",
        "\n",
        "    # load ground truth data\n",
        "    path_ground_truth = os.path.join(data_folder, f\"ground_truth_{bank}.csv\")\n",
        "    df_ground_truth = pd.read_csv(path_ground_truth)\n",
        "\n",
        "    # load model labels\n",
        "    path_model = os.path.join(finbert_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    df_finbert = pd.read_csv(path_model)\n",
        "\n",
        "    # merge the datasets on uid (multiple labels per uid possible)\n",
        "    df_merged = df_finbert.merge(df_ground_truth, on=['uid'], how='left', suffixes=['_model', '_true'])\n",
        "\n",
        "    # exclude rows with no ground truth\n",
        "    df_merged = df_merged.dropna(axis=0, subset=['true_topic'])\n",
        "\n",
        "\n",
        "    logit_cols = [f\"topic_{i}_logit\" for i in label_dict.keys()]\n",
        "\n",
        "    df_logits = df_merged[df_merged['true_topic']==true_topic][logit_cols].copy()\n",
        "    cmap = sns.color_palette(\"tab20\")\n",
        "\n",
        "    x_range = np.linspace(-10, 10, 1000)\n",
        "    fig,ax = plt.subplots(1,1, figsize=(4,2))\n",
        "    for col in df_logits.columns:\n",
        "        i = int(col.split(\"_\")[1])  # get topic label id\n",
        "\n",
        "        kde = gaussian_kde(df_logits.loc[:,col].values, bw_method=\"scott\")\n",
        "        kde_values = kde(x_range)\n",
        "        kde_peak = x_range[np.argmax(kde_values)]\n",
        "\n",
        "        if kde_peak<=0:\n",
        "            sns.kdeplot(df_logits.loc[:,col].values, color='grey', alpha=0.2, lw=1)\n",
        "        else:\n",
        "            sns.kdeplot(df_logits.loc[:,col].values, color=cmap[i], alpha=1, lw=2, label=label_dict[i])\n",
        "    fsize=12\n",
        "    ax.set_title(f\"True label: {topic}\", fontsize=fsize)\n",
        "    ax.set_xlabel(\"FinBERT logit\", fontsize=fsize)\n",
        "    ax.set_ylabel(\"Density\", fontsize=fsize)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=fsize)\n",
        "    plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), prop={'size': fsize})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQW8JdF1HEPh"
      },
      "outputs": [],
      "source": [
        "for topic in ['Company | Product News', 'Earnings', 'Fed | Central Banks',\n",
        "               'Financials', 'Legal | Regulation', 'M&A | Investments']:\n",
        "    plot_model_logits(true_topic=topic,\n",
        "                      data_folder=processed_data_folder,\n",
        "                      finbert_folder=output_data_folder,\n",
        "                      label_dict=id2label,\n",
        "                      chunking=False,\n",
        "                      max_length=512,\n",
        "                      bank='JPMorgan'\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIkMB9aIRXqQ"
      },
      "source": [
        "These plots confirm that texts manually labelled as, for example, \"M&A | Investments\" often get classified as \"Company | Product News\", which is a pair of categories our human labeller often struggled to discriminate as well. Similarly, texts manually labelled as \"Legal | Regulation\" (which the model seems to rarely use for Q&A texts) often get classified as \"Fed | Central Banks\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n8Xy-rURZDR"
      },
      "source": [
        "##### 2.3.1.2.4 `chunking=True`, `maxlength=512`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ5WKMFbRkqB"
      },
      "source": [
        "With these arguments, longer texts (3% of the data) will be chunked into equal-sized texts. All information will thus be considered for classification. Some `uid` will no longer be unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OpK1MbkWovxy"
      },
      "outputs": [],
      "source": [
        "process_finbert(\n",
        "        finbert_tokeniser=finbert_topic_tokeniser,\n",
        "        finbert_model=finbert_topic_model,\n",
        "        save_folder=output_data_folder,\n",
        "        label_dict=id2label,\n",
        "        data_folder=processed_data_folder,\n",
        "        chunking=True,\n",
        "        max_length=512,\n",
        "        qa_only=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FMdLYI5fFt-k"
      },
      "outputs": [],
      "source": [
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=True,\n",
        "                          max_length=512\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdsYdyTDRugo"
      },
      "source": [
        "Since the evaluated dataset still contains 108 texts, it is either that none of the chunked texts are part of the ground truth dataset or that all chunked texts in the ground truth dataset ended up with the same label. Thus, it is unsurprising that the evaluation results are exactly the same as before.\n",
        "\n",
        "We will relabel the output of this model for later use, but will not re-evaluate the relabelled outcome, since the above result already shows that there will be no difference to the previous re-evaluation (2.3.1.2.2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1clT-MFI0ie"
      },
      "outputs": [],
      "source": [
        "# reclassify the model\n",
        "reclassify_finbert(\n",
        "                  finbert_folder=output_data_folder,\n",
        "                  label_dict=id2label,\n",
        "                  chunking=True,\n",
        "                  max_length=512,\n",
        "                  exclude='General News | Opinion'\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_y2JzKTSMpp"
      },
      "source": [
        "##### 2.3.1.2.5 `chunking=True`, `maxlength=128`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gXNBRv0SQSB"
      },
      "source": [
        "We will also try shorter chunks to see if it can help reduce the prevalence of the `General News | Opinion` label and improve model accuracy.\n",
        "\n",
        "First, let us check how many texts will get chunked now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnC72Hku1r0M"
      },
      "outputs": [],
      "source": [
        "plot_finbert_token_lengths(processed_data_folder, finbert_topic_tokeniser, max_length=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuXMW5LQSVdC"
      },
      "source": [
        "Now, we will generate the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xlp46KUTbVov"
      },
      "outputs": [],
      "source": [
        "process_finbert(\n",
        "        finbert_tokeniser=finbert_topic_tokeniser,\n",
        "        finbert_model=finbert_topic_model,\n",
        "        save_folder=output_data_folder,\n",
        "        label_dict=id2label,\n",
        "        data_folder=processed_data_folder,\n",
        "        chunking=True,\n",
        "        max_length=128,\n",
        "        qa_only=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjykQOeBSbmI"
      },
      "source": [
        "Finally, we can evaluate the model's performance on this chunked dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tjfvYS5THW6r"
      },
      "outputs": [],
      "source": [
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=True,\n",
        "                          max_length=128,\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNTlgd1GSf8x"
      },
      "source": [
        "Model accuracy has not improved.\n",
        "\n",
        "Many texts still get misclassified as General News | Opinion. We can check model performance if this topic is ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyj25izAYZYX"
      },
      "outputs": [],
      "source": [
        "# reclassify the model\n",
        "reclassify_finbert(\n",
        "                  finbert_folder=output_data_folder,\n",
        "                  label_dict=id2label,\n",
        "                  chunking=True,\n",
        "                  max_length=128,\n",
        "                  exclude='General News | Opinion'\n",
        "                  )\n",
        "\n",
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=True,\n",
        "                          max_length=128,\n",
        "                          appdx='_relabelled'\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnb2Fw6dSkeJ"
      },
      "source": [
        "Ignoring General News | Opinion, the model has 16% accuracy, which is still better than the model *with* General News | Opinion, but <u>slightly worse than the model with `maxlength=512`</u>.\n",
        "\n",
        "---\n",
        "It is also worth checking the model logit distributions for the most frequent ground truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO2dv7ABJs0y"
      },
      "outputs": [],
      "source": [
        "for topic in ['Company | Product News', 'Earnings', 'Fed | Central Banks',\n",
        "               'Financials', 'Legal | Regulation', 'M&A | Investments']:\n",
        "    plot_model_logits(true_topic=topic,\n",
        "                      data_folder=processed_data_folder,\n",
        "                      finbert_folder=output_data_folder,\n",
        "                      label_dict=id2label,\n",
        "                      chunking=True,\n",
        "                      max_length=128,\n",
        "                      bank='JPMorgan'\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GVWv95uSqT4"
      },
      "source": [
        "As before, topics with the best logits (KDE peak above the chance level logit of 0) seem to be in good conceptual alignment with ground truth labels. Similarities between some KDE curves also illustrate the potential downsides of a winner-takes-all classification.\n",
        "\n",
        "Therefore, **all further analyses will be based on model logits, not discrete labels**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bl1kzs05zTP"
      },
      "source": [
        "#### 2.3.1.3 Using summarised texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZcCZALdSwW4"
      },
      "source": [
        "A reason for texts being misclassified as \"General News | Opinion\" might be the conversational language used in the Q&A sections of the calls. To see if a change in tome might mitigate this flaw, we will apply the model to **texts summarised using Phi-3.5**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEzgCGtpYJGS"
      },
      "outputs": [],
      "source": [
        "# checking token lengths of summarised texts\n",
        "plot_finbert_token_lengths(processed_data_folder, finbert_topic_tokeniser, max_length=512, summarised=True,\n",
        "                           appdx=\"_summarised\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUBLHzp9TmDp"
      },
      "source": [
        "As required by the prompt, all texts are really short! Chunking will not be necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ODyOCCJDZ2QM"
      },
      "outputs": [],
      "source": [
        "# topic label prediction on summarised texts\n",
        "process_finbert(\n",
        "        finbert_tokeniser=finbert_topic_tokeniser,\n",
        "        finbert_model=finbert_topic_model,\n",
        "        save_folder=output_data_folder,\n",
        "        label_dict=id2label,\n",
        "        data_folder=processed_data_folder,\n",
        "        chunking=False,\n",
        "        max_length=512,\n",
        "        qa_only=False,\n",
        "        summarised=True,\n",
        "        appdx=\"_summarised\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTfuEYC8aqyF"
      },
      "outputs": [],
      "source": [
        "# evaluating model performance on summarised texts\n",
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          appdx='_summarised',\n",
        "                          save_ground_truth=True\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bav3vpwTjbx"
      },
      "source": [
        "Accuracy has increased to 26%! The model has become less prone to labelling texts as \"General News | Opinion\". Interestingly, many of its misclassifications now fall into the \"Company | Product News\" category.\n",
        "\n",
        "Let us check if relabelling based on model logits, while ignoring \"General News | Opinion\", still improves accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT6SrikxbbBh"
      },
      "outputs": [],
      "source": [
        "# reclassify the model\n",
        "reclassify_finbert(\n",
        "                  finbert_folder=output_data_folder,\n",
        "                  label_dict=id2label,\n",
        "                  chunking=False,\n",
        "                  max_length=512,\n",
        "                  exclude='General News | Opinion',\n",
        "                  appdx='_summarised'\n",
        "                  )\n",
        "\n",
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          appdx='_summarised_relabelled'\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbDCXJ2CTty5"
      },
      "source": [
        "Accuracy dropped to 22%. This makes sense because the model was clearly less prone to labelling texts as \"General News | Opinion\". Instead, the topic texts are most often misclassified as is \"Company | Product News\". We will therefore check how accuracy changes if this topic is removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZpEjzucCTVz"
      },
      "outputs": [],
      "source": [
        "# reclassify the model\n",
        "reclassify_finbert(\n",
        "                  finbert_folder=output_data_folder,\n",
        "                  label_dict=id2label,\n",
        "                  chunking=False,\n",
        "                  max_length=512,\n",
        "                  exclude='Company | Product News',\n",
        "                  appdx='_summarised'\n",
        "                  )\n",
        "\n",
        "evaluate_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          appdx='_summarised_relabelled'\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dIstUrPTxmi"
      },
      "source": [
        "Ignoring \"Company | Product News\", yielded 17% accuracy. The inadequacy of such blanket removal is not surprising given that this topic is genuinely quite prevalent in the dataset.\n",
        "\n",
        "Overall, it is <u>a good sign</u> that ignoring whole labels no longer helps improve the accuracy.\n",
        "\n",
        "As a last check, let us see the probability distributions of the most common topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2stKdq9OYugG"
      },
      "outputs": [],
      "source": [
        "for topic in ['Company | Product News', 'Earnings', 'Fed | Central Banks',\n",
        "               'Financials', 'Legal | Regulation', 'M&A | Investments']:\n",
        "    plot_model_logits(true_topic=topic,\n",
        "                      data_folder=processed_data_folder,\n",
        "                      finbert_folder=output_data_folder,\n",
        "                      label_dict=id2label,\n",
        "                      chunking=False,\n",
        "                      max_length=512,\n",
        "                      bank='JPMorgan',\n",
        "                      appdx='_summarised',\n",
        "                      save_topic='Legal | Regulation'\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4IXXqBHT3sT"
      },
      "source": [
        "As before, topic probability distributions illustrate that several topics can be associated with texts of a particular \"true label\", and the margins for misclassification are likely small. Thus, the use of probability distributions, rather than discrete labels, will provide a more complete view of the information contained in the texts.\n",
        "\n",
        "---\n",
        "\n",
        "🚨 Overall, we will proceed by using:\n",
        "- FinBERT output on summarised texts to analyse the <u>Q&A portions</u> of the transcripts because of the better performance;\n",
        "- FinBERT output on raw texts (`max_length=512`) to analyse the <u>presentation portions</u> of the transcripts, because executives' summaries cover a wide range of topics and a two-sentence summary of several pages of text does not capture this topic diversity adequately.\n",
        "\n",
        "In both cases, we will use topic probability distributions, not discrete labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyOHR2Xy-r3Q"
      },
      "source": [
        "#### 2.3.1.4 Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnKNHWmpDJvg"
      },
      "outputs": [],
      "source": [
        "# modified evaluate_model_performance function for side by side output\n",
        "def compare_model_performance(data_folder, finbert_folder, label_dict,\n",
        "                               chunking=False, max_length=512, appdx=\"\",\n",
        "                               save_ground_truth=False):\n",
        "    bank = 'JPMorgan'\n",
        "    appdx_dict = {'_relabelled': 'Raw texts', '_summarised': 'Summarised texts'}\n",
        "\n",
        "    # load ground truth data\n",
        "    path_ground_truth = os.path.join(data_folder, f\"ground_truth_{bank}.csv\")\n",
        "    df_ground_truth = pd.read_csv(path_ground_truth)\n",
        "\n",
        "    # load model labels\n",
        "    path_model = os.path.join(finbert_folder, f\"finbert_topics_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    df_finbert = pd.read_csv(path_model)\n",
        "\n",
        "    # merge the datasets on uid (multiple labels per uid possible)\n",
        "    df_merged = df_finbert.merge(df_ground_truth, on=['uid'], how='left', suffixes=['_model', '_true'])\n",
        "\n",
        "    # exclude rows with no ground truth\n",
        "    df_merged = df_merged.dropna(axis=0, subset=['true_topic'])\n",
        "\n",
        "    if save_ground_truth:\n",
        "        logit_cols = [col for col in df_merged.columns if 'logit' in col]\n",
        "        df_merged_short = df_merged.drop(['qa_text_model','qa_text_true',\n",
        "                                          'true_sentiment', 'true_Q_outcome',\n",
        "                                          'qa_num', 'finbert_topic_id', 'qa_type'] + logit_cols,\n",
        "                                         axis=1)\n",
        "        df_merged_short.to_csv(os.path.join(data_folder, f\"finbert_topics_ground_truth_{bank}_chunking{chunking}_maxlength{max_length}_QA{appdx}.csv\"),\n",
        "                         index=False)\n",
        "\n",
        "    # get the topics covered in labelled and ground truth datasets\n",
        "    topics = np.unique(np.concatenate((df_merged['true_topic'],df_merged['finbert_topic_label'])))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # plot confusion matrix - just one for presentation\n",
        "    fig2, axes2 = plt.subplots(1,1, figsize=(9,8), sharey=True)\n",
        "    conf_matrix = confusion_matrix(df_merged['true_topic'],\n",
        "                                    df_merged['finbert_topic_label'],\n",
        "                                    normalize='true')\n",
        "    sns.heatmap(conf_matrix, ax=axes2, cmap='crest')\n",
        "    fsize=19\n",
        "    axes2.set_xticklabels(topics, rotation=90, fontsize=fsize)\n",
        "    axes2.set_xlabel(\"Predicted label\", fontsize=fsize)\n",
        "    axes2.set_title(\"Confusion matrix\", fontsize=fsize)\n",
        "    axes2.set_ylabel(\"True label\", fontsize=fsize)\n",
        "    axes2.set_yticklabels(topics, rotation=0, fontsize=fsize)\n",
        "    cbar = axes2.collections[0].colorbar\n",
        "    cbar.ax.tick_params(labelsize=fsize)\n",
        "    cbar.set_label(\"Fraction of true label\", fontsize=fsize)\n",
        "    axes2.set_title(appdx_dict[appdx], fontsize=40)\n",
        "    plt.tight_layout()\n",
        "    fig2.savefig(f\"finbert-topic-{bank}-chunking{chunking}-maxlength{max_length}{appdx}.png\",\n",
        "                 dpi=300,bbox_inches=\"tight\")\n",
        "\n",
        "\n",
        "    # plot confusion matrix\n",
        "    fig, axes = plt.subplots(1,2, figsize=(8,3), sharey=True)\n",
        "    for i, (norm, tlt) in enumerate(zip(['pred', 'true'], ['Normalised by predicted', 'Normalised by true'])):\n",
        "        conf_matrix = confusion_matrix(df_merged['true_topic'],\n",
        "                                       df_merged['finbert_topic_label'],\n",
        "                                       normalize=norm)\n",
        "        sns.heatmap(conf_matrix, ax=axes[i], cmap='crest')\n",
        "        axes[i].set_xticklabels(topics, rotation=90)\n",
        "        axes[i].set_xlabel(\"Predicted label\")\n",
        "        axes[i].set_title(tlt)\n",
        "    axes[0].set_ylabel(\"True label\")\n",
        "    axes[0].set_yticklabels(topics, rotation=0)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # get classification report\n",
        "    report = classification_report(\n",
        "                  df_merged['true_topic'],\n",
        "                  df_merged['finbert_topic_label'],\n",
        "                  zero_division=0\n",
        "                  )\n",
        "    print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKqLoH4rBXXa"
      },
      "outputs": [],
      "source": [
        "out1 = Output()\n",
        "out2 = Output()\n",
        "\n",
        "with out1:\n",
        "  compare_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          appdx='_relabelled'\n",
        "                          )\n",
        "with out2:\n",
        "  compare_model_performance(\n",
        "                          data_folder=processed_data_folder,\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          appdx='_summarised',\n",
        "                          save_ground_truth=True\n",
        "                          )\n",
        "display(HBox([out1, out2]))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl1wISMp_EF9"
      },
      "source": [
        "### 2.3.2 BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G_UXE4P7muP"
      },
      "source": [
        "The flexibility of BERTopic allows us to choose the model for the different components of BERTopic. We are using PCA for dimension reduction, KMeans cllustering for clustering, and `all-MiniLM-L6-v2` for the embedding model.\n",
        "\n",
        "For the embedding model, the open-ai embedding and the Google `universal-sentence-encoder` were also used but the results difference is narrow. So we chose `all-MiniLM-L6-v2` at the end for its performance speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3ZVd-Ql7ony"
      },
      "outputs": [],
      "source": [
        "# load the embedding_model (same for all runs)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ckNCabYm0Q"
      },
      "source": [
        "#### 2.3.2.1 Pre-processing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYWDw05oYmKB"
      },
      "outputs": [],
      "source": [
        "def preprocess_spacy(text):\n",
        "    # Load spaCy language model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Define a custom list of stop words\n",
        "    custom_stopwords = set([\n",
        "        \"and\", \"billions\", \"due\", \"jeremy\", \"text\", \"net\", \"qt\", \"billion\", \"million\", \"think\", \"thank\", \"year\", \"years\",\n",
        "        \"month\", \"months\", \"yeah\", \"okay\", \"go\", \"ok\", \"hi\", \"good\", \"hey\", \"morning\", \"sure\", \"jamie\", \"jim\", \"like\",\n",
        "        \"thing\", \"bit\", \"little\", \"key\", \"ceo\", \"got\", \"lot\", \"guy\"\n",
        "    ])\n",
        "\n",
        "    # Define the target part-of-speech tags to keep and consolidate\n",
        "    target_pos = {\"NOUN\"}  # In this example, we choose to consolidate nouns in their lemmatized form\n",
        "\n",
        "    # Process the text as a spaCy document\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Create an empty set to store consolidated tokens\n",
        "    consolidated_tokens = set()\n",
        "\n",
        "    # Perform preprocessing\n",
        "    for token in doc:\n",
        "        lemma = token.lemma_.lower()  # Use lemmatized, lowercase form of the word\n",
        "\n",
        "        # Filtering: remove default stop words, custom stop words, punctuation, whitespace, numbers, and single characters\n",
        "        if (not token.is_stop                # Default stop words\n",
        "            and lemma not in custom_stopwords  # Custom stop words\n",
        "            and not token.is_punct           # Punctuation\n",
        "            and not token.is_space           # Whitespace\n",
        "            and not token.like_num           # Numbers\n",
        "            and len(lemma) > 1):             # Single characters\n",
        "\n",
        "            # If the part of speech is one of the target types (e.g., nouns), consolidate by adding the lemmatized form\n",
        "            if token.pos_ in target_pos:\n",
        "                consolidated_tokens.add(lemma)  # Add the lemmatized form of nouns to the set\n",
        "            else:\n",
        "                # Other parts of speech can be added as they are\n",
        "                consolidated_tokens.add(token.text.lower())\n",
        "\n",
        "    # Join tokens back to a string\n",
        "    text = \" \".join(consolidated_tokens)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-yHHwbDZH-B"
      },
      "source": [
        "#### 2.3.2.2 Performing on summarised dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-wSQCRrlhG7"
      },
      "source": [
        "We have already seen that summarisation greatly improve topic modelling. Hence we will run BERTopic on summarised dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR8SUWHZRoaK"
      },
      "outputs": [],
      "source": [
        "# phi full table summarised data.xlsx\n",
        "df_phi_fulltable_summarised = pd.read_excel(processed_data_folder + '/phi_fulltable_summarised.xlsx')\n",
        "\n",
        "# phi_ground_truth_summarised data.xlsx\n",
        "df_phi_ground_truth_summarised = pd.read_excel(processed_data_folder + '/phi_ground_truth_summarised.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJdevSRen3IU"
      },
      "outputs": [],
      "source": [
        "# apply preprocessing\n",
        "df_phi_fulltable_summarised_list = df_phi_fulltable_summarised['summarised_text'].apply(preprocess_spacy).to_list()\n",
        "\n",
        "# running BERTopic\n",
        "dim_model = PCA(n_components=5)\n",
        "cluster_model = KMeans(n_clusters=20)\n",
        "\n",
        "topic_model_phi_summarised = BERTopic(umap_model=dim_model, embedding_model=embedding_model,\n",
        "                       hdbscan_model=cluster_model, calculate_probabilities=True)\n",
        "topics, probabilities = topic_model_phi_summarised.fit_transform(df_phi_fulltable_summarised_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJhzPtvZ2O3s"
      },
      "outputs": [],
      "source": [
        "# Plot each BERTtopic visualization into a subplot\n",
        "topic_model_phi_summarised.visualize_topics().write_html(\"topic_model_phi_summarised_topic.html\")\n",
        "topic_model_phi_summarised.visualize_barchart(top_n_topics=20,n_words=8, autoscale=True).write_html(\"topic_model_phi_summarised_barchart.html\")\n",
        "topic_model_phi_summarised.visualize_heatmap().write_html(\"topic_model_phi_summarised_heatmap.html\")\n",
        "topic_model_phi_summarised.visualize_hierarchy().write_html(\"topic_model_phi_summarised_hierarchy.html\")\n",
        "\n",
        "# Load each plot into a subplot\n",
        "\n",
        "display(HTML(\"topic_model_phi_summarised_topic.html\"))\n",
        "display(HTML(\"topic_model_phi_summarised_barchart.html\"))\n",
        "display(HTML(\"topic_model_phi_summarised_heatmap.html\"))\n",
        "display(HTML(\"topic_model_phi_summarised_hierarchy.html\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyown3V8raOM"
      },
      "source": [
        "## 2.4 Q/A Evasion and Generalisability of Phi-3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_AADzkgFTOR"
      },
      "source": [
        "### 2.4.0 Using FinBERT classification for Q/A evasion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT1co36LFc9I"
      },
      "source": [
        "Using the labels from FinBERT, we can perform a very basic analysis to see if the the topics of questions and answers match.\n",
        "\n",
        "We will correlate the topic probability distributions of all question-answer pairs. This will create a distribution of correlation coefficients.\n",
        "\n",
        "We will repeat this analysis on data with shuffled Q-A relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdDg5xYhGGmW"
      },
      "outputs": [],
      "source": [
        "def plot_answers_to_questions(finbert_folder, data_folder,\n",
        "                                label_dict, chunking=True, max_length=512,\n",
        "                                bank=\"JPMorgan\", datatype=\"QA\", appdx=\"\", save=False,\n",
        "                              summarised=False, synthetic=False, exclude=None):\n",
        "\n",
        "  df_merged, _, metadata = get_merged_data_for_plotting(finbert_folder, data_folder,\n",
        "                                              label_dict, chunking=chunking, max_length=max_length,\n",
        "                                              bank=bank, datatype=datatype, appdx=appdx,\n",
        "                                              summarised=summarised, synthetic=synthetic)\n",
        "\n",
        "  # subset QAs\n",
        "  df_merged = df_merged[~df_merged['qa_num'].isna()].reset_index(drop=True).copy()\n",
        "\n",
        "  # select only Qs that have As\n",
        "  mask = df_merged.groupby(\"qa_num\")[\"qa_type\"].transform(lambda x: {'A', 'Q'}.issubset(set(x)))\n",
        "  df_merged = df_merged[mask]\n",
        "\n",
        "  # exclude General News | Opinion\n",
        "  if exclude is not None:\n",
        "      gen_key = [key for key,val in label_dict.items() if val==exclude][0]\n",
        "\n",
        "      # define probability columns\n",
        "      prob_cols = [col for col in df_merged.columns if 'prob' in col and f'topic_{gen_key}' not in col]\n",
        "  else:\n",
        "      prob_cols = [col for col in df_merged.columns if 'prob' in col]\n",
        "\n",
        "  # define all columns to keep and subset the df\n",
        "  subset_cols =prob_cols+ ['uid','qa_num', 'qa_type', 'finbert_topic_label']\n",
        "  df_qa = df_merged[subset_cols]\n",
        "\n",
        "  # subset answers\n",
        "  df_answers = df_qa[df_qa['qa_type']=='A'].reset_index(drop=True).copy()\n",
        "  df_questions = df_qa[df_qa['qa_type']=='Q'].reset_index(drop=True).copy()\n",
        "  df_questions['qa_num_shuffled'] = df_questions['qa_num'].sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  # add cols to fill\n",
        "  df_answers['qa_corr'] = np.nan\n",
        "  df_answers['qa_corr_shuffled'] = np.nan\n",
        "\n",
        "  # iterate over answers\n",
        "  for i, row in df_answers.iterrows():\n",
        "      answer = np.array([row[col] for col in prob_cols])\n",
        "      question = df_questions[df_questions['qa_num']==row.qa_num][prob_cols].values.flatten()\n",
        "      question_shuffled = df_questions[df_questions['qa_num_shuffled']==row.qa_num][prob_cols].values.flatten()\n",
        "      df_answers.at[i,'qa_corr'] = pearsonr(answer, question)[0]\n",
        "      df_answers.at[i,'qa_corr_shuffled'] = pearsonr(answer, question_shuffled)[0]\n",
        "\n",
        "  # stats\n",
        "  _, pval = mannwhitneyu(df_answers['qa_corr'], df_answers['qa_corr_shuffled'], alternative='two-sided')\n",
        "  print(f\"p-val: {pval}\")\n",
        "\n",
        "  # plot\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(4,2))\n",
        "  sns.kdeplot(df_answers['qa_corr'], ax=ax, color='#FE7F2D', alpha=0.6, label='Data', lw=2)\n",
        "  sns.kdeplot(df_answers['qa_corr_shuffled'], ax=ax, color='#233D4D', alpha=0.6, label='Shuffled', lw=2, ls='dashed')\n",
        "\n",
        "  ax.set_xlabel(\"Pearson correlation coefficient\")\n",
        "  plt.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
        "\n",
        "  # save correlation data\n",
        "  if save:\n",
        "      df_for_saving = df_answers[['uid', 'qa_num', 'qa_corr']]\n",
        "      if synthetic:\n",
        "          df_for_saving['uid'] = [\"_\".join(x.split(\"_\")[:-1]) for x in df_for_saving['uid']]\n",
        "      path_for_saving = os.path.join(finbert_folder, f\"finbert_QA_topic_correlations{appdx}.csv\")\n",
        "      df_for_saving.to_csv(path_for_saving, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jDhgEEHBX-k"
      },
      "outputs": [],
      "source": [
        "# Checking question evasiveness on raw text\n",
        "plot_answers_to_questions(finbert_folder=output_data_folder, data_folder=processed_data_folder,\n",
        "                                label_dict=id2label, chunking=False, max_length=512,\n",
        "                                bank=\"JPMorgan\", datatype=\"QA\", appdx=\"\",\n",
        "                          exclude=\"General News | Opinion\", save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1IExbLOxRm6"
      },
      "outputs": [],
      "source": [
        "# Now - on text summarised by Phi-3.5\n",
        "plot_answers_to_questions(finbert_folder=output_folder, data_folder=data_folder,\n",
        "                                label_dict=id2label, chunking=False, max_length=512,\n",
        "                                bank=\"JPMorgan\", datatype=\"QA\", appdx=\"_summarised\",\n",
        "                                summarised=True, save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8bUtePXICX6"
      },
      "source": [
        "With both datasets, topic alignment is significantly better than chance for the true Q-A pairs than for shuffled pairs. Interestingly, text summarisation has resulted in equal-height peaks near Q-A topic correlation coefficients of 0 and 1, suggesting similar numbers of answered and avoided questions. Analysis of raw text was a bit more complimentary for the executives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mux5QXBIKky8"
      },
      "source": [
        "### 2.4.1 Phi-3.5 Pipeline initialisation and Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHhQmNuSH6gE"
      },
      "outputs": [],
      "source": [
        "# Initialise the pipeline - note T4 GPU does not contain enough RAM, so use CPU and ignore warning OR run on A100 GPU\n",
        "pipe = pipeline(\"text-generation\", model=\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWf4pCEhK0ET"
      },
      "source": [
        "#### 2.4.1.0 Preparing datasets for Phi 3.5 (with results from other analyses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHOacirLKm_0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aggregation function to merge Questions with their Answers\n",
        "    - all answers to a question will be merged into one\n",
        "    - each qa_text row will now start with the questions asked and then continue into all of the answers given\n",
        "    - this allows for assessment of question evasion in the answers\n",
        "    - assumes RoBERTa Sentiment and finBERT(qa_corr) columns already included\n",
        "\"\"\"\n",
        "# Function to aggregate Q and A pairs into single rows for question-evasion testing (test data only)\n",
        "def aggregate_test_data(df):\n",
        "\n",
        "  # Select columns to keep for test data\n",
        "  df = df[['uid', 'qa_type', 'qa_num', 'qa_text', 'true_sentiment', 'true_topic', 'true_Q_outcome', 'RoBERTa Sentiment', 'qa_corr', 'qa_corr_summarised', 'finBERT Topic Classification', 'finBERT(summarised) Topic Classification']]\n",
        "\n",
        "  # Convert 'qa_num' to numeric, forcing non-numeric values to NaN, then drop rows where 'qa_num' is NaN\n",
        "  # Note this will exclude all 'N' type rows, which is required for QA pair analysis\n",
        "  df['qa_num'] = pd.to_numeric(df['qa_num'], errors='coerce')\n",
        "  df = df.dropna(subset=['qa_num'])\n",
        "\n",
        "  # Convert qa_num, qa_corr to integers/floats and qa_text to strings\n",
        "  df.loc[:, 'qa_num'] = df['qa_num'].astype(int)\n",
        "  df.loc[:, 'qa_corr'] = df['qa_corr'].astype(float)\n",
        "  df.loc[:, 'qa_corr_summarised'] = df['qa_corr_summarised'].astype(float)\n",
        "  df.loc[:, 'qa_text'] = df['qa_text'].astype(str)\n",
        "\n",
        "\n",
        "  # Group by 'qa_num', with different approach for each column\n",
        "  def custom_aggregation(group):\n",
        "\n",
        "    # Aggregate qa_text by joining texts for each question\n",
        "    aggregated_text = ' '.join(group['qa_text'])\n",
        "\n",
        "    # For true_Q_outcome, take the first value\n",
        "    true_Q_outcome = group['true_Q_outcome'].iloc[0] if 'true_Q_outcome' in group else None\n",
        "\n",
        "    # For qa_corrs, take the mean\n",
        "    qa_corr_value = group['qa_corr'].mean() if 'qa_corr' in group else None\n",
        "    qa_corr_value_summarised = group['qa_corr_summarised'].mean() if 'qa_corr_summarised' in group else None\n",
        "\n",
        "    # For true_topic, true_sentiment, finBERT and RoBERTa Sentiment, only take values where qa_type is 'A'\n",
        "    true_topic = group.loc[group['qa_type'] == 'A', 'true_topic'].unique().tolist() if 'true_topic' in group else []\n",
        "    true_sentiment = group.loc[group['qa_type'] == 'A', 'true_sentiment'].unique().tolist() if 'true_sentiment' in group else []\n",
        "    roberta_sentiment = group.loc[group['qa_type'] == 'A', 'RoBERTa Sentiment'].unique().tolist() if 'RoBERTa Sentiment' in group else []\n",
        "    finBERT_topics_1 = group.loc[group['qa_type'] == 'A', 'finBERT Topic Classification'].unique().tolist() if 'finBERT Topic Classification' in group else []\n",
        "    finBERT_topics_summarised = group.loc[group['qa_type'] == 'A', 'finBERT(summarised) Topic Classification'].unique().tolist() if 'finBERT(summarised) Topic Classification' in group else []\n",
        "\n",
        "    # Print uids for manual review where multiple sentiments/topics in answer parts\n",
        "    check_list = [true_sentiment, roberta_sentiment, true_topic, finBERT_topics_1, finBERT_topics_summarised]\n",
        "    check_names = ['true_sentiment', 'RoBERTa Sentiment', 'true_topic', 'finBERT Topic Classification', 'finBERT(summarised) Topic Classification']\n",
        "    manual_review_list = []\n",
        "\n",
        "    for i, check in enumerate(check_list):\n",
        "      if len(check) > 1:\n",
        "        manual_review_list.append(group['uid'].iloc[0])\n",
        "\n",
        "    # Remove duplicates and filter out empty strings from the list\n",
        "    clean_manual_review_list = list(set([uid for uid in manual_review_list if uid]))\n",
        "    print(clean_manual_review_list)\n",
        "\n",
        "    # Convert lists to strings (removing brackets) for values expected to be singular values\n",
        "    true_topic = ', '.join(true_topic)\n",
        "    true_sentiment = ', '.join(true_sentiment)\n",
        "    roberta_sentiment = ', '.join(roberta_sentiment)\n",
        "    finBERT_topics_1 = ', '.join(finBERT_topics_1)\n",
        "    finBERT_topics_summarised = ', '.join(finBERT_topics_summarised)\n",
        "\n",
        "\n",
        "    # Return a dictionary of aggregated values\n",
        "    return pd.Series({\n",
        "        'uid': group['uid'].iloc[0].replace('_Q_', '_QA_'),\n",
        "        'qa_type': 'QA',\n",
        "        'qa_num': group['qa_num'].iloc[0],\n",
        "        'qa_text': aggregated_text,\n",
        "        'true_sentiment': true_sentiment,\n",
        "        'true_topic': true_topic,\n",
        "        'true_Q_outcome': true_Q_outcome,\n",
        "        'RoBERTa Sentiment': roberta_sentiment,\n",
        "        'qa_corr': qa_corr_value,\n",
        "        'finBERT Topic Classification': finBERT_topics_1,\n",
        "        'qa_corr_summarised': qa_corr_value_summarised,\n",
        "        'finBERT(summarised) Topic Classification': finBERT_topics_summarised\n",
        "    })\n",
        "\n",
        "\n",
        "  # Apply custom aggregation\n",
        "  aggregated_df = df.groupby('qa_num', as_index=False).apply(custom_aggregation).reset_index(drop=True)\n",
        "\n",
        "  # Rename columns and set index to 'uid' for both dfs\n",
        "  for table in [aggregated_df, df]:\n",
        "    for col in table.columns:\n",
        "      if col == 'true_sentiment': table.rename(columns={'true_sentiment': 'True Sentiment'}, inplace=True)\n",
        "      if col == 'true_topic': table.rename(columns={'true_topic': 'True Topic'}, inplace=True)\n",
        "      if col == 'true_Q_outcome': table.rename(columns={'true_Q_outcome': 'True Evasion Present'}, inplace=True)\n",
        "\n",
        "  # Visualise DataFrames\n",
        "  print(\"\\nOriginal DataFrame:\", len(df), \"rows\")\n",
        "  display(df.head())\n",
        "\n",
        "  print(\"\\nAggregated QA DataFrame:\", len(aggregated_df), \"rows\")\n",
        "  display(aggregated_df.head())\n",
        "\n",
        "  return df, aggregated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRrR4ADHKqFY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aggregation function to merge Questions with their Answers\n",
        "    - all answers to a question will be merged into one\n",
        "    - each qa_text row will now start with the questions asked and then continue into all of the answers given\n",
        "    - this allows for assessment of question evasion in the answers\n",
        "\"\"\"\n",
        "# Function to aggregate Q and A pairs into single rows for question-evasion testing (Non-test data only)\n",
        "def aggregate_QA_data(df):\n",
        "\n",
        "  # Convert 'qa_num' to numeric, forcing non-numeric values to NaN, then drop rows where 'qa_num' is NaN\n",
        "  # Note this will exclude all 'N' type rows, which is required for QA pair analysis\n",
        "  df.loc[:, 'qa_num'] = pd.to_numeric(df['qa_num'], errors='coerce')\n",
        "  df = df.dropna(subset=['qa_num'])\n",
        "\n",
        "  # Convert qa_num to integers and qa_text to strings\n",
        "  df.loc[:, 'qa_num'] = df['qa_num'].astype(int)\n",
        "  df.loc[:, 'qa_text'] = df['qa_text'].astype(str)\n",
        "\n",
        "  # Check\n",
        "  print(\"\\nOriginal DataFrame:\", len(df), \"rows\")\n",
        "  display(df.head())\n",
        "\n",
        "\n",
        "  # Only keep essential columns: 'uid', 'qa_type', 'qa_num', and 'qa_text' (common to all pre-processed files) for real data\n",
        "  df = df[['uid', 'qa_type', 'qa_num', 'qa_text']]\n",
        "\n",
        "  # Group by 'qa_num' and aggregate the 'qa_text' column by joining texts for each question\n",
        "  df.loc[:, 'qa_text'] = df.groupby('qa_num')['qa_text'].transform(lambda x: ' '.join(x))\n",
        "\n",
        "  # Drop duplicates to keep only one row per 'qa_num' with the aggregated text\n",
        "  df = df.drop_duplicates(subset=['qa_num'])\n",
        "\n",
        "  # Convert all qa_type entries to QA and 'uid' to QA\n",
        "  df.loc[:, 'qa_type'] = 'QA'\n",
        "  df['uid'] = df['uid'].str.replace('_Q_', '_QA_')\n",
        "\n",
        "  # Set index to uid and drop index column\n",
        "  df = df.set_index('uid', drop=True)\n",
        "\n",
        "  # Check\n",
        "  print(\"\\nAggregated QA DataFrame:\", len(df), \"rows\")\n",
        "  display(df.head())\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeAMyqJgKr9J"
      },
      "outputs": [],
      "source": [
        "# Function to calculate finBERT Evasion result from qa_corr or qa_corr_summarised value (if higher than 0.9, become Not Evasive, else Evasive)\n",
        "def calculate_finBERT_evasion(df):\n",
        "\n",
        "    # Convert qa_corr to float type\n",
        "  if 'qa_corr' in df.columns:\n",
        "    df['qa_corr'] = df['qa_corr'].astype(float)\n",
        "    df['qa_corr'] = np.where(df['qa_corr'] > 0.9, 'Not Evasive', 'Evasive')\n",
        "    df.rename(columns={'qa_corr': 'finBERT Evasion Present'}, inplace=True)\n",
        "\n",
        "  if 'qa_corr_summarised' in df.columns:\n",
        "    df['qa_corr_summarised'] = df['qa_corr_summarised'].astype(float)\n",
        "    df['qa_corr_summarised'] = np.where(df['qa_corr_summarised'] > 0.9, 'Not Evasive', 'Evasive')\n",
        "    df.rename(columns={'qa_corr_summarised': 'finBERT(summarised) Evasion Present'}, inplace=True)\n",
        "\n",
        "  # Set index to uid and drop index column\n",
        "  df = df.set_index('uid', drop=True)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAbL_QXOp6O1"
      },
      "source": [
        "#### 2.4.1.1 Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8liJxyJp7Hr"
      },
      "outputs": [],
      "source": [
        "# Define prompt questions for Evasion, Sentiment and Topic analysis with Phi-3.5 and Q&A tables of pre-processed financial transcripts\n",
        "\n",
        "# All questions (for evasion to run, must be aggregated dataset, otherwise it will ignore evasion question)\n",
        "combined_questions = [\n",
        "    f\"1. Analyze the following financial question-answer pairs. Determine if the answer is evasive or non-evasive. Consider the following factors:\\n\"\n",
        "    \"1. Deception: Is the answer ambiguous, misleading or trying to conceal something? 2. Relevance: Is the information provided relevant to the question? 3. Specificity: Is the answer specific enough to provide an informative response in a financial context?\\n\\n\"\n",
        "    \"Example of an Evasive answer: What is the company's strategy for addressing the rising costs of raw materials? We are taking positive steps to mitigate the impact of rising costs. Explanation: This is evasive because the answer does not give any detail about the strategy. \\n\\n\"\n",
        "    \"Example of a non-evasive answer: How will the recent interest rate hike impact the bank's net interest margin? The recent interest rate hike is expected to positively impact the bank's net interest margin, as the bank's interest-earning assets tend to reprice faster than its interest-bearing liabilities. Explanation: This is a non-evasive answer as it clearly states the impact.\\n\\n\"\n",
        "    \"If evasion is detected, classify it as Evasive, with a level of 'Low', 'Moderate', or 'High'. For each instance of evasion, identify up to 5 specific topics that were not adequately addressed.\\n\"\n",
        "    \"Each theme should be a short keyword or phrase summarizing the topic, e.g., '2024 consensus,' 'NII Markets,' 'Asset-Sensitive Number.'\\n\"\n",
        "    \"Your response should use the format:\\n\\n\"\n",
        "    \"Evasive: [Low/Moderate/High]; [topic1, topic2, topic3]\\n\\n\",\n",
        "\n",
        "    f\"2. Categorise the sentiment in this text as positive, negative, or neutral. Provide relative percentage scores and up to 5 concise keywords or themes contributing to each sentiment, suitable for direct input into a table. Use the following format:\\n\\n\"\n",
        "    \"Positive: %; [theme1, theme2, theme3]\\n\"\n",
        "    \"Negative: %; [theme1, theme2, theme3]\\n\"\n",
        "    \"Neutral: %; [theme1, theme2, theme3]\\n\"\n",
        "    \"Each theme should be a short keyword or phrase summarizing the topic, e.g., 'NII,' '2024 consensus,' or 'Asset-Sensitive Number.'\\n\"\n",
        "\n",
        "    f\"3. Identify the primary topic discussed in the following financial transcript and classify it as one of these categories: \"\n",
        "    \"Earnings, Financials, M&A | Investments, General News | Opinion, Fed | Central Banks, Company | Product News, Markets, Treasuries | Corporate Debt, Legal | Regulation, Macro, Energy | Oil, Currencies, Analyst Update, IPO, Dividend, Politics, Gold | Metals | Materials, Stock Movement, Personnel Change, Stock Commentary.\\n\"\n",
        "    \"Do not make up other categories, assign it to the best fit from the categories provided. Your final response must use the format:\\n\\n\"\n",
        "    \"finBERT Topic: [category]\\n\\n\"\n",
        "    ]\n",
        "\n",
        "# Sentiment only\n",
        "sentiment_questions = [\n",
        "    f\"1. Categorise the financial sentiment in this text as positive, negative, or neutral. Provide relative percentage scores and up to 5 concise keywords or themes contributing to each sentiment, suitable for direct input into a table. Use the following format:\\n\\n\"\n",
        "    \"Positive: %; [theme1, theme2, theme3]\\n\"\n",
        "    \"Negative: %; [theme1, theme2, theme3]\\n\"\n",
        "    \"Neutral: %; [theme1, theme2, theme3]\\n\"\n",
        "    \"Each theme should be a short keyword or phrase summarizing the topic, e.g., 'NII,' '2024 consensus,' or 'Asset-Sensitive Number.'\\n\",\n",
        "]\n",
        "\n",
        "# Evasion only (needs aggregated QA dataset)\n",
        "evasion_questions = [\n",
        "    f\"Analyze the following financial question-answer pairs. Determine if the answer is evasive or non-evasive. Consider the following factors:\\n\"\n",
        "    \"1. Deception: Is the answer ambiguous, misleading or trying to conceal something? 2. Relevance: Is the information provided relevant to the question? 3. Specificity: Is the answer specific enough to provide an informative response in a financial context?\\n\\n\"\n",
        "    \"Example of an Evasive answer: What is the company's strategy for addressing the rising costs of raw materials? We are taking positive steps to mitigate the impact of rising costs. Explanation: This is evasive because the answer does not give any detail about the strategy. \\n\\n\"\n",
        "    \"Example of a non-evasive answer: How will the recent interest rate hike impact the bank's net interest margin? The recent interest rate hike is expected to positively impact the bank's net interest margin, as the bank's interest-earning assets tend to reprice faster than its interest-bearing liabilities. Explanation: This is a non-evasive answer as it clearly states the impact.\\n\\n\"\n",
        "    \"If evasion is detected, classify it as Evasive, with a level of 'Low', 'Moderate', or 'High'. For each instance of evasion, identify up to 5 specific topics that were not adequately addressed.\\n\"\n",
        "    \"Each theme should be a short keyword or phrase summarizing the topic, e.g., '2024 consensus,' 'NII Markets,' 'Asset-Sensitive Number.'\\n\"\n",
        "    \"Your response should use the format:\\n\\n\"\n",
        "    \"Evasive: [Low/Moderate/High]; [topic1, topic2, topic3]\\n\\n\"\n",
        "]\n",
        "\n",
        "# Topic assignment to match finBERT topics\n",
        "finBERT_topic_questions = [\n",
        "    f\"Identify the primary topic discussed in the following financial transcript and classify it as one of these categories: \"\n",
        "    \"Earnings, Financials, M&A | Investments, General News | Opinion, Fed | Central Banks, Company | Product News, Markets, Treasuries | Corporate Debt, Legal | Regulation, Macro, Energy | Oil, Currencies, Analyst Update, IPO, Dividend, Politics, Gold | Metals | Materials, Stock Movement, Personnel Change, Stock Commentary.\\n\"\n",
        "    \"Do not make up other categories, assign it to the best fit from the categories provided. Your final response must use the format:\\n\\n\"\n",
        "    \"finBERT Topic: [category]\\n\\n\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9L_JQZJK8sZ"
      },
      "source": [
        "#### 2.4.1.2 Functions to run analyses with Phi-3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msGKE_NwLFNh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Financial keywords to search for in text are defined here\n",
        "\"\"\"\n",
        "keyword_search_terms = [\"SS3/24\", \"SS3\", \"SS three\", \"supervisory statement\", \"PRA\", \"credit risk definition of default\", \"definition of default\", \"Bank of England\", \"BoE\", \"England\", \"Basel\", \"Basel III\", \"Basel IV\", \"S9/24\", \"S9\", \"S nine\"]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function to search provided text for financial keywords\n",
        "  - this can be done by whole pdf (as text file) or row by row of Q&A table\n",
        "\"\"\"\n",
        "# Search for keywords of interest\n",
        "def keyword_search(text, keywords):\n",
        "    found_terms = [term for term in keywords if term.lower() in text.lower()]\n",
        "    if found_terms:\n",
        "        term_list = ', '.join(found_terms)\n",
        "        return term_list\n",
        "    else:\n",
        "        return \"None\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baxHBxmHLGvx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function to run phi-3.5 on tabular questions and answers.\n",
        "  This will take as input either the aggregated Q&A pairs (for combined or separate question-evasion and sentiment/topic analysis) or separate Q, A and N rows table (for sentiment and topic analysis only)\n",
        "  The prompt used can vary but must produce the following output format (where applicable for Sentiment or Evasion analysis):\n",
        "   \"Positive: %; [theme1, theme2, theme3]\n",
        "    Negative: %; [theme1, theme2, theme3]\n",
        "    Neutral: %; [theme1, theme2, theme3]\n",
        "    Each theme should be a short keyword or phrase summarizing the topic, e.g., 'NII,' '2024 consensus,' or 'Asset-Sensitive Number.'\n",
        "    Evasive: [Low/Moderate/High]; [topic1, topic2, topic3]\n",
        "    Each theme should be a short keyword or phrase summarizing the topic, e.g., '2024 consensus,' 'NII Markets,' 'Asset-Sensitive Number.''\n",
        "   \"\n",
        "\n",
        "  The non-aggregated 'separate Q, A and N rows table' input option is intended only for ground-truth assessment of performance against other methods such as BERTopic, finBERT and RoBERTa.\n",
        "  Analysis path is determined by presence of 'sentiment' and/or 'Evasive' in the prompt\n",
        "  There is a sense check against non-aggregated data being accidentally input with the evasive prompt (as this only works with aggregated data).\n",
        "\"\"\"\n",
        "\n",
        "# Function to run phi-3.5 on tabular questions and answers\n",
        "def phi_question_answer(input_df, prompt_questions, input_aggregated='N', input_col='qa_text'):\n",
        "\n",
        "  print(\"input_aggregated:\", input_aggregated)\n",
        "\n",
        "  # Set up progress counter and timer\n",
        "  start_time = time.time()\n",
        "  total_count = len(input_df)\n",
        "  x = 0\n",
        "\n",
        "  # Determine table columns based on the questions asked\n",
        "  columns = [\"uid\", \"keywords\"]\n",
        "  if any(\"Evasive\" in q for q in prompt_questions):\n",
        "      columns.extend([\"Phi-3.5 Evasion Present\", \"Phi-3.5 Evasion Degree\", \"Phi-3.5 Evaded Topics\"])\n",
        "\n",
        "      # Check that aggregated flag is on for evasion input data - if not exit function with warning\n",
        "      if input_aggregated == 'N':\n",
        "        print(\"Error: Evasion question present in prompt but non-aggregated dataset provided. Analysis stopped.\")\n",
        "        return None\n",
        "\n",
        "  if any(\"sentiment\" in q for q in prompt_questions):\n",
        "      columns.extend([\"Phi-3.5 Sentiment\", \"Phi-3.5 Positive %\", \"Phi-3.5 Negative %\", \"Phi-3.5 Neutral %\", \"Phi-3.5 Positive Topics\", \"Phi-3.5 Negative Topics\", \"Phi-3.5 Neutral Topics\"])\n",
        "\n",
        "  if any(\"finBERT\" in q for q in prompt_questions):\n",
        "      columns.extend([\"Phi-3.5 Topic Classification\"])\n",
        "\n",
        "\n",
        "  # Initialise PrettyTable with dynamic columns\n",
        "  table = PrettyTable(columns)\n",
        "\n",
        "  # Loop through rows in phi_input_df and apply keyword search and Phi3.5 questions to each row\n",
        "  for i, row in input_df.iterrows():\n",
        "\n",
        "      # Iterate counter\n",
        "      x += 1\n",
        "\n",
        "      # Check for keywords\n",
        "      text = row[input_col]\n",
        "      keywords = keyword_search(text, keyword_search_terms)\n",
        "      if keywords != 'None':\n",
        "        print(f\"Keywords found: {keywords}\")\n",
        "      if x == 1 or x % 5 == 0:\n",
        "        print(f\"{x}/{total_count} ...\")\n",
        "\n",
        "      # Initialise variables to store extracted data (depending on questions asked)\n",
        "      row_data = {\"uid\": i, \"keywords\": keywords}\n",
        "      pos_value, neg_value, neut_value = 0, 0, 0\n",
        "\n",
        "      if \"Phi-3.5 Positive %\" in columns:\n",
        "          row_data.update({\"Phi-3.5 Sentiment\": \"N/A\", \"Phi-3.5 Positive %\": \"N/A\", \"Phi-3.5 Negative %\": \"N/A\", \"Phi-3.5 Neutral %\": \"N/A\", \"Phi-3.5 Positive Topics\": \"N/A\", \"Phi-3.5 Negative Topics\": \"N/A\", \"Phi-3.5 Neutral Topics\": \"N/A\"})\n",
        "      if \"Phi-3.5 Evasion Present\" in columns:\n",
        "          row_data.update({\"Phi-3.5 Evasion Present\": \"N/A\", \"Phi-3.5 Evasion Degree\": \"N/A\", \"Phi-3.5 Evaded Topics\": \"N/A\"})\n",
        "      if \"Phi-3.5 Topic Classification\" in columns:\n",
        "          row_data.update({\"Phi-3.5 Topic Classification\": \"N/A\"})\n",
        "\n",
        "\n",
        "      # Answer prompt questions with Phi-3.5 on text from each row\n",
        "      for question in prompt_questions:\n",
        "          prompt = f\"Context: {text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "          output = pipe(prompt, max_new_tokens=130, min_length=20, do_sample=False, eos_token_id=pipe.tokenizer.eos_token_id)\n",
        "          answer = output[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
        "          #print(answer)\n",
        "\n",
        "          # Extract Topic Classification information from the answer, if finBERT classification question asked\n",
        "          if \"finBERT\" in question:\n",
        "            if 'finBERT Topic:' in answer:\n",
        "              row_data[\"Phi-3.5 Topic Classification\"] = answer.split('Topic: ')[1].split('\\n')[0].strip() if 'Topic: ' in answer else \"N/A\"\n",
        "\n",
        "          # Extract sentiment-related information from the answer, if sentiment question asked\n",
        "          if \"sentiment\" in question:\n",
        "              if \"Positive:\" in answer:\n",
        "                pos_part = answer.split(\"Positive: \")[1]\n",
        "                pos_value = float(pos_part.split(\";\")[0].strip().replace(\"%\", \"\")) if pos_part else 0\n",
        "                row_data[\"Phi-3.5 Positive %\"] = pos_value\n",
        "                row_data[\"Phi-3.5 Positive Topics\"] = pos_part.split(\"[\")[1].split(\"]\")[0].strip() if \"[\" in pos_part and \"]\" in pos_part else \"N/A\"\n",
        "\n",
        "              if \"Negative:\" in answer:\n",
        "                neg_part = answer.split(\"Negative: \")[1]\n",
        "                neg_value = float(neg_part.split(\";\")[0].strip().replace(\"%\", \"\")) if neg_part else 0\n",
        "                row_data[\"Phi-3.5 Negative %\"] = neg_value\n",
        "                row_data[\"Phi-3.5 Negative Topics\"] = neg_part.split(\"[\")[1].split(\"]\")[0].strip() if \"[\" in neg_part and \"]\" in neg_part else \"N/A\"\n",
        "\n",
        "              if \"Neutral:\" in answer:\n",
        "                neutral_part = answer.split(\"Neutral: \")[1]\n",
        "                neut_value = float(neutral_part.split(\";\")[0].strip().replace(\"%\", \"\")) if neutral_part else 0\n",
        "                row_data[\"Phi-3.5 Neutral %\"] = neut_value\n",
        "                row_data[\"Phi-3.5 Neutral Topics\"] = neutral_part.split(\"[\")[1].split(\"]\")[0].strip() if \"[\" in neutral_part and \"]\" in neutral_part else \"N/A\"\n",
        "\n",
        "              # Determine the overall sentiment based on the highest percentage value, checking for \"Neutral\" first, with logic conflicts also resulting in Neutral\n",
        "              if neut_value >= pos_value and neut_value >= neg_value:\n",
        "                row_data[\"Phi-3.5 Sentiment\"] = \"Neutral\"\n",
        "              elif neg_value >= pos_value and neg_value >= neut_value:\n",
        "                row_data[\"Phi-3.5 Sentiment\"] = \"Negative\"\n",
        "              elif pos_value > neg_value and pos_value > neut_value:\n",
        "                row_data[\"Phi-3.5 Sentiment\"] = \"Positive\"\n",
        "              else:\n",
        "                row_data[\"Phi-3.5 Sentiment\"] = \"Neutral\"\n",
        "\n",
        "          # Extract question evasion determination from the answer, if evasion question asked\n",
        "          if \"Evasive\" in question:\n",
        "            if \"Evasive\" in answer:\n",
        "              row_data[\"Phi-3.5 Evasion Present\"] = \"Evasive\"\n",
        "\n",
        "              # If Evasion degree is low, set to Non-Evasive\n",
        "              if \"Low\" in answer:\n",
        "                row_data[\"Phi-3.5 Evasion Degree\"] = \"Low\"\n",
        "                row_data[\"Phi-3.5 Evasion Present\"] = \"Not Evasive\"\n",
        "\n",
        "              elif \"Moderate\" in answer: row_data[\"Phi-3.5 Evasion Degree\"] = \"Moderate\"\n",
        "              elif \"High\" in answer: row_data[\"Phi-3.5 Evasion Degree\"] = \"High\"\n",
        "            else:\n",
        "              row_data[\"Phi-3.5 Evasion Present\"] = \"Not Evasive\"\n",
        "              row_data[\"Phi-3.5 Evasion Degree\"] = \"None\"\n",
        "            row_data[\"Phi-3.5 Evaded Topics\"] = answer.split('[')[1].split(']')[0].strip() if '[' in answer and ']' in answer else \"N/A\"\n",
        "\n",
        "      # Add the data to the PrettyTable\n",
        "      table.add_row([row_data[col] for col in columns])\n",
        "\n",
        "\n",
        "  # Convert PrettyTable to a DataFrame and merge with original table\n",
        "  phi_temp_df = pd.DataFrame(table.rows, columns=table.field_names)\n",
        "  phi_final_df = pd.merge(input_df, phi_temp_df, left_index=True, right_on=\"uid\", how=\"outer\")\n",
        "  phi_final_df.set_index('uid', inplace=True)\n",
        "  phi_final_df = phi_final_df.sort_values(by='qa_num')\n",
        "\n",
        "  # View df\n",
        "  display(phi_final_df.head())\n",
        "\n",
        "  # Calculate time for full dataset\n",
        "  end_time = time.time()\n",
        "  time_taken = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken for {len(phi_final_df)} QA: {round(time_taken/60, 2)} minutes\")\n",
        "\n",
        "  if input_aggregated == 'Y':\n",
        "    print(f\"Estimate for all transcripts: {round((time_taken/len(phi_final_df) * 375)/60/60, 2)} hours\")\n",
        "  else:\n",
        "    print(f\"Estimate for all transcripts: {round((time_taken/len(phi_final_df) * 927)/60/60, 2)} hours\")\n",
        "\n",
        "  return phi_final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dAB-zhXLK3K"
      },
      "source": [
        "#### 2.4.1.3 Functions for accuracy plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMfj7xe9LKEq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions to calculate accuracy metrics for sentiment and evasion comparisons and display barcharts and confusion matrices for Phi results\n",
        "\"\"\"\n",
        "\n",
        "# Function to compare all columns with 'Sentiment' or 'Evasion Present' in the column name against 'True Sentiment' or 'True Evasion Present' column for each row\n",
        "def calculate_accuracy(df, sentiment_flag='Y', evasion_flag='Y', topic_flag='Y'):\n",
        "\n",
        "  accuracy_results = {\n",
        "        'Sentiment Accuracy': {},\n",
        "        'Evasion Accuracy': {},\n",
        "        'Topic Accuracy': {}\n",
        "  }\n",
        "\n",
        "  threshold_results = {\n",
        "      'Sentiment Threshold': 0.33,\n",
        "      'Evasion Threshold': 0.5,\n",
        "      'Topic Threshold': 0.2\n",
        "  }\n",
        "\n",
        "  num_sentiments = 3\n",
        "  num_evasion = 2\n",
        "  num_topics = 20\n",
        "\n",
        "  # Find all columns that contain 'Sentiment' and compare them against 'True Sentiment'\n",
        "  sentiment_columns = [col for col in df.columns if 'sentiment' in col.lower() and col != 'True Sentiment']\n",
        "\n",
        "  if sentiment_columns:\n",
        "    for col in sentiment_columns:\n",
        "      sentiment_accuracy = ((df[col].astype(str).str.lower() == df['True Sentiment'].astype(str).str.lower()).sum() / len(df)) * 100\n",
        "      accuracy_results['Sentiment Accuracy'][col] = round(sentiment_accuracy, 2)\n",
        "      unique_values = df[col].unique()\n",
        "    threshold_results['Sentiment Threshold'] = round((100 / num_sentiments), 2)\n",
        "  else:\n",
        "    print(\"No sentiment columns found.\")\n",
        "\n",
        "\n",
        "  # Find all columns that contain 'Evasion Present' and compare them against 'True Evasion Present'\n",
        "  evasion_columns = [col for col in df.columns if 'evasion present' in col.lower() and col != 'True Evasion Present']\n",
        "\n",
        "  if evasion_columns:\n",
        "    for col in evasion_columns:\n",
        "      evasion_accuracy = ((df[col].astype(str).str.lower() == df['True Evasion Present'].astype(str).str.lower()).sum() / len(df)) * 100\n",
        "      accuracy_results['Evasion Accuracy'][col] = round(evasion_accuracy, 2)\n",
        "      unique_values = df[col].unique()\n",
        "    threshold_results['Evasion Threshold'] = round((100 / num_evasion), 2)\n",
        "  else:\n",
        "    print(\"No evasion columns found.\")\n",
        "\n",
        "  # Find all columns that contain 'Classification' and compare them against 'True Topic'\n",
        "  topic_columns = [col for col in df.columns if 'classification' in col.lower() and col != 'True Topic']\n",
        "\n",
        "  if topic_columns:\n",
        "    for col in topic_columns:\n",
        "      topic_accuracy = ((df[col].astype(str).str.lower() == df['True Topic'].astype(str).str.lower()).sum() / len(df)) * 100\n",
        "      accuracy_results['Topic Accuracy'][col] = round(topic_accuracy, 2)\n",
        "      unique_values = df[col].unique()\n",
        "    threshold_results['Topic Threshold'] = round((100 / num_topics), 2)\n",
        "  else:\n",
        "    print(\"No topic classification columns found.\")\n",
        "\n",
        "\n",
        "  # Print the accuracy results\n",
        "  if accuracy_results['Topic Accuracy'] and topic_flag == 'Y':\n",
        "    print(f\"\\nTopic Accuracy Results (Significance Threshold {threshold_results['Topic Threshold']}%):\")\n",
        "    for method, acc in accuracy_results['Topic Accuracy'].items():\n",
        "      print(f\"{method}: {acc}%\")\n",
        "\n",
        "  if accuracy_results['Sentiment Accuracy'] and sentiment_flag == 'Y':\n",
        "    print(f\"\\nSentiment Accuracy Results (Significance Threshold {threshold_results['Sentiment Threshold']}%):\")\n",
        "    for method, acc in accuracy_results['Sentiment Accuracy'].items():\n",
        "      print(f\"{method}: {acc}%\")\n",
        "\n",
        "  if accuracy_results['Evasion Accuracy'] and evasion_flag == 'Y':\n",
        "    print(f\"\\nEvasion Accuracy Results (Significance Threshold {threshold_results['Evasion Threshold']}%):\")\n",
        "    for method, acc in accuracy_results['Evasion Accuracy'].items():\n",
        "      print(f\"{method}: {acc}%\")\n",
        "\n",
        "  plot_accuracy_charts(accuracy_results, df, threshold_results, sentiment_flag, evasion_flag, topic_flag)\n",
        "\n",
        "  return accuracy_results\n",
        "\n",
        "\n",
        "\n",
        "# Function to plot separate accuracy barcharts for sentiment and evasion comparisons\n",
        "def plot_accuracy_charts(accuracy_results, df, threshold_results, sentiment_flag='Y', evasion_flag='Y', topic_flag='Y'):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    plot_idx = 0\n",
        "    threshold_line_colour = 'darkgrey'\n",
        "\n",
        "     # Plot Topic Accuracy\n",
        "    a2 = 0\n",
        "    if accuracy_results['Topic Accuracy'] and topic_flag == 'Y':\n",
        "      plot_name = 'Topic Accuracy'\n",
        "      threshold = threshold_results.get('Topic Threshold', 0)\n",
        "      labels = [label.split()[0] for label in accuracy_results[plot_name].keys()]\n",
        "      axes[0, a2].bar(accuracy_results[plot_name].keys(), accuracy_results[plot_name].values(), color='darkgreen')\n",
        "      axes[0, a2].set_title(plot_name, fontsize=16)\n",
        "      axes[0, a2].set_ylabel('Accuracy %', fontsize=14)\n",
        "      axes[0, a2].set_ylim(0, 100)\n",
        "      axes[0, a2].set_xticks(range(len(accuracy_results[plot_name])))\n",
        "      axes[0, a2].set_xticklabels(labels, rotation=45, ha='right')\n",
        "      axes[0, a2].tick_params(axis='y', labelsize=14)\n",
        "      axes[0, a2].tick_params(axis='x', labelsize=14)\n",
        "\n",
        "      # Add a dashed horizontal line at the chance threshold\n",
        "      axes[0, a2].axhline(y=threshold, color=threshold_line_colour, linestyle='--')\n",
        "\n",
        "      # Annotate each bar with the accuracy value\n",
        "      for i, (label, value) in enumerate(accuracy_results[plot_name].items()):\n",
        "        axes[0, a2].text(i, value -1, f'{value:.1f}%', ha='center', va='top', fontsize=12, color='white')\n",
        "\n",
        "    else:\n",
        "      axes[0, a2].axis('off')\n",
        "\n",
        "\n",
        "    # Plot Sentiment Accuracy\n",
        "    a2 = 1\n",
        "    if accuracy_results['Sentiment Accuracy'] and sentiment_flag == 'Y':\n",
        "      plot_name = 'Sentiment Accuracy'\n",
        "      threshold = threshold_results.get('Sentiment Threshold', 0)\n",
        "      labels = [label.split()[0] for label in accuracy_results[plot_name].keys()]\n",
        "      axes[0, a2].bar(accuracy_results[plot_name].keys(), accuracy_results[plot_name].values(), color='royalblue')\n",
        "      axes[0, a2].set_title(plot_name, fontsize=16)\n",
        "      axes[0, a2].set_ylabel('Accuracy %', fontsize=14)\n",
        "      axes[0, a2].set_ylim(0, 100)\n",
        "      axes[0, a2].set_xticks(range(len(accuracy_results[plot_name])))\n",
        "      axes[0, a2].set_xticklabels(labels, rotation=45, ha='right')\n",
        "      axes[0, a2].tick_params(axis='y', labelsize=14)\n",
        "      axes[0, a2].tick_params(axis='x', labelsize=14)\n",
        "\n",
        "      # Add a dashed horizontal line at the chance threshold\n",
        "      axes[0, a2].axhline(y=threshold, color=threshold_line_colour, linestyle='--')\n",
        "\n",
        "      # Annotate each bar with the accuracy value\n",
        "      for i, (label, value) in enumerate(accuracy_results[plot_name].items()):\n",
        "        axes[0, a2].text(i, value -1, f'{value:.1f}%', ha='center', va='top', fontsize=12, color='white')\n",
        "\n",
        "    else:\n",
        "      axes[0, a2].axis('off')\n",
        "\n",
        "\n",
        "    # Plot Evasion Accuracy\n",
        "    a2 = 2\n",
        "    if accuracy_results['Evasion Accuracy'] and evasion_flag == 'Y':\n",
        "      plot_name = 'Evasion Accuracy'\n",
        "      threshold = threshold_results.get('Evasion Threshold', 0)\n",
        "      labels = [label.split()[0] for label in accuracy_results[plot_name].keys()]\n",
        "      axes[0, a2].bar(accuracy_results[plot_name].keys(), accuracy_results[plot_name].values(), color='brown')\n",
        "      axes[0, a2].set_title(plot_name, fontsize=16)\n",
        "      axes[0, a2].set_ylabel('Accuracy %', fontsize=14)\n",
        "      axes[0, a2].set_ylim(0, 100)\n",
        "      axes[0, a2].set_xticks(range(len(accuracy_results[plot_name])))\n",
        "      axes[0, a2].set_xticklabels(labels, rotation=45, ha='right')\n",
        "      axes[0, a2].tick_params(axis='y', labelsize=14)\n",
        "      axes[0, a2].tick_params(axis='x', labelsize=14)\n",
        "\n",
        "      # Add a dashed horizontal line at the chance threshold\n",
        "      axes[0, a2].axhline(y=threshold, color=threshold_line_colour, linestyle='--')\n",
        "\n",
        "      # Annotate each bar with the accuracy value\n",
        "      for i, (label, value) in enumerate(accuracy_results[plot_name].items()):\n",
        "        axes[0, a2].text(i, value -1, f'{value:.1f}%', ha='center', va='top', fontsize=12, color='white')\n",
        "\n",
        "    else:\n",
        "      axes[0, a2].axis('off')\n",
        "\n",
        "\n",
        "    # Find the model with the highest accuracy for each category\n",
        "    best_sentiment_model = max(accuracy_results['Sentiment Accuracy'], key=accuracy_results['Sentiment Accuracy'].get, default=None)\n",
        "    best_evasion_model = max(accuracy_results['Evasion Accuracy'], key=accuracy_results['Evasion Accuracy'].get, default=None)\n",
        "    best_topic_model = max(accuracy_results['Topic Accuracy'], key=accuracy_results['Topic Accuracy'].get, default=None)\n",
        "\n",
        "    # Plot confusion matrix for best Topic Classification model\n",
        "    a2 = 0\n",
        "    if 'True Topic' in df.columns and best_topic_model in df.columns and topic_flag == 'Y':\n",
        "      cm_best_topics = confusion_matrix(df['True Topic'], df[best_topic_model], labels=df['True Topic'].unique())\n",
        "      sns.heatmap(cm_best_topics, annot=True, fmt='d', cmap='Greens', ax=axes[1, a2], xticklabels=df['True Topic'].unique(), yticklabels=df['True Topic'].unique(), annot_kws={\"size\": 16})\n",
        "      axes[1, a2].set_title(f'{best_topic_model.split()[0]}: Best Model\\nTopic Classification', fontsize=14)\n",
        "      axes[1, a2].set_xlabel('Predicted', fontsize=14)\n",
        "      axes[1, a2].set_ylabel('True', fontsize=14)\n",
        "      axes[1, a2].tick_params(axis='y', labelsize=12)\n",
        "      axes[1, a2].tick_params(axis='x', labelsize=12)\n",
        "    else:\n",
        "      axes[1, a2].axis('off')\n",
        "\n",
        "    # Plot confusion matrix for best Sentiment model\n",
        "    a2 = 1\n",
        "    if 'True Sentiment' in df.columns and best_sentiment_model in df.columns and sentiment_flag == 'Y':\n",
        "      cm_best_sentiment = confusion_matrix(df['True Sentiment'], df[best_sentiment_model], labels=df['True Sentiment'].unique())\n",
        "      sns.heatmap(cm_best_sentiment, annot=True, fmt='d', cmap='Blues', ax=axes[1, a2], xticklabels=df['True Sentiment'].unique(), yticklabels=df['True Sentiment'].unique(), annot_kws={\"size\": 16})\n",
        "      axes[1, a2].set_title(f'{best_sentiment_model.split()[0]}: Best Model\\nSentiment Analysis', fontsize=14)\n",
        "      axes[1, a2].set_xlabel('Predicted', fontsize=14)\n",
        "      axes[1, a2].set_ylabel('True', fontsize=14)\n",
        "      axes[1, a2].tick_params(axis='y', labelsize=12)\n",
        "      axes[1, a2].tick_params(axis='x', labelsize=12)\n",
        "    else:\n",
        "      axes[1, a2].axis('off')\n",
        "\n",
        "    # Plot confusion matrix for best Evasion model\n",
        "    a2 = 2\n",
        "    if 'True Evasion Present' in df.columns and best_evasion_model in df.columns and evasion_flag == 'Y':\n",
        "      cm_best_evasion = confusion_matrix(df['True Evasion Present'], df[best_evasion_model], labels=df['True Evasion Present'].unique())\n",
        "      sns.heatmap(cm_best_evasion, annot=True, fmt='d', cmap='Reds', ax=axes[1, a2], xticklabels=df['True Evasion Present'].unique(), yticklabels=df['True Evasion Present'].unique(), annot_kws={\"size\": 16})\n",
        "      axes[1, a2].set_title(f'{best_evasion_model.split()[0]}: Best Model\\nQuestion Evasion', fontsize=14)\n",
        "      axes[1, a2].set_xlabel('Predicted', fontsize=14)\n",
        "      axes[1, a2].set_ylabel('True', fontsize=14)\n",
        "      axes[1, a2].tick_params(axis='y', labelsize=12)\n",
        "      axes[1, a2].tick_params(axis='x', labelsize=12)\n",
        "    else:\n",
        "      axes[1, a2].axis('off')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9gD33ZyLtvC"
      },
      "source": [
        "### 2.4.2 QA Evasion, Sentiment Analysis and Topic Modelling on Synthetic Data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFpoIWw4NYcB"
      },
      "source": [
        "#### 2.4.2.0 Preparing synthetic data tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxOeP7JFDk8R"
      },
      "source": [
        "##### 2.4.2.0.1 Running RoBERTa on the synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6Om-6qQq3Bc"
      },
      "outputs": [],
      "source": [
        "def get_synthetic(data_folder):\n",
        "  data = pd.read_csv(data_folder + \"Synthetic_Data.csv\")\n",
        "  data['true_sentiment'] = data['True Sentiment'].str.lower()\n",
        "  return data\n",
        "\n",
        "# load the synthetic dataset\n",
        "synthetic_data = get_synthetic(processed_data_folder)\n",
        "synthetic_data['true_score'] = synthetic_data['true_sentiment'].map(score_dict)\n",
        "\n",
        "# get response from RoBERTa\n",
        "roberta_model = ClassificationModel(\"financial-roberta-large\", \"soleimanian/financial-roberta-large-sentiment\")\n",
        "synthetic_data = roberta_model.get_model_response_for_df(synthetic_data, 'qa_text')\n",
        "synthetic_data.rename(columns={'financial-roberta-large_sentiment':'RoBERTa Sentiment'}, inplace=True)\n",
        "synthetic_data = synthetic_data[['uid', 'RoBERTa Sentiment']]\n",
        "\n",
        "# save the result\n",
        "synthetic_data.to_csv(output_data_folder + '/sentiment_eval_result_synthetic.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvoOsFLyDov_"
      },
      "source": [
        "##### 2.4.2.0.2 Running Finbert on the synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQuiLKKuD1aD"
      },
      "outputs": [],
      "source": [
        "def preprocess_synthetic_data(data_folder):\n",
        "    df_synthetic = pd.read_csv(data_folder + \"/Synthetic_Data.csv\")\n",
        "\n",
        "    df_synthetic_processed = pd.DataFrame(columns = ['uid', 'qa_type', 'qa_num', 'qa_text'])\n",
        "    for row in df_synthetic.itertuples():\n",
        "        q,a = row.qa_text.split(\"?\")\n",
        "        q += '?'\n",
        "        rows_to_append = pd.DataFrame({\n",
        "            'uid': [row.uid+'_Q', row.uid+'_A'],\n",
        "            'qa_type': ['Q','A'],\n",
        "            'qa_num': [row.qa_num]*2,\n",
        "            'qa_text': [q, a]\n",
        "        })\n",
        "        df_synthetic_processed = pd.concat([df_synthetic_processed, rows_to_append], ignore_index=True)\n",
        "\n",
        "    save_path = os.path.join(data_folder, f\"synthetic_data_for_finbert.csv\")\n",
        "    df_synthetic_processed.to_csv(save_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYXb7JxYDgeH"
      },
      "outputs": [],
      "source": [
        "preprocess_synthetic_data(processed_data_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XLgOGeNzw_7r"
      },
      "outputs": [],
      "source": [
        "process_finbert(\n",
        "        finbert_tokeniser=finbert_topic_tokeniser,\n",
        "        finbert_model=finbert_topic_model,\n",
        "        save_folder=output_data_folder,\n",
        "        label_dict=id2label,\n",
        "        data_folder=processed_data_folder,\n",
        "        chunking=False,\n",
        "        max_length=512,\n",
        "        qa_only=True,\n",
        "        synthetic=True,\n",
        "        appdx='_synthetic'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyBGly_oubq8"
      },
      "outputs": [],
      "source": [
        "# Compute question evasion\n",
        "plot_answers_to_questions(finbert_folder=output_data_folder,\n",
        "                          data_folder=processed_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          chunking=False,\n",
        "                          max_length=512,\n",
        "                          bank=\"JPMorgan\",\n",
        "                          datatype=\"QA\",\n",
        "                          appdx=\"_synthetic\",\n",
        "                          synthetic=True,\n",
        "                          save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrFn8YiiDr_E"
      },
      "source": [
        "##### 2.4.2.0.3 Create synthetic data table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK6FQidRNb5L"
      },
      "outputs": [],
      "source": [
        "# Load Synthetic data files (change to filepaths in Inputs/Outputs folder later)\n",
        "synthetic_QA_df = pd.read_excel(processed_data_folder + \"Synthetic_QA_dataset.xlsx\")\n",
        "\n",
        "# Get RoBERTa synthetic output file\n",
        "synthetic_Roberta_result = pd.read_csv(processed_data_folder + \"sentiment_eval_result_synthetic.csv\")\n",
        "\n",
        "# Get finBERT synthetic output files\n",
        "synthetic_finBERT_result_corr = pd.read_csv(output_data_folder + \"finbert_QA_topic_correlations_synthetic.csv\")\n",
        "synthetic_finBERT_result_topics = pd.read_csv(output_data_folder + \"finbert_topics_JPMorgan_chunkingFalse_maxlength512_synthetic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA61PN8vNjhU"
      },
      "outputs": [],
      "source": [
        "# Merge RoBERTa Sentiment column into Synthetic results table\n",
        "synthetic_QA_df = pd.merge(synthetic_QA_df, synthetic_Roberta_result[['uid', 'RoBERTa Sentiment']], on='uid', how='left')\n",
        "\n",
        "# Drop _Q entries in uid column from synthetic_finBERT_result_topics and remove _A to match uid in other tables\n",
        "synthetic_finBERT_result_topics = (\n",
        "    synthetic_finBERT_result_topics[~synthetic_finBERT_result_topics['uid'].str.contains('_Q')]\n",
        "    .assign(uid=lambda x: x['uid'].str.replace('_A', '', regex=False))\n",
        ")\n",
        "\n",
        "# Merge specific columns from finBERT tables into Synthetic results table\n",
        "synthetic_QA_df = pd.merge(synthetic_QA_df, synthetic_finBERT_result_corr[['uid', 'qa_corr']], on='uid', how='left')\n",
        "synthetic_QA_df = pd.merge(synthetic_QA_df, synthetic_finBERT_result_topics[['uid', 'finbert_topic_label']], on='uid', how='left')\n",
        "synthetic_QA_df.rename(columns={'finbert_topic_label': 'finBERT Topic Classification'}, inplace=True)\n",
        "\n",
        "# Calculate evasion for finBERT on synthetic data (also re-sets index to uid)\n",
        "synthetic_QA_df = calculate_finBERT_evasion(synthetic_QA_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5yujRbANnLy"
      },
      "source": [
        "#### 2.4.2.1 Running Phi-3.5 on synthetic data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "seeeF84YsUHF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Assess accuracy of Phi3.5 with provided prompt against GPT4-generated synthetic dataset,\n",
        "  with benchmarking against 'True' human-confirmed labels and finBERT/RoBERTa.\n",
        "\n",
        "A small synthetic financial Q&A dataset with combined/aggregated questions and answers in a 'qa_text' column\n",
        "Question-answer pairs were generated by manual adaption of GPT-4o-mini responses\n",
        "  4x Positive sentiment with: No Evasion, Low Evasion, Moderate Evasion, High Evasion\n",
        "  4x Negative sentiment with: No Evasion, Low Evasion, Moderate Evasion, High Evasion\n",
        "  4x Neutral sentiment with: No Evasion, Low Evasion, Moderate Evasion, High Evasion\n",
        "All were checked manually and wording adapted where required to conform to human-accurate sentiment and evasion categorisation\n",
        "\n",
        "This dataset (synthetic_QA_df) is designed to assess Phi3.5 accuracy against human-labelled data. It has first been run through RoBERTa and finBERT to create a benchmarking set.\n",
        "\"\"\"\n",
        "\n",
        "# Run Phi-3.5 function on synthetic data for classification into finBERT topic categories\n",
        "print(\"Running Text Classification analysis\")\n",
        "synthetic_QA_df_topics = phi_question_answer(synthetic_QA_df, finBERT_topic_questions, input_aggregated='Y', input_col='qa_text')\n",
        "\n",
        "# Run Phi-3.5 function on synthetic data for evasion\n",
        "print(\"Running Evasion analysis\")\n",
        "synthetic_QA_df_evasion = phi_question_answer(synthetic_QA_df, evasion_questions, input_aggregated='Y', input_col='qa_text')\n",
        "\n",
        "## Run Phi-3.5 function on synthetic data for sentiment\n",
        "print(\"Running Sentiment analysis\")\n",
        "synthetic_QA_df_sentiment = phi_question_answer(synthetic_QA_df, sentiment_questions, input_aggregated='Y', input_col='qa_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BHyJPhskHRXO"
      },
      "outputs": [],
      "source": [
        "# Merge unique columns in synthetic tables\n",
        "unique_columns_in_topics = [\n",
        "    col for col in synthetic_QA_df_topics.columns if col not in synthetic_QA_df_evasion.columns\n",
        "]\n",
        "\n",
        "unique_columns_in_sentiment = [\n",
        "    col for col in synthetic_QA_df_sentiment.columns if col not in synthetic_QA_df_evasion.columns\n",
        "]\n",
        "\n",
        "synthetic_QA_df_results = pd.merge(synthetic_QA_df_evasion, synthetic_QA_df_topics[unique_columns_in_topics], left_index=True, right_on=\"uid\", how=\"outer\")\n",
        "synthetic_QA_df_results = pd.merge(synthetic_QA_df_results, synthetic_QA_df_sentiment[unique_columns_in_sentiment], left_index=True, right_on=\"uid\", how=\"outer\")\n",
        "\n",
        "synthetic_QA_df_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcE6gzSelYdK"
      },
      "outputs": [],
      "source": [
        "synthetic_QA_df_results.to_csv(output_data_folder + 'phi_synthetic_QA_results.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhNdiTpGN195"
      },
      "source": [
        "#### 2.4.2.2 Compare results with other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvu7K4Ausp4a"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracies of Overall Sentiment, Topic and Question Evasion results\n",
        "accuracy_results = calculate_accuracy(synthetic_QA_df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWddaxkVO6by"
      },
      "source": [
        "**SYNTHETIC DATA RESULTS**\n",
        "\n",
        "<u>Question Evasion</u>\n",
        "\n",
        "Initial results showed that Phi3.5 was overestimating answers as Evasive (5/6 Not Evasive categorised as Evasive) - this led to initial conclusion of poor performance.\n",
        "\n",
        "The prompt wording was then adapted to attempt to fine-tune the results, e.g. by adding in examples of Not Evasive answers. This approach was successful, and made the model more accurate (results above).\n",
        "\n",
        "When the prompt for Phi3.5 focussed on directness of the answer, the results were similar to finBERT, perhaps suggesting Phi was interpreting directness of answer in terms of question topics covered. This 'direct' prompt was removed and is not shown here.\n",
        "\n",
        "Note that Gemini was also tested on this dataset but was excluded from analysis as it gave very poor Evasion results (likely because its prompt was not optimised). This illustrated the importance of prompt optimisation for such tasks, but was outside of the scope of this project. Therefore only GPT4 was used for benchmarking accuracy.\n",
        "\n",
        "**Phi-3.5 outperformed finBERT for Evasion analysis**, as might be expected as finBERT was using topic classification correlation as a proxy for Evasion.\n",
        "\n",
        "<u>Sentiment Analysis</u>\n",
        "\n",
        "**RoBERTa outperformed Phi-3.5 for Sentiment categorisation, though Phi3.5 still gave good results.**\n",
        "\n",
        "<u> Topic Classification</u>\n",
        "\n",
        "**finBERT outperformed Phi-3.5 for Topic classification**, again this is not surprising as finBERT is specifically designed for use with financial datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX7O6vRZLhGB"
      },
      "source": [
        "### 2.4.3 QA Evasion, Sentiment Analysis and Topic Modelling on Ground Truth Data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5jDDuhMTJS"
      },
      "source": [
        "#### 2.4.3.0 Preparing ground truth data tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cC8cAFNQEpp"
      },
      "outputs": [],
      "source": [
        "def merge_datasets_for_plotting(finbert_folder, data_folder,\n",
        "                                chunking=True, max_length=512,\n",
        "                                bank=\"JPMorgan\", appdx=\"\"):\n",
        "\n",
        "    path_qa_corr = os.path.join(finbert_folder, f\"finbert_QA_topic_correlations{appdx}.csv\")\n",
        "    df_qa_corr = pd.read_csv(path_qa_corr)\n",
        "\n",
        "    path_finbert_ground_truth = os.path.join(data_folder, f\"finbert_topics_ground_truth_{bank}_chunking{chunking}_maxlength{max_length}_QA{appdx}.csv\")\n",
        "    df_finbert_ground_truth = pd.read_csv(path_finbert_ground_truth)\n",
        "\n",
        "    merged = df_finbert_ground_truth.merge(df_qa_corr, on='uid')\n",
        "    path_finbert_ground_truth_answers_only = os.path.join(data_folder, f\"finbert_topics_ground_truth_{bank}_chunking{chunking}_maxlength{max_length}{appdx}.csv\")\n",
        "    merged.to_csv(path_finbert_ground_truth, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVNNaDWzMX_1"
      },
      "outputs": [],
      "source": [
        "# Load ground truth files (change to filepaths in Inputs/Outputs folders later)\n",
        "ground_truth_df = pd.read_excel(processed_data_folder + \"ground_truth_JPMorgan_manual.xlsx\")\n",
        "\n",
        "# Get RoBERTa ground truth output file\n",
        "ground_truth_Roberta_result = pd.read_csv(output_data_folder + \"sentiment_eval_result_ground_truth.csv\")\n",
        "\n",
        "\n",
        "# merging Q/A evasion correlations with Finbert + ground truth output\n",
        "merge_datasets_for_plotting(output_data_folder, processed_data_folder,\n",
        "                                chunking=False, max_length=512,\n",
        "                                bank=\"JPMorgan\", appdx=\"\")\n",
        "\n",
        "merge_datasets_for_plotting(output_data_folder, processed_data_folder,\n",
        "                                chunking=False, max_length=512,\n",
        "                                bank=\"JPMorgan\", appdx=\"_summarised\")\n",
        "\n",
        "# Get finBERT ground truth output files for correlation scores\n",
        "ground_truth_finBERT_result_corr = pd.read_csv(output_data_folder + \"finbert_topics_ground_truth_JPMorgan_chunkingFalse_maxlength512.csv\")\n",
        "ground_truth_finBERT_result_corr = ground_truth_finBERT_result_corr[['uid','qa_corr']]\n",
        "ground_truth_finBERT_result_corr_sum = pd.read_csv(output_data_folder + \"finbert_topics_ground_truth_JPMorgan_chunkingFalse_maxlength512_summarised.csv\")\n",
        "ground_truth_finBERT_result_corr_sum = ground_truth_finBERT_result_corr_sum[['uid','qa_corr']]\n",
        "\n",
        "# Get finBERT ground truth output files for Topics\n",
        "ground_truth_finBERT_result_topics = pd.read_csv(output_data_folder + \"finbert_topics_ground_truth_JPMorgan_chunkingFalse_maxlength512_QA.csv\")\n",
        "ground_truth_finBERT_result_topics_sum = pd.read_csv(output_data_folder + \"finbert_topics_ground_truth_JPMorgan_chunkingFalse_maxlength512_QA_summarised.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVvugXL4MaGy"
      },
      "outputs": [],
      "source": [
        "# Evasion and Topic Classification\n",
        "# Re-name columns to distinguish columns between summarised and non-summarised results\n",
        "ground_truth_finBERT_result_topics.rename(columns={'finbert_topic_label': 'finBERT Topic Classification'}, inplace=True)\n",
        "ground_truth_finBERT_result_topics_sum.rename(columns={'finbert_topic_label': 'finBERT(summarised) Topic Classification'}, inplace=True)\n",
        "ground_truth_finBERT_result_corr_sum.rename(columns={'qa_corr': 'qa_corr_summarised'}, inplace=True)\n",
        "\n",
        "# Merge results columns from finBERT into ground truth table\n",
        "ground_truth_df = pd.merge(ground_truth_df, ground_truth_finBERT_result_topics[['uid', 'finBERT Topic Classification']], on='uid', how='left')\n",
        "ground_truth_df = pd.merge(ground_truth_df, ground_truth_finBERT_result_topics_sum[['uid', 'finBERT(summarised) Topic Classification']], on='uid', how='left')\n",
        "ground_truth_df = pd.merge(ground_truth_df, ground_truth_finBERT_result_corr[['uid', 'qa_corr']], on='uid', how='left')\n",
        "ground_truth_df = pd.merge(ground_truth_df, ground_truth_finBERT_result_corr_sum[['uid', 'qa_corr_summarised']], on='uid', how='left')\n",
        "\n",
        "# Sentiment Analysis\n",
        "# Merge results column from RoBERTa into ground truth table\n",
        "ground_truth_df = pd.merge(ground_truth_df, ground_truth_Roberta_result[['uid', 'financial-roberta-large_sentiment']], on='uid', how='left')\n",
        "\n",
        "# Capitalise each entry in sentiment column and re-name column\n",
        "ground_truth_df['financial-roberta-large_sentiment'] = ground_truth_df['financial-roberta-large_sentiment'].str.capitalize()\n",
        "ground_truth_df.rename(columns={'financial-roberta-large_sentiment': 'RoBERTa Sentiment'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F37mjYFHMb2D"
      },
      "outputs": [],
      "source": [
        "# Tidy up non-aggregated df column names and create an aggregated df with combined Q&A on each row for evasion analysis\n",
        "# Contains manual check on aggregated data for multiple entries caused by multi-part answers - these must be removed where possible for accurate comparison.\n",
        "ground_truth_df, ground_truth_df_agg = aggregate_test_data(ground_truth_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5douobsXMg5p"
      },
      "outputs": [],
      "source": [
        "# View data of multi-result fields in aggregated ground truth results - replace with result for longest text string in qa_text\n",
        "# List of prefixes to search for in the 'uid' column\n",
        "prefixes = [\n",
        "    \"JPMorganChase_4Q23_A_30\",\n",
        "    \"JPMorganChase_3Q23_A_11\",\n",
        "    \"JPMorganChase_2Q23_A_23\",\n",
        "    \"JPMorganChase_2Q23_A_43\",\n",
        "    \"JPMorganChase_1Q23_A_42\",\n",
        "    \"JPMorganChase_3Q22_A_29\",\n",
        "    \"JPMorganChase_2Q22_A_1\",\n",
        "    \"JPMorganChase_2Q22_A_21\",\n",
        "    \"JPMorganChase_2Q22_A_41\",\n",
        "    \"JPMorganChase_4Q21_A_41\",\n",
        "    \"JPMorganChase_4Q21_A_43\",\n",
        "    \"JPMorganChase_3Q21_A_8\",\n",
        "    \"JPMorganChase_2Q21_A_24\",\n",
        "    \"JPMorganChase_2Q21_A_37\"\n",
        "]\n",
        "\n",
        "manual_corrections = {\n",
        "    'uid': '',\n",
        "    'True Sentiment': '',\n",
        "    'RoBERTa Sentiment': '',\n",
        "    'True Topic': '',\n",
        "    'finBERT Topic Classification': '',\n",
        "    'finBERT(summarised) Topic Classification': ''\n",
        "}\n",
        "\n",
        "# Loop through each prefix and filter rows in the non-aggregated DataFrame\n",
        "for prefix in prefixes:\n",
        "    filtered_rows_non_agg = ground_truth_df[ground_truth_df['uid'].str.contains(prefix, regex=True)]\n",
        "\n",
        "    # Find the row with the longest 'qa_text' for each prefix\n",
        "    if not filtered_rows_non_agg.empty:\n",
        "        longest_text_row = filtered_rows_non_agg.loc[filtered_rows_non_agg['qa_text'].str.len().idxmax()]\n",
        "        updated_uid = prefix.replace('_Q_', '_QA_') + \".0\"\n",
        "        updated_uid = prefix.replace('_A_', '_QA_') + \".0\"\n",
        "\n",
        "        # Update the ground_truth_df_agg with values from the non-agg DataFrame\n",
        "        if updated_uid in ground_truth_df_agg['uid'].values:\n",
        "\n",
        "            ground_truth_df_agg.loc[\n",
        "                ground_truth_df_agg['uid'] == updated_uid,\n",
        "                ['True Sentiment', 'RoBERTa Sentiment', 'True Topic', 'finBERT Topic Classification', 'finBERT(summarised) Topic Classification']\n",
        "            ] = [\n",
        "                longest_text_row['True Sentiment'],\n",
        "                longest_text_row['RoBERTa Sentiment'],\n",
        "                longest_text_row['True Topic'],\n",
        "                longest_text_row['finBERT Topic Classification'],\n",
        "                longest_text_row['finBERT(summarised) Topic Classification']\n",
        "            ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsuix1Er01Bp"
      },
      "outputs": [],
      "source": [
        "# Calculate evasion for finBERT (also re-sets index to uid)\n",
        "ground_truth_df_agg = calculate_finBERT_evasion(ground_truth_df_agg)\n",
        "print(\"Aggregated ground truth dataframe\")\n",
        "display(ground_truth_df_agg.head())\n",
        "\n",
        "ground_truth_df = calculate_finBERT_evasion(ground_truth_df)\n",
        "print(\"Ground truth dataframe\")\n",
        "display(ground_truth_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaTDg5-MMn8h"
      },
      "source": [
        "#### 2.4.3.1 Running Phi-3.5 on ground truth data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TnPj-iD1Wo1"
      },
      "outputs": [],
      "source": [
        "\"\"\"Aggregated ground truth dataset - Evasion (aggregated ground truth)\n",
        "    Evasion for Phi must be run on an aggregated dataset, otherwise it will ignore the question.\n",
        "\"\"\"\n",
        "# Run Phi-3.5 function for Evasion on aggregated data\n",
        "ground_truth_df_evasion = phi_question_answer(ground_truth_df_agg, evasion_questions, input_aggregated='Y', input_col='qa_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs_yRkB4KEAH"
      },
      "outputs": [],
      "source": [
        "\"\"\" Sentiment and Topic Analysis\n",
        "    Non-aggregated ground truth dataset - Sentiment (non-aggregated ground truth) and Topics\n",
        "    For true comparability assessment here against RoBERTa, Phi Sentiment analysis is run on non-aggregated set\n",
        "    However, it is also run on aggregated set for assessment of the method of aggregating RoBERTa sentiments and finBERT topics for the QA_ground_truth set, which would be required to run the combined_questions prompt\n",
        "\"\"\"\n",
        "# Run Phi-3.5 function for Sentiment/ Topics\n",
        "print(\"Sentiment analysis on Q&A ground truth table\")\n",
        "ground_truth_df_sentiment = phi_question_answer(ground_truth_df, sentiment_questions, input_aggregated='N', input_col='qa_text')\n",
        "print(\"Topic analysis on Q&A ground truth table\")\n",
        "ground_truth_df_topics = phi_question_answer(ground_truth_df, finBERT_topic_questions, input_aggregated='N', input_col='qa_text')\n",
        "\n",
        "print(\"Sentiment analysis on aggregated QA ground truth table\")\n",
        "ground_truth_df_sentiment_agg = phi_question_answer(ground_truth_df_agg, sentiment_questions, input_aggregated='Y', input_col='qa_text')\n",
        "print(\"Topic analysis on aggregated QA ground truth table\")\n",
        "ground_truth_df_topics_agg = phi_question_answer(ground_truth_df_agg, finBERT_topic_questions, input_aggregated='Y', input_col='qa_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgtgBZkmTa1k"
      },
      "outputs": [],
      "source": [
        "# Merge unique columns into single aggregated table result\n",
        "unique_columns_in_topics_agg = [\n",
        "    col for col in ground_truth_df_topics_agg.columns if col not in ground_truth_df_evasion.columns\n",
        "]\n",
        "\n",
        "unique_columns_in_sentiment_agg = [\n",
        "    col for col in ground_truth_df_sentiment_agg.columns if col not in ground_truth_df_evasion.columns\n",
        "]\n",
        "\n",
        "ground_truth_df_all_results_agg = pd.merge(ground_truth_df_evasion, ground_truth_df_topics_agg[unique_columns_in_topics_agg], left_index=True, right_on=\"uid\", how=\"outer\")\n",
        "ground_truth_df_all_results_agg = pd.merge(ground_truth_df_all_results_agg, ground_truth_df_sentiment_agg[unique_columns_in_sentiment_agg], left_index=True, right_on=\"uid\", how=\"outer\")\n",
        "print(\"Aggregated data results table (all analyses on aggregated QA set):\")\n",
        "display(ground_truth_df_all_results_agg.head())\n",
        "\n",
        "\n",
        "# Merge unique columns into single non-aggregated table result\n",
        "# Note that finBERT classification only has results for answers\n",
        "unique_columns_in_topics = [\n",
        "    col for col in ground_truth_df_topics.columns if col not in ground_truth_df_sentiment.columns\n",
        "]\n",
        "\n",
        "ground_truth_df_all_results = pd.merge(ground_truth_df_sentiment, ground_truth_df_topics[unique_columns_in_topics], left_index=True, right_on=\"uid\", how=\"outer\")\n",
        "print(\"Sentiment/Topics results table (all analyses on non-aggregated Q&A set):\")\n",
        "display(ground_truth_df_all_results.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZYJVVFUlw0V"
      },
      "outputs": [],
      "source": [
        "ground_truth_df_all_results.to_csv(output_data_folder + 'phi_ground_truth_results.csv')\n",
        "ground_truth_df_all_results_agg.to_csv(output_data_folder + 'phi_ground_truth_results_agg.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiujNt7FMzsx"
      },
      "source": [
        "#### 2.4.3.2 Compare results with other models (Output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AsWNUKzSnfc"
      },
      "outputs": [],
      "source": [
        "# Assess validity of aggregating RoBERTa and finBERT results (needed for direct comparison with Evasive results later)\n",
        "print(\"Non-aggregated answers:\")\n",
        "accuracy_results_gt = calculate_accuracy(ground_truth_df_all_results, sentiment_flag='Y', evasion_flag='N', topic_flag='Y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MnNPIj2St86"
      },
      "outputs": [],
      "source": [
        "print(\"Aggregated answer Results:\")\n",
        "accuracy_results_gt_agg = calculate_accuracy(ground_truth_df_all_results_agg, sentiment_flag='Y', evasion_flag='Y', topic_flag='Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBGQsUITNPVa"
      },
      "source": [
        "**<u>Topic Classification</u>**\n",
        "\n",
        "Aggregating answers improved finBERT performance for summarised texts, with aggregated summarised answer analysis giving the best accuracy (28.6%), (finBERT results were aggregated where there was more than one answer to a question by taking the topic class of the longest answer). The increased accuracy for summarised data likely reflects the removal of irrelavant (non-financial) language from the text by Phi-3.5, and increased accuracy for aggregated answers gives better content for more accurate topic classification.\n",
        "\n",
        "Phi3.5 performance was slightly worsened by summarisation, which likely reflects its primary usage as a generalist language model performing better with larger context.\n",
        "\n",
        "**Therefore: use finBERT(summarised) with answer aggregation is better than Phi-3.5 for further classification analysis.**\n",
        "\n",
        "**<u>Sentiment</u>**\n",
        "\n",
        "The opposite effect (decreased performance) was seen for answer-aggregated sentiment analysis (where the sentiment of the longest answer was taken for the whole answer sentiment, where multiple answers were present). RoBERTA decreased from 66.7% to 61.9%.\n",
        "\n",
        "**Therefore: preferable to use RoBERTa without answer aggregation, but for Negative Evasion analysis (which requires aggregated answers) RoBERTa is still preferable to Phi-3.5.**\n",
        "\n",
        "**<u>Question-Evasion</u>**\n",
        "\n",
        "Can only be performed on aggregated QA set by Phi-3.5 - outperforms all other models with 62% accuracy.\n",
        "\n",
        "Accuracy of evasion analysis was much improved by running the evasion prompt question separately from the sentiment question, but this means proceeding to analysing full transcript sets of Q&As is not viable as running the analysis multiple times for each prompt would negate the proposed time/resource-saving benefits of using Phi as a replacement for other analysis methods.\n",
        "\n",
        "**As Phi is the most accurate question-evasion method tested, we will apply it to analyse Q&As from transcripts covering specific quarters of interest (identified in EDA section).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw9yKjjxpOhy"
      },
      "source": [
        "# 3. Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTboap-VpPg2"
      },
      "source": [
        "## 3.0 Running model on the full dataset to get responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj8leHSVpUFO"
      },
      "source": [
        "### 3.0.1 RoBERTa sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpaKQ8uFvWMe"
      },
      "outputs": [],
      "source": [
        "full_data = pd.read_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan_clean.csv\")\n",
        "full_data = full_data[full_data['qa_type'].isin(['Q', 'A'])]\n",
        "roberta_model = ClassificationModel(\"financial-roberta-large\", \"soleimanian/financial-roberta-large-sentiment\")\n",
        "full_data = roberta_model.get_model_response_for_df(full_data, 'qa_text')\n",
        "full_data.to_csv(output_data_folder + \"/sentiment_full_result.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kF_DTFpWrw"
      },
      "source": [
        "### 3.0.2 Phi-3.5 by quarter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0PXjSvGvvjM"
      },
      "outputs": [],
      "source": [
        "qa_df = pd.read_csv(processed_data_folder + \"/transcripts_tabular_JPMorgan_clean.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5qHRanVaj-I2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Interrogate QA pairs from specific quarters from pre-processed table for Sentiment and Question-Evasion Analysis (with associated topics - non-categorical)\n",
        "This analysis was only performed on the aggregated dataset, in order to analyse Phi-3.5 identified Negative Evasive topics\n",
        "    Note that a more comprehensive approach would analyse the quarters separately using RoBERTa, and then compare these sentiments to Phi-3.5-identified Evasion.\n",
        "    However, for a simple exploration of the capabilities of Phi-3.5, a Phi-3.5-only approach was used.\n",
        "\"\"\"\n",
        "\n",
        "# Define quarters of interest\n",
        "quarters_of_interest = ['1Q22', '2Q24', '3Q24']\n",
        "\n",
        "# Filter the DataFrame for rows where the 'uid' column contains any of the quarters of interest\n",
        "quarter_filtered_df = qa_df[qa_df['uid'].str.contains('|'.join(quarters_of_interest))]\n",
        "\n",
        "# Aggregate df for Phi3.5 evasion analysis\n",
        "agg_quarters_df = aggregate_QA_data(quarter_filtered_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sUi1AJsbYrlr"
      },
      "outputs": [],
      "source": [
        "# Run Phi-3.5 function for Evasion and Sentiment analysis (both create non-categorical topics for each sentiment and evasion)\n",
        "print(f'Running Evasion analysis on {quarters_of_interest}')\n",
        "quarter_evasion_df = phi_question_answer(agg_quarters_df, evasion_questions, input_aggregated='Y', input_col='qa_text')\n",
        "print(f'Running Sentiment analysis on {quarters_of_interest}')\n",
        "quarter_sentiment_df = phi_question_answer(agg_quarters_df, sentiment_questions, input_aggregated='Y', input_col='qa_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xvGyp9osgh2d"
      },
      "outputs": [],
      "source": [
        "# Merge results back into single dataframe\n",
        "quarter_sentiment_df = quarter_sentiment_df.drop(columns=['qa_type', 'qa_num', 'qa_text','keywords'])\n",
        "quarter_df_results = pd.merge(quarter_evasion_df, quarter_sentiment_df, left_index=True, right_index=True, how=\"outer\")\n",
        "\n",
        "# Extract quarter and year information and add to dataframe\n",
        "quarter_df_results['quarter_ID'] = quarter_df_results.index.to_series().str.extract(r'_(\\dQ\\d{2})_')\n",
        "quarter_df_results['year'] = quarter_df_results['quarter_ID'].str[-2:].apply(lambda x: '20' + x)\n",
        "\n",
        "display(quarter_df_results.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dOvCtyawVGB"
      },
      "outputs": [],
      "source": [
        "quarter_df_results.to_csv(output_data_folder + \"/phi_quarter_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iawxOwRxWX8e"
      },
      "source": [
        "## 3.1 22Q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWpkmyJhz3qw"
      },
      "source": [
        "We run a small number of analysis on 22Q1 to show proof of concepts. More detailed analysis on performed on 24Q2 and 24Q3 to identify emerging risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWfldqARyrdB"
      },
      "source": [
        "### 3.1.1 Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v7TfB6JzdjY"
      },
      "outputs": [],
      "source": [
        "full_result = pd.read_csv(output_data_folder + \"/sentiment_full_result.csv\")\n",
        "full_result['date'] = pd.to_datetime(full_result['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol09nCUDzdqH"
      },
      "outputs": [],
      "source": [
        "score_by_quarter = full_result.groupby('date')['financial-roberta-large_score'].mean()\n",
        "year_quarter = full_result[['date', 'year', 'quarter']].drop_duplicates()\n",
        "score_by_quarter = pd.merge(score_by_quarter, year_quarter, on='date', how='left')\n",
        "score_by_quarter['year_quarter'] = score_by_quarter['quarter'].astype(str) + 'Q' + (score_by_quarter['year']-2000).astype(str)\n",
        "score_by_quarter.set_index('year_quarter', inplace=True)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,3))\n",
        "\n",
        "sns.lineplot(x=score_by_quarter.index, y=score_by_quarter['financial-roberta-large_score'], ax=ax1,\n",
        "              color='orange', lw=2, label='average sentiment', legend=False)\n",
        "xticks = [f\"{row['quarter']}Q{row['year']-2000}\" for _, row in score_by_quarter.iterrows()]\n",
        "plt.xticks(ticks=score_by_quarter.index, labels=xticks, rotation=25)\n",
        "ax1.set_xlabel('')\n",
        "ax1.set_ylabel('Average sentiment score',  fontsize=14)\n",
        "ax1.set_ylim(-0.3, 0.3)\n",
        "for label in ax1.get_yticklabels():\n",
        "  label.set_size(fontsize=14)\n",
        "plt.axhline(y=0, lw=0.8, ls=(0, (5,10)), color='black')\n",
        "ax1.axvspan(2,4, facecolor='orange', alpha=0.2, edgecolor=None)\n",
        "ax1.set_title(\"Average sentiment score over time\")\n",
        "for label in ax1.get_xticklabels():\n",
        "  label.set_size(fontsize=12)\n",
        "\n",
        "fig.legend(fontsize=14, loc='center left', bbox_to_anchor=(0.9, 0.5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGWImjk0zlqE"
      },
      "outputs": [],
      "source": [
        "quarter_count = full_result.groupby(['year', 'quarter'])['financial-roberta-large_sentiment'].value_counts(normalize=True).reset_index()\n",
        "quarter_count = quarter_count.pivot(index=['year', 'quarter'], columns='financial-roberta-large_sentiment', values='proportion').reset_index()\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,3))\n",
        "sns.lineplot(x=quarter_count['quarter'].astype(str)+'Q'+(quarter_count['year']-2000).astype(str),\n",
        "             y=quarter_count['negative'],\n",
        "             label='negative',lw=2,\n",
        "             color='lightblue',\n",
        "             legend=False)\n",
        "sns.lineplot(x=quarter_count['quarter'].astype(str)+'Q'+(quarter_count['year']-2000).astype(str),\n",
        "             y=quarter_count['positive'],\n",
        "             label='positive',\n",
        "             color='lightcoral',\n",
        "             ls='dashed', lw=2,\n",
        "             legend=False)\n",
        "sns.lineplot(x=quarter_count['quarter'].astype(str)+'Q'+(quarter_count['year']-2000).astype(str),\n",
        "             y=quarter_count['neutral'],\n",
        "             label='neutral',\n",
        "             color='grey',lw=2,\n",
        "             ls='dashdot',\n",
        "             legend=False)\n",
        "plt.xticks(rotation=25)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Proportion of sentiment', fontsize=14)\n",
        "for label in ax1.get_yticklabels():\n",
        "  label.set_size(fontsize=14)\n",
        "ax1.axvspan(2,4, facecolor='orange', alpha=0.2, edgecolor=None)\n",
        "\n",
        "for label in ax1.get_xticklabels():\n",
        "  label.set_size(fontsize=12)\n",
        "\n",
        "fig.legend(fontsize=14, loc='center left', bbox_to_anchor=(0.9, 0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjci1d1zCmN"
      },
      "source": [
        "### 3.1.2 Topic modelling on negative-sentiment texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH1fMz_dzubx"
      },
      "source": [
        "#### 3.1.2.1 BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJcumW-q5VXB"
      },
      "outputs": [],
      "source": [
        "# Read the sentiment and summarised data\n",
        "sentiment_df = pd.read_csv(output_data_folder + \"/sentiment_full_result.csv\")\n",
        "df_phi_fulltable_summarised = pd.read_excel(processed_data_folder + '/phi_fulltable_summarised.xlsx')\n",
        "\n",
        "# subset relevant columns from sentiment file\n",
        "sentiment_colname = 'financial-roberta-large_sentiment'\n",
        "sentiment_df_22Q1= sentiment_df[(sentiment_df.year == 2022) & (sentiment_df.quarter == 1)]\n",
        "sentiment_df_22Q1 = sentiment_df_22Q1[['uid', sentiment_colname]]\n",
        "\n",
        "# Subset relevant columns from summarised text file\n",
        "df_phi_fulltable_summarised_2022Q1 = df_phi_fulltable_summarised[(df_phi_fulltable_summarised.year == 2022) &\n",
        "                                                                 (df_phi_fulltable_summarised.quarter == 1)]\n",
        "summarised_22Q1_df = df_phi_fulltable_summarised_2022Q1[['uid', 'summarised_text']]\n",
        "\n",
        "# Further filter sentiment to only include negative sentiments\n",
        "sentiment_df_22Q1 = sentiment_df_22Q1[sentiment_df_22Q1[sentiment_colname] == 'negative'][['uid', sentiment_colname]]\n",
        "\n",
        "# Merge neagtive data with ground truth data\n",
        "df_sentiment_22Q1_summarised = pd.merge(sentiment_df_22Q1,summarised_22Q1_df , on=['uid'], how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpfX50Ji6HWJ"
      },
      "outputs": [],
      "source": [
        "df_phi_2022Q1_neg_list = df_sentiment_22Q1_summarised['summarised_text'].apply(preprocess_spacy).to_list()\n",
        "dim_model = PCA(n_components=11)\n",
        "cluster_model = KMeans(n_clusters=11)\n",
        "\n",
        "topic_model_phi_2022Q1_neg = BERTopic(embedding_model=embedding_model,\n",
        "                       hdbscan_model=cluster_model, calculate_probabilities=True)\n",
        "topics, probabilities = topic_model_phi_2022Q1_neg.fit_transform(df_phi_2022Q1_neg_list)\n",
        "# Reduce topics with higher diversity\n",
        "topic_model_phi_2022Q1_neg = topic_model_phi_2022Q1_neg.reduce_topics(df_phi_2022Q1_neg_list, nr_topics=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXXqQ0yYp2bV"
      },
      "outputs": [],
      "source": [
        "# Plot each BERTtopic visualization into a subplot\n",
        "topic_model_phi_2022Q1_neg.visualize_topics().write_html(\"topic_model_phi_2022Q1_neg_topic.html\")\n",
        "topic_model_phi_2022Q1_neg.visualize_barchart(n_words=10, autoscale=True).write_html(\"topic_model_phi_2022Q1_neg_topic_barchart.html\")\n",
        "topic_model_phi_2022Q1_neg.visualize_heatmap().write_html(\"topic_model_phi_2022Q1_neg_topic_heatmap.html\")\n",
        "topic_model_phi_2022Q1_neg.visualize_hierarchy().write_html(\"topic_model_phi_2022Q1_neg_topic_hierarchy.html\")\n",
        "\n",
        "# Load each plot into a subplot\n",
        "\n",
        "display(HTML(\"topic_model_phi_2022Q1_neg_topic.html\"))\n",
        "display(HTML(\"topic_model_phi_2022Q1_neg_topic_barchart.html\"))\n",
        "display(HTML(\"topic_model_phi_2022Q1_neg_topic_heatmap.html\"))\n",
        "display(HTML(\"topic_model_phi_2022Q1_neg_topic_hierarchy.html\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpRGbSci0foi"
      },
      "source": [
        "#### 3.1.2.2 FinBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zCpIV3TRoaX"
      },
      "source": [
        "##### 3.1.2.2.0 Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6a5q3BdRoaX"
      },
      "outputs": [],
      "source": [
        "# Function to standardise formatting for the particular analysts: by omitting middle names and using full names.\n",
        "def standardise_analyst_names(df, bank):\n",
        "    if bank=='JPMorgan':\n",
        "        analyst_dict = {\n",
        "            \"Charles W. Peabody\": \"Charles Peabody\",\n",
        "            \"Ebrahim H. Poonawala\": \"Ebrahim Poonawala\",\n",
        "            \"Jim Mitchell\": \"James Mitchell\",\n",
        "            \"John E. McDonald\": \"John McDonald\",\n",
        "            \"Kenneth M. Usdin\": \"Kenneth Usdin\",\n",
        "            \"Ken Usdin\": \"Kenneth Usdin\",\n",
        "            \"Matt O’Connor\": \"Matt O'Connor\"\n",
        "        }\n",
        "    return df['name'].replace(analyst_dict)\n",
        "\n",
        "# Plotting the topic probability distributions for a specific condition (e.g. analyst name, quarter, etc.)\n",
        "def plot_topics_by_condition(condition_dict, finbert_folder, data_folder, label_dict,\n",
        "                               chunking=True, max_length=512, bank='JPMorgan',\n",
        "                              datatype='all', appdx=\"\", summarised=False,\n",
        "                             synthetic=False, sentiment=None, sentiment_folder=None,\n",
        "                             save=False):\n",
        "    \"\"\"\n",
        "    condition_dict should be a dictionary of len=1\n",
        "    the key should be a column name, the value should be a value in that column\n",
        "    \"\"\"\n",
        "\n",
        "    df_merged, _, metadata = get_merged_data_for_plotting(finbert_folder, data_folder,\n",
        "                                             label_dict, chunking=chunking, appdx=appdx,\n",
        "                                             max_length=max_length, bank=bank, datatype=datatype,\n",
        "                                            summarised=summarised, synthetic=synthetic,\n",
        "                                            sentiment=sentiment, sentiment_folder=sentiment_folder)\n",
        "\n",
        "\n",
        "    key = list(condition_dict.keys())[0]\n",
        "    # standardise analyst names (see 5.1 below - adding it here to reuse this function later)\n",
        "    if key=='name':\n",
        "        df_merged['name'] = standardise_analyst_names(df_merged, bank)\n",
        "    if type(condition_dict[key]) != list:\n",
        "        condition_dict[key] = [condition_dict[key]]\n",
        "    df_qrt = df_merged[df_merged[key].isin(condition_dict[key])].copy()\n",
        "\n",
        "    topic_labels = list(label_dict.keys())\n",
        "    cmap = sns.color_palette(\"tab20\")\n",
        "\n",
        "    prob_cols = [f\"topic_{i}_prob\" for i in label_dict.keys()]\n",
        "\n",
        "    # order topic labels by their prevalence\n",
        "    if sentiment is None or sentiment=='negative':\n",
        "        topic_labels = df_qrt[prob_cols].median(axis=0).sort_values(ascending=False).index\n",
        "    else:   # have the same order as in negative sentiment texts\n",
        "        df_merged_neg, _, _ = get_merged_data_for_plotting(finbert_folder, data_folder,\n",
        "                                             label_dict, chunking=chunking, appdx=appdx,\n",
        "                                             max_length=max_length, bank=bank, datatype=datatype,\n",
        "                                            summarised=summarised, synthetic=synthetic,\n",
        "                                            sentiment='negative', sentiment_folder=sentiment_folder)\n",
        "        if key=='name':\n",
        "            df_merged_neg['name'] = standardise_analyst_names(df_merged_neg, bank)\n",
        "        df_qrt_neg = df_merged_neg[df_merged_neg[key].isin(condition_dict[key])].copy()\n",
        "        topic_labels = df_qrt_neg[prob_cols].median(axis=0).sort_values(ascending=False).index\n",
        "\n",
        "    fig, ax = plt.subplots(1,1, figsize=(5,5), sharex=True)\n",
        "    ax.vlines(0.05, 0, len(topic_labels), ls='dashed', color='grey', lw=0.5)\n",
        "    for num, topic_label in enumerate(topic_labels):\n",
        "        i = int(topic_label.split(\"_\")[1])\n",
        "\n",
        "        # subset the df to keep only one topic\n",
        "        cols_to_keep = [\"finbert_topic_id\", topic_label]\n",
        "        df_topic = df_qrt[cols_to_keep].copy()\n",
        "\n",
        "        # plot\n",
        "        sns.boxplot(x=df_topic[topic_label],\n",
        "                    y=[num]*df_topic.shape[0],\n",
        "                    orient='h',\n",
        "                    showfliers=True,\n",
        "                    color=cmap[i],\n",
        "                    flierprops=dict(marker='o', markerfacecolor=cmap[i], markersize=6, alpha=0.7)\n",
        "                    )\n",
        "    fsize=14\n",
        "    ax.set_yticks(np.arange(len(topic_labels)),\n",
        "                  labels=[label_dict[int(i.split(\"_\")[1])] for i in topic_labels],\n",
        "                  fontsize=fsize)\n",
        "    ax.set_xlabel(\"Topic probability\", fontsize=fsize)\n",
        "    ax.tick_params(axis='x', labelsize=fsize)\n",
        "    # axes2.set_yticklabels(topics, rotation=0, fontsize=fsize)\n",
        "    if save:\n",
        "        fig.savefig(f\"topic_boxplots_{key}_{condition_dict.values()}.png\",\n",
        "                        dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftwp15h4RoaX"
      },
      "source": [
        "##### 3.1.2.2.1 Plotting probability distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTZ5kQJT6lyz"
      },
      "source": [
        "We will start by plotting the topic probability distribution of **negative** texts in the quarter of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZopxoW8ynJX"
      },
      "outputs": [],
      "source": [
        "plot_topics_by_condition(condition_dict={\"quarter_str\": [\"1Q22\"]},\n",
        "                          finbert_folder=output_data_folder,\n",
        "                          data_folder=output_data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          max_length=512,\n",
        "                          chunking=False,\n",
        "                          bank='JPMorgan',\n",
        "                          datatype='QA',\n",
        "                         sentiment='negative',\n",
        "                         appdx='_summarised',\n",
        "                         sentiment_folder=output_data_folder,\n",
        "                         save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMI6-WgW6nox"
      },
      "source": [
        "Topics with the highest probabilities in negative sentiment texts are <u>Macro</u>, <u>General News | Opinion</u>, and <u>Fed | Central Banks</u>. However, theseare fairly frequently assigned in general and might not reflect an effect specific to negative sentiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAHiH5A40OD"
      },
      "source": [
        "### 3.1.3 Evasion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J6lTdK468FR"
      },
      "outputs": [],
      "source": [
        "# Filter Dataframe for quarter 1Q22\n",
        "filtered_df_1Q22 = quarter_df_results[(quarter_df_results['quarter_ID'] == '1Q22')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y__kEFncRoaZ"
      },
      "outputs": [],
      "source": [
        "# Plot a pie chart\n",
        "def plot_pie_chart(data, title):\n",
        "  results_series = data.squeeze()\n",
        "  results_counts = results_series.value_counts()\n",
        "\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.pie(results_counts, labels=results_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'lightblue'])\n",
        "  plt.title(f\"Evasive vs Not-Evasive answers in {title}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z97f6IabET_S"
      },
      "outputs": [],
      "source": [
        "# Plot a pie chart\n",
        "plot_pie_chart(filtered_df_1Q22['Phi-3.5 Evasion Present'], '1Q22')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JMvcFmp6-hi"
      },
      "source": [
        "#### 3.1.3.1 Topic modelling on evasive texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41GvgIh7A9R"
      },
      "source": [
        "##### 3.1.3.1.1 BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krvejjgU7CxR"
      },
      "outputs": [],
      "source": [
        "def extract_keywords(model):\n",
        "    topic_words = set()\n",
        "    for topic_num in range(len(model.get_topics())):\n",
        "        # Extract the words for each topic\n",
        "        words = [word for word, _ in model.get_topic(topic_num)]\n",
        "        topic_words.update(words)\n",
        "        # Print topic number, words, and the number of words\n",
        "        print(f\"Topic {topic_num}: {words} (Count: {len(words)})\")\n",
        "    return topic_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeSP1EGj7GIY"
      },
      "outputs": [],
      "source": [
        "# subset relevant columns from combined evasion file\n",
        "qevasion_colname = 'Phi-3.5 Evasion Present'\n",
        "qevasion_df = filtered_df_1Q22[['qa_num', qevasion_colname]]\n",
        "\n",
        "# subset relevant columns from summarised text file\n",
        "fulltable_summarised_df = df_phi_fulltable_summarised[['uid', 'qa_num', 'summarised_text']]\n",
        "qevasion_status='Evasive'\n",
        "df_qevasion_summarised = pd.merge(fulltable_summarised_df, qevasion_df, on=['qa_num'], how='left')\n",
        "\n",
        "# subset relevant columns from sentiment file\n",
        "qevasion_colname = 'Phi-3.5 Evasion Present'\n",
        "qevasion_df = qevasion_df[['qa_num', qevasion_colname]]\n",
        "\n",
        "# subset relevant columns from full summarised text file\n",
        "fulltable_summarised_df = fulltable_summarised_df[['uid', 'qa_num', 'summarised_text']]\n",
        "\n",
        "# merge two dataset\n",
        "qevasion_status='Evasive'\n",
        "df_qevasion_summarised = pd.merge(fulltable_summarised_df, qevasion_df, on=['qa_num'], how='left')\n",
        "df_qevasion_summarised['quarter_str'] = [x.split(\"_\")[1] for x in df_qevasion_summarised['uid']]\n",
        "df_evaded_summarised = df_qevasion_summarised[df_qevasion_summarised[qevasion_colname].eq(qevasion_status)].copy().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_nrNfi07gNT"
      },
      "outputs": [],
      "source": [
        "quarter_str_status_22 = '1Q22'\n",
        "quarter_colname = 'quarter_str'\n",
        "df_evaded_summarised_22Q1 = df_evaded_summarised[df_evaded_summarised[quarter_colname].eq(quarter_str_status_22)].copy().reset_index(drop=True)\n",
        "df_evaded_summarised_22Q1_list = df_evaded_summarised_22Q1['summarised_text'].apply(preprocess_spacy).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Ddfldp7o4R"
      },
      "outputs": [],
      "source": [
        "dim_model = PCA(n_components=5)\n",
        "cluster_model = KMeans(n_clusters=20)\n",
        "\n",
        "topic_evaded_summarised_22Q1 = BERTopic(umap_model=dim_model, embedding_model=embedding_model,\n",
        "                       hdbscan_model=cluster_model, calculate_probabilities=True)\n",
        "topics, probs = topic_evaded_summarised_22Q1.fit_transform(df_evaded_summarised_22Q1_list)\n",
        "topic_evaded_summarised_22Q1 = topic_evaded_summarised_22Q1.reduce_topics(df_evaded_summarised_22Q1_list, nr_topics=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtzVFf2Nno-l"
      },
      "outputs": [],
      "source": [
        "# Plot each BERTtopic visualization into a subplot\n",
        "topic_evaded_summarised_22Q1.visualize_topics().write_html(\"topic_evaded_summarised_22Q1_topic.html\")\n",
        "topic_evaded_summarised_22Q1.visualize_barchart(top_n_topics=20,n_words=8, autoscale=True).write_html(\"topic_evaded_summarised_22Q1_barchart.html\")\n",
        "topic_evaded_summarised_22Q1.visualize_heatmap().write_html(\"topic_evaded_summarised_22Q1_heatmap.html\")\n",
        "topic_evaded_summarised_22Q1.visualize_hierarchy().write_html(\"topic_evaded_summarised_22Q1_hierarchy.html\")\n",
        "\n",
        "# Load each plot into a subplot\n",
        "\n",
        "display(HTML(\"topic_evaded_summarised_22Q1_topic.html\"))\n",
        "display(HTML(\"topic_evaded_summarised_22Q1_barchart.html\"))\n",
        "display(HTML(\"topic_evaded_summarised_22Q1_heatmap.html\"))\n",
        "display(HTML(\"topic_evaded_summarised_22Q1_hierarchy.html\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBMlfTUW8Kwo"
      },
      "source": [
        "### 3.1.4 Evasion + Negativity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hozCUx2LRoaa"
      },
      "outputs": [],
      "source": [
        "# Extract keywords from each model\n",
        "keywords_evaded_22Q1 = extract_keywords(topic_evaded_summarised_22Q1)\n",
        "keywords_phi_22Q1neg = extract_keywords(topic_model_phi_2022Q1_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csXLg9NaOQJ2"
      },
      "outputs": [],
      "source": [
        "# Create a Venn diagram\n",
        "venn_22 = venn2(\n",
        "    [keywords_evaded_22Q1, keywords_phi_22Q1neg],\n",
        "    ('Evaded', 'Negative')\n",
        ")\n",
        "# Set colors\n",
        "venn_22.get_patch_by_id('10').set_color('#FFBF00')  # Evaded only\n",
        "venn_22.get_patch_by_id('01').set_color('#00BFFF')    # Negative only\n",
        "# Add a title\n",
        "plt.title(\"BERTopic Word Frequencies\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExbISpbrO12J"
      },
      "outputs": [],
      "source": [
        "# Inspect overlaps\n",
        "intersection_evaded_phi_22Q1neg = keywords_evaded_22Q1 & keywords_phi_22Q1neg\n",
        "print(\"Overlap between Evaded and Phi Negative:\", intersection_evaded_phi_22Q1neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YBOYxLkWao2"
      },
      "source": [
        "## 3.2 Recent two quarters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t31DNbflUJUi"
      },
      "source": [
        "By looking at the most two recent quarters, we can identify any emerging risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2sf5enBXG78"
      },
      "source": [
        "### 3.2.0 Initial data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV_UfxHeiPgP"
      },
      "outputs": [],
      "source": [
        "# Getting the unique metric types\n",
        "unique_metric_types = metrics_df['metric_type'].unique()\n",
        "\n",
        "# Defining colors for each plot\n",
        "colors = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "# Plotting in a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 9))\n",
        "\n",
        "# Flattening axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Looping through metric types and corresponding axes\n",
        "for i, metric in enumerate(unique_metric_types):\n",
        "    subset = metrics_df[metrics_df['metric_type'] == metric]\n",
        "    axes[i].plot(subset['Q&FY'], subset['metric_value'], marker='o', color=colors[i])\n",
        "    axes[i].set_title(f'Metric: {metric}', fontsize=16, color=colors[i])\n",
        "    axes[i].set_xlabel('Quarter', fontsize=14)\n",
        "    axes[i].set_ylabel('Metric Value', fontsize=14)\n",
        "    axes[i].tick_params(axis='x', labelsize=12, rotation=45)\n",
        "    axes[i].tick_params(axis='y', labelsize=12)\n",
        "    axes[i].grid(visible=True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-cXVzdyXNtl"
      },
      "source": [
        "Over the last two quarters, the bank has maintained a high CET1 ratio, indicating a strong capital position. However, net income and EPS spiked in 2Q24 before dropping in 3Q24, signaling earnings volatility that could impact profitability. Additionally, elevated provisions for credit losses suggest caution around potential credit risks. Overall, while capital strength is reassuring, addressing credit risk and earnings instability will be crucial for sustaining financial health and investor confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KSCXZjrXqDa"
      },
      "outputs": [],
      "source": [
        "# Creating a dataframs for the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24 = transcripts_df[(transcripts_df['date'] >= '2024-07-01') & (transcripts_df['date'] < '2024-11-01')]\n",
        "\n",
        "# Pulling all words from qa_text_processed column into a list\n",
        "Q23_FY24_all_words = [word for tokens in Q23_FY24['qa_text_processed'] for word in tokens]\n",
        "\n",
        "# Calculating the frequency distribution of the words from the dataset\n",
        "Q23_FY24_freq_dist = FreqDist(Q23_FY24_all_words)\n",
        "\n",
        "# Getting the top 10 words and their frequencies from the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_top_10 = Q23_FY24_freq_dist.most_common(10)\n",
        "Q23_FY24_words, Q23_FY24_counts = zip(*Q23_FY24_top_10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f65KcR4_XsbM"
      },
      "outputs": [],
      "source": [
        "# Setting up the plot\n",
        "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "# Plotting the top 10 words in a barplot for the 2Q24 and 3Q24 Q&As\n",
        "sns.barplot(x=list(Q23_FY24_words), y=list(Q23_FY24_counts), palette=\"viridis\", ax=ax1)\n",
        "ax1.set_title('Top 10 Words in the 2Q24 and 3Q24 Q&As')\n",
        "ax1.set_xlabel('Words')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-qpqwZFXyUA"
      },
      "outputs": [],
      "source": [
        "# Creating a dataframe excluding 2Q24 and 3Q24 Q&As\n",
        "not_Q23_FY24 = transcripts_df[(transcripts_df['date'] < '2024-07-01') | (transcripts_df['date'] >= '2024-11-01')]\n",
        "\n",
        "# Getting unique words for the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_unique_words = set(Q23_FY24_freq_dist.keys()) - set(not_Q23_FY24['qa_text_processed'].explode())\n",
        "Q23_FY24_unique_counts = {word: Q23_FY24_freq_dist[word] for word in Q23_FY24_unique_words}\n",
        "Q23_FY24_unique_df = pd.DataFrame(Q23_FY24_unique_counts.items(), columns=['word', 'Q23_FY24_frequency'])\n",
        "\n",
        "# Getting the top 10 unique words for the 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_top_words = Q23_FY24_unique_df.nlargest(10, 'Q23_FY24_frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6uB4K3TX3gi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mljAOZf0X4vw"
      },
      "outputs": [],
      "source": [
        "# Setting up the plot\n",
        "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "# Creating a bar plot for the top 10 2Q24 and 3Q24 exclusive words\n",
        "sns.barplot(data=Q23_FY24_top_words, x='word', y='Q23_FY24_frequency', palette='Blues')\n",
        "plt.title('Top 10 Unique Words Used in the 2Q24 and 3Q24 Q&As')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Adjusting the layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Showing the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4qt195QX9D6"
      },
      "source": [
        "Words like “spike” and “trough” could reflect the recent fluctuations in net income and EPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HEc7VtxYBIk"
      },
      "outputs": [],
      "source": [
        "# Calculating word frequencies and total word count in 2Q24 and 3Q24 Q&As\n",
        "Q23_FY24_freq = transcripts_df[(transcripts_df['date'] >= '2024-07-01') & (transcripts_df['date'] < '2024-11-01')]\n",
        "Q23_FY24_word_counts = Q23_FY24_freq['qa_text_processed'].explode().value_counts()\n",
        "Q23_FY24_total_words = Q23_FY24_word_counts.sum()  # Total word count for 2Q24 & 3Q24\n",
        "\n",
        "# Calculating word frequencies and total word count in other quarters\n",
        "not_Q23_FY24_word_counts = not_Q23_FY24['qa_text_processed'].explode().value_counts()\n",
        "not_Q23_FY24_total_words = not_Q23_FY24_word_counts.sum()  # Total word count for other quarters\n",
        "\n",
        "# Creating a DataFrame comparing relative frequencies\n",
        "word_comparison_df = pd.DataFrame({\n",
        "    'Q23_FY24_proportion': Q23_FY24_word_counts / Q23_FY24_total_words,\n",
        "    'other_quarters_proportion': not_Q23_FY24_word_counts / not_Q23_FY24_total_words\n",
        "}).fillna(0)\n",
        "\n",
        "# Adding a column for proportion difference\n",
        "word_comparison_df['proportion_difference'] = word_comparison_df['Q23_FY24_proportion'] - word_comparison_df['other_quarters_proportion']\n",
        "\n",
        "# Filtering for words that are relatively more frequent in 2Q24 and 3Q24\n",
        "higher_in_Q23_FY24 = word_comparison_df[word_comparison_df['proportion_difference'] > 0]\n",
        "\n",
        "# Selecting the top 10 words with the highest proportion difference\n",
        "top_higher_words = higher_in_Q23_FY24.nlargest(10, 'proportion_difference').reset_index().rename(columns={'qa_text_processed': 'word'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1HJordTpl5M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WTJVuzdYJyZ"
      },
      "outputs": [],
      "source": [
        "# Plotting the top words by proportional difference\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "sns.barplot(data=top_higher_words, x='word', y='proportion_difference', palette='Purples')\n",
        "plt.title('Top 10 Words with Higher Proportion in 2Q24 and 3Q24 Compared to Other Quarters')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Proportional Difference')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfnRc7-gYLKO"
      },
      "source": [
        "A higher usage of \"capital\" could relate to how the bank has maintained a high CET1 capital ratio, indicating a strong capital position.\n",
        "\n",
        "A higher usage of \"nii\" (net interest income) could relate to the recent fluctuations in net income and EPS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh1hRX5YZCRb"
      },
      "source": [
        "### 3.2.1 Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNzeeY7EplyY"
      },
      "outputs": [],
      "source": [
        "full_result = pd.read_csv(output_data_folder + \"/sentiment_full_result.csv\")\n",
        "full_result['date'] = pd.to_datetime(full_result['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP2jilLXBki2"
      },
      "outputs": [],
      "source": [
        "score_by_quarter = full_result.groupby('date')['financial-roberta-large_score'].mean()\n",
        "year_quarter = full_result[['date', 'year', 'quarter']].drop_duplicates()\n",
        "score_by_quarter = pd.merge(score_by_quarter, year_quarter, on='date', how='left')\n",
        "score_by_quarter['year_quarter'] = score_by_quarter['quarter'].astype(str) + 'Q' + (score_by_quarter['year']-2000).astype(str)\n",
        "score_by_quarter.set_index('year_quarter', inplace=True)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,3))\n",
        "\n",
        "sns.lineplot(x=score_by_quarter.index, y=score_by_quarter['financial-roberta-large_score'], ax=ax1,\n",
        "              color='orange', lw=2, label='average sentiment', legend=False)\n",
        "xticks = [f\"{row['quarter']}Q{row['year']-2000}\" for _, row in score_by_quarter.iterrows()]\n",
        "plt.xticks(ticks=score_by_quarter.index, labels=xticks, rotation=25)\n",
        "ax1.set_xlabel('')\n",
        "ax1.set_ylabel('Average sentiment score',  fontsize=14)\n",
        "ax1.set_ylim(-0.3, 0.3)\n",
        "for label in ax1.get_yticklabels():\n",
        "  label.set_size(fontsize=14)\n",
        "plt.axhline(y=0, lw=0.8, ls=(0, (5,10)), color='black')\n",
        "ax1.axvspan(12,13, facecolor='orange', alpha=0.2, edgecolor=None)\n",
        "ax1.set_title(\"Average sentiment score over time\")\n",
        "for label in ax1.get_xticklabels():\n",
        "  label.set_size(fontsize=12)\n",
        "\n",
        "fig.legend(fontsize=14, loc='center left', bbox_to_anchor=(0.9, 0.5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz8mCIYVBoh-"
      },
      "outputs": [],
      "source": [
        "quarter_count = full_result.groupby(['year', 'quarter'])['financial-roberta-large_sentiment'].value_counts(normalize=True).reset_index()\n",
        "quarter_count = quarter_count.pivot(index=['year', 'quarter'], columns='financial-roberta-large_sentiment', values='proportion').reset_index()\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10,3))\n",
        "sns.lineplot(x=quarter_count['quarter'].astype(str)+'Q'+(quarter_count['year']-2000).astype(str),\n",
        "             y=quarter_count['negative'],\n",
        "             label='negative',lw=2,\n",
        "             color='lightblue',\n",
        "             legend=False)\n",
        "sns.lineplot(x=quarter_count['quarter'].astype(str)+'Q'+(quarter_count['year']-2000).astype(str),\n",
        "             y=quarter_count['positive'],\n",
        "             label='positive',\n",
        "             color='lightcoral',\n",
        "             ls='dashed', lw=2,\n",
        "             legend=False)\n",
        "sns.lineplot(x=quarter_count['quarter'].astype(str)+'Q'+(quarter_count['year']-2000).astype(str),\n",
        "             y=quarter_count['neutral'],\n",
        "             label='neutral',\n",
        "             color='grey',lw=2,\n",
        "             ls='dashdot',\n",
        "             legend=False)\n",
        "plt.xticks(rotation=25)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Proportion of sentiment', fontsize=14)\n",
        "for label in ax1.get_yticklabels():\n",
        "  label.set_size(fontsize=14)\n",
        "ax1.axvspan(12,13, facecolor='orange', alpha=0.2, edgecolor=None)\n",
        "\n",
        "for label in ax1.get_xticklabels():\n",
        "  label.set_size(fontsize=12)\n",
        "\n",
        "fig.legend(fontsize=14, loc='center left', bbox_to_anchor=(0.9, 0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fk0C2CvaJDn"
      },
      "source": [
        "Here we looked at the evolution of sentiment over time with the most recent 2 quarters highlighted.\n",
        "\n",
        "We can see that the average sentiment decreases. However it is not because negative sentiment has increased\n",
        " but because neutral sentiment has become more prevalent at the expense of positive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vnUlZtAUsQe"
      },
      "outputs": [],
      "source": [
        "recent_result = full_result[(full_result['year']==2024) & full_result['quarter'].isin([3,2])]\n",
        "recent_count = recent_result['financial-roberta-large_sentiment'].value_counts()\n",
        "plt.pie(recent_count, labels=recent_count.index, autopct='%1.1f%%', startangle=90, colors=['grey', 'lightcoral', 'lightblue'])\n",
        "plt.title('Question and answer sentiment from 2024Q2 and 2024Q3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AQ3DlUHaHL-"
      },
      "source": [
        "This is the sentiment split in the highlighted quarters. Mostly neutral with a fairly equal split of +ve and -ve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9jAzKnIibdv"
      },
      "source": [
        "It is worth of exploring more of the negative texts since we are interested in emerging risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk17s2GqaLxL"
      },
      "source": [
        "### 3.2.2 Topic modelling on negative-sentiment texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnVT-WHjfaBk"
      },
      "source": [
        "#### 3.2.2.1 BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59dkql5Hdzvi"
      },
      "outputs": [],
      "source": [
        "sentiment_df = pd.read_csv(output_data_folder + \"/sentiment_full_result.csv\")\n",
        "df_phi_fulltable_summarised = pd.read_excel(processed_data_folder + '/phi_fulltable_summarised.xlsx')\n",
        "# subset relevant columns from sentiment file\n",
        "sentiment_df_24Q2Q3= sentiment_df[(sentiment_df.year.isin([2024])) & (sentiment_df.quarter.isin([2, 3]))]\n",
        "sentiment_colname = 'financial-roberta-large_sentiment'\n",
        "sentiment_df_24Q2Q3 = sentiment_df_24Q2Q3[['uid', sentiment_colname]]\n",
        "# subset relevant columns from summarised text file\n",
        "summarised_24Q2Q3_df = df_phi_fulltable_summarised_2024Q2_Q3[['uid', 'summarised_text']]\n",
        "sentiment_df_24Q2Q3 = sentiment_df_24Q2Q3[sentiment_df_24Q2Q3['financial-roberta-large_sentiment'] == 'negative'][['uid', 'financial-roberta-large_sentiment']]\n",
        "# merge neagtive data with ground truth data\n",
        "df_sentiment_24Q2Q3_summarised = pd.merge(sentiment_df_24Q2Q3,summarised_24Q2Q3_df , on=['uid'], how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hp_sCQpevPM"
      },
      "outputs": [],
      "source": [
        "df_phi_2024Q2Q3_neg_list = df_sentiment_24Q2Q3_summarised['summarised_text'].apply(preprocess_spacy).to_list()\n",
        "dim_model = PCA(n_components=18)\n",
        "cluster_model = KMeans(n_clusters=18)\n",
        "\n",
        "topic_model_phi_2024Q2Q3_neg = BERTopic( embedding_model=embedding_model,\n",
        "                       hdbscan_model=cluster_model, calculate_probabilities=True)\n",
        "topics, probabilities = topic_model_phi_2024Q2Q3_neg.fit_transform(df_phi_2024Q2Q3_neg_list)\n",
        "# Reduce topics with higher diversity\n",
        "topic_model_phi_2024Q2Q3_neg = topic_model_phi_2024Q2Q3_neg.reduce_topics(df_phi_2024Q2Q3_neg_list, nr_topics=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzeH4TANphqf"
      },
      "outputs": [],
      "source": [
        "# Plot each BERTtopic visualization into a subplot\n",
        "topic_model_phi_2024Q2Q3_neg.visualize_topics().write_html(\"topic_model_phi_2024Q2Q3_neg_topic.html\")\n",
        "topic_model_phi_2024Q2Q3_neg.visualize_barchart(n_words=8, autoscale=True).write_html(\"topic_model_phi_2024Q2Q3_neg_barchart.html\")\n",
        "topic_model_phi_2024Q2Q3_neg.visualize_heatmap().write_html(\"topic_model_phi_2024Q2Q3_neg_heatmap.html\")\n",
        "topic_model_phi_2024Q2Q3_neg.visualize_hierarchy().write_html(\"topic_model_phi_2024Q2Q3_neg_hierarchy.html\")\n",
        "\n",
        "# Load each plot into a subplot\n",
        "\n",
        "display(HTML(\"topic_model_phi_2024Q2Q3_neg_topic.html\"))\n",
        "display(HTML(\"topic_model_phi_2024Q2Q3_neg_barchart.html\"))\n",
        "display(HTML(\"topic_model_phi_2024Q2Q3_neg_heatmap.html\"))\n",
        "display(HTML(\"topic_model_phi_2024Q2Q3_neg_hierarchy.html\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aTruCAPfdC3"
      },
      "source": [
        "#### 3.2.2.2 FinBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLPvhycnfiBI"
      },
      "source": [
        "Topic distribution in negative sentiment texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWZub8z-zzaR"
      },
      "outputs": [],
      "source": [
        "plot_topics_by_condition(condition_dict={\"quarter_str\": [\"2Q24\"]},\n",
        "                          finbert_folder=output_folder,\n",
        "                          data_folder=data_folder,\n",
        "                          label_dict=id2label,\n",
        "                          max_length=512,\n",
        "                          chunking=False,\n",
        "                          bank='JPMorgan',\n",
        "                          datatype='QA',\n",
        "                         sentiment='negative',\n",
        "                         appdx='_summarised',\n",
        "                         sentiment_folder=sentiment_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz5_v0uzfoU0"
      },
      "source": [
        "Top three topics in negative texts:\n",
        "- Fed | Central Banks\n",
        "- Macro\n",
        "- Financials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdra4f2UfyB6"
      },
      "source": [
        "### 3.2.3 Evasion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJPreBp-hs2b"
      },
      "outputs": [],
      "source": [
        "filtered_df_last2Q = quarter_df_results[(quarter_df_results['quarter_ID'] == '2Q24') | (quarter_df_results['quarter_ID'] == '3Q24')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMhTQO-7D1eA"
      },
      "outputs": [],
      "source": [
        "# Plot a pie chart\n",
        "plot_pie_chart(filtered_df_last2Q['Phi-3.5 Evasion Present'], '2Q24/3Q24')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vwvZdq9h10U"
      },
      "source": [
        "Over 80% of answers were classed as evasive.\n",
        "\n",
        "Note that Phi-3.5 does overestimate Evasiveness in answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u09Ykldik8F"
      },
      "source": [
        "#### 3.2.3.1 Topic modelling on evasive texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVi0_yeDmO9t"
      },
      "source": [
        "##### 3.2.3.1.1 BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQUzbJ7XmRA9"
      },
      "outputs": [],
      "source": [
        "df_phi_fulltable_summarised = pd.read_excel(processed_data_folder + '/phi_fulltable_summarised.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl_ntZWdmfZA"
      },
      "outputs": [],
      "source": [
        "# subset relevant columns from sentiment file\n",
        "qevasion_colname = 'Phi-3.5 Evasion Present'\n",
        "qevasion_df = filtered_df_last2Q[['qa_num', qevasion_colname]]\n",
        "\n",
        "# subset relevant columns from summarised text file\n",
        "fulltable_summarised_df = df_phi_fulltable_summarised[['uid', 'qa_num', 'summarised_text']]\n",
        "qevasion_status='Evasive'\n",
        "df_qevasion_summarised = pd.merge(fulltable_summarised_df, qevasion_df, on=['qa_num'], how='left')\n",
        "\n",
        "# subset relevant columns from sentiment file\n",
        "qevasion_colname = 'Phi-3.5 Evasion Present'\n",
        "qevasion_df = qevasion_df[['qa_num', qevasion_colname]]\n",
        "\n",
        "# subset relevant columns from summarised text file\n",
        "fulltable_summarised_df = fulltable_summarised_df[['uid', 'qa_num', 'summarised_text']]\n",
        "# merge two dataset\n",
        "qevasion_status='Evasive'\n",
        "df_qevasion_summarised = pd.merge(fulltable_summarised_df, qevasion_df, on=['qa_num'], how='left')\n",
        "df_qevasion_summarised['quarter_str'] = [x.split(\"_\")[1] for x in df_qevasion_summarised['uid']]\n",
        "df_evaded_summarised = df_qevasion_summarised[df_qevasion_summarised[qevasion_colname].eq(qevasion_status)].copy().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWUGPkIjmhWc"
      },
      "outputs": [],
      "source": [
        "quarter_str_status_24 = ('2Q24','3Q24')\n",
        "quarter_colname = 'quarter_str'\n",
        "df_evaded_summarised_24Q2Q3 = df_evaded_summarised[df_evaded_summarised[quarter_colname] != (quarter_str_status_22)]\n",
        "df_evaded_summarised_24Q2Q3_list = df_evaded_summarised_24Q2Q3['summarised_text'].apply(preprocess_spacy).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bweIbltmn8N"
      },
      "outputs": [],
      "source": [
        "dim_model = PCA(n_components=5)\n",
        "cluster_model = KMeans(n_clusters=20)\n",
        "\n",
        "topic_evaded_summarised_24Q2Q3 = BERTopic(umap_model=dim_model, embedding_model=embedding_model,\n",
        "                       hdbscan_model=cluster_model, calculate_probabilities=True)\n",
        "topics, probs = topic_evaded_summarised_24Q2Q3.fit_transform(df_evaded_summarised_24Q2Q3_list)\n",
        "topic_evaded_summarised_24Q2Q3 = topic_evaded_summarised_24Q2Q3.reduce_topics(df_evaded_summarised_24Q2Q3_list, nr_topics=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW31Pnyp0o2G"
      },
      "outputs": [],
      "source": [
        "# Plot each BERTtopic visualization into a subplot\n",
        "topic_evaded_summarised_24Q2Q3.visualize_topics().write_html(\"topic_evaded_summarised_24Q2Q3_topic.html\")\n",
        "topic_evaded_summarised_24Q2Q3.visualize_barchart(top_n_topics=20,n_words=8, autoscale=True).write_html(\"topic_evaded_summarised_24Q2Q3_barchart.html\")\n",
        "topic_evaded_summarised_24Q2Q3.visualize_heatmap().write_html(\"topic_evaded_summarised_24Q2Q3_heatmap.html\")\n",
        "topic_evaded_summarised_24Q2Q3.visualize_hierarchy().write_html(\"topic_evaded_summarised_24Q2Q3_hierarchy.html\")\n",
        "\n",
        "# Load each plot into a subplot\n",
        "\n",
        "display(HTML(\"topic_evaded_summarised_24Q2Q3_topic.html\"))\n",
        "display(HTML(\"topic_evaded_summarised_24Q2Q3_barchart.html\"))\n",
        "display(HTML(\"topic_evaded_summarised_24Q2Q3_heatmap.html\"))\n",
        "display(HTML(\"topic_evaded_summarised_24Q2Q3_hierarchy.html\"))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuJS2UsLyepZ"
      },
      "source": [
        "Topics associated with evaded questions were capital market and growth dynamics (including NII), and financial indicators.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHnJ5sL9nLWP"
      },
      "source": [
        "### 3.2.4 Evasion + Negativity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f41nKz0XnUQl"
      },
      "source": [
        "To look at the overlapped topics in evasion and negativity, we create a Venn diagram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Yz61IWPCBC"
      },
      "outputs": [],
      "source": [
        "# Create a Venn diagram\n",
        "keywords_evaded_24Q2Q3 = extract_keywords(topic_evaded_summarised_24Q2Q3)\n",
        "keywords_phi_24Q2Q3neg = extract_keywords(topic_model_phi_2024Q2Q3_neg)\n",
        "venn_24  =venn2(\n",
        "    [keywords_evaded_24Q2Q3, keywords_phi_24Q2Q3neg],\n",
        "    ('Evaded  ', 'Negative ')\n",
        ")\n",
        "# Set colors\n",
        "venn_24.get_patch_by_id('10').set_color('#FFBF00')  # Evaded only\n",
        "venn_24.get_patch_by_id('01').set_color('#00BFFF')    # Negative only\n",
        "# Add a title\n",
        "plt.title(\"BERTopic Word Frequencies \")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POJ9qkLvPJ-t"
      },
      "outputs": [],
      "source": [
        "# Inspect overlaps\n",
        "intersection_evaded_phi_24Q2Q3neg = keywords_evaded_24Q2Q3 & keywords_phi_24Q2Q3neg\n",
        "print(\"Overlap between Evaded and Phi Negative:\", intersection_evaded_phi_24Q2Q3neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_OUohfqyhzR"
      },
      "source": [
        "These are the most common words in topics discovered by our model (bert)\n",
        "They correspond to the most frequent topics in our analysis of both negative and evaded topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnSxvbxDo7-h"
      },
      "source": [
        "### 3.2.5 Regulatory keyword outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMMjopMgRoak"
      },
      "outputs": [],
      "source": [
        "# Function to create a wordcloud (basic)\n",
        "def create_wordcloud_basic(title, topics):\n",
        "  wordcloud = WordCloud(width=1000, height=600, background_color='white').generate(\" \".join(topics))\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.title(f'{title}')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1xq1n3VRoal"
      },
      "outputs": [],
      "source": [
        "# Filter the quarters of interest df to be where 'keywords' column is not null or empty\n",
        "filtered_df_keywords = quarter_df_results[quarter_df_results['keywords'].notna() & (quarter_df_results['keywords'] != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qYwKK8ES2LW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Look at all text in answers where Basel III mentioned.\n",
        "  Create a wordcloud for easy visualisation of common words in these answers.\n",
        "\"\"\"\n",
        "\n",
        "# Extract and clean 'qa_text' by words\n",
        "qa_topics = [\n",
        "    topic.strip()\n",
        "    for row in filtered_df_keywords['qa_text'].dropna()\n",
        "    for topic in row.split(' ')\n",
        "]\n",
        "\n",
        "# Remove uninformative words\n",
        "words_to_remove = ['Jamie', 'obviously', 'thing', 'whatever', 'right', 'know', 'look', 'think', 'question', 'actually', 'really', 'still', 'III', 'yeah']\n",
        "qa_topics = [topic for topic in qa_topics if topic.lower() not in map(str.lower, words_to_remove)]\n",
        "\n",
        "# Plot wordcloud of qa_text where Basel III mentioned\n",
        "create_wordcloud_basic('WordCloud of Q&As where Basel III mentioned', qa_topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia0HRTYDSCIv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Create filtered dataframe containing only entries where 'keywords' is not empty.\n",
        "  Analyse the Evaded topics for these entries.\n",
        "  Note that all these answers were classed as Evasive.\n",
        "\"\"\"\n",
        "# Extract and clean the 'Phi-3.5 Evaded Topics\n",
        "evaded_topics = [\n",
        "    topic.strip()\n",
        "    for row in filtered_df_keywords['Phi-3.5 Evaded Topics'].dropna()\n",
        "    for topic in row.split(',')\n",
        "]\n",
        "\n",
        "# Remove uninformative words (e.g., Basel III Endgame) from wordcloud (as the title includes Basel III)\n",
        "words_to_remove = ['Basel', 'III', 'Endgame', 'Basel III Endgame']\n",
        "evaded_wordcloud_topics = [topic for topic in evaded_topics if topic.lower() not in map(str.lower, words_to_remove)]\n",
        "\n",
        "# Plot a wordcloud for evaded topics where Basel III mentioned\n",
        "create_wordcloud_basic('Evaded Topics where Basel III mentioned', evaded_wordcloud_topics)\n",
        "\n",
        "# Display list of topics\n",
        "topic_series = pd.Series(evaded_topics)\n",
        "topic_counts = topic_series.value_counts()\n",
        "print(\"\\nEvaded Topics where Basel III mentioned:\")\n",
        "display(topic_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J_SugAtylS7"
      },
      "source": [
        "Evasive Topics: focus on Capital requirements / returns, GSIB, NII, Yield, CET1 ratio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLkWMwRVpYLy"
      },
      "source": [
        "# 4 Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US_VNrTEtEgT"
      },
      "source": [
        "To derive meaningful insight from transcripts, we recommend a solution pipeline that involves identification of texts with specific sentiment or question evasion status, followed by topic modelling using BERTopic on Phi-3.5-summarised text. By focusing on negative-sentiment Q&A in 1Q22, BERTopic highlighted themes like declining Net Interest Income in 55% of texts and geopolitical risks in 27% of texts. In evasive answers, the most common topic (60% of texts) concerned corporate growth, but geopolitical challenges were present too (9%). These findings enhanced our understanding of sentiment and thematic concerns during critical periods.\n",
        "\n",
        "Emerging risks were identified through analysis of the two most recent quarters, where we observed a decline in the average sentiment, driven by greater neutrality at the expense of positive sentiment. Overlapping BERTopic-discovered themes in negative and evasive Q&As provided valuable insights into areas such as capital market uncertainty and reserve dynamics that might require closer regulatory attention. Detection of keywords related to regulatory changes like Basel III, capital adequacy, and credit loss provisions contextualised these findings, providing actionable insights into emerging risks.\n",
        "\n",
        "The methodology shows potential for scalability with tailored preprocessing for different transcript formats. Automating transcript preprocessing would enhance adaptability. Generalised models, such as Phi-3.5 and BERTopic, already exhibit strong applicability across financial datasets, but fine-tuning of Phi-3.5 with domain-specific training datasets is likely to further improve performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZEV6_H3_jXfW",
        "4uUeRA7Zms6-",
        "1IWF0u0doot6",
        "PAoM9dJYmaju",
        "cw9yKjjxpOhy",
        "Kdra4f2UfyB6",
        "4u09Ykldik8F",
        "EVi0_yeDmO9t",
        "wHnJ5sL9nLWP",
        "OLkWMwRVpYLy"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}